{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Learning Data Assimilation Increments\n",
    "\n",
    "This notebook is derived from the previous notebooks: {doc}`Neural_network_for_Lorenz96` and {doc}`DA_demo_L96`.\n",
    "\n",
    "We've restricted it to only using the 3-layer network (not the linear regression model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import DA_methods\n",
    "from L96_model import L96, L96_eq1_xdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring reproducibility\n",
    "rng = np.random.default_rng()\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Defining the Model, its Parameters, and Other Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Defining the General Circulation Model (GCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GCM(X_init, F, dt, nt, param=[0]):\n",
    "    \"\"\"A toy `General Circulation Model` (GCM) that uses the single\n",
    "    time-scale Lorenz 1996 model with a parameterised coupling term\n",
    "    to represent the interaction between the observed coarse scale\n",
    "    processes `X` and unobserved fine scale processes `Y` of the two\n",
    "    time-scale model.\n",
    "\n",
    "    Args:\n",
    "        X_init: Initial conditions of X\n",
    "        F: Forcing term\n",
    "        dt: Sampling frequency of the model\n",
    "        nt: Number of timesteps for which to run the model\n",
    "        param: Weights to give to the coupling term\n",
    "\n",
    "    Returns:\n",
    "        Model output for all variables of X at each timestep along with\n",
    "        the corresponding time units\n",
    "    \"\"\"\n",
    "    time, hist, X = (\n",
    "        dt * np.arange(nt + 1),\n",
    "        np.zeros((nt + 1, len(X_init))) * np.nan,\n",
    "        X_init.copy(),\n",
    "    )\n",
    "    hist[0] = X\n",
    "\n",
    "    for n in range(nt):\n",
    "        X = X + dt * (L96_eq1_xdot(X, F) - np.polyval(param, X))\n",
    "        hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "\n",
    "    return hist, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Defining the Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def s(k, K):\n",
    "    \"\"\"A non-dimension coordinate from -1..+1 corresponding to k=0..K\"\"\"\n",
    "    return 2 * (0.5 + k) / K - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dist(i, j, K):\n",
    "    \"\"\"Compute the absolute distance between two element indices\n",
    "    within a square matrix of size (K x K)\n",
    "\n",
    "    Args:\n",
    "        i: the ith row index\n",
    "        j: the jth column index\n",
    "        K: shape of square array\n",
    "\n",
    "    Returns:\n",
    "        Distance\n",
    "    \"\"\"\n",
    "    return abs(i - j) if abs(i - j) <= 0.5 * K / 2 else K - abs(i - j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def observation_operator(K, l_obs, t_obs, i_t):\n",
    "    \"\"\"Observation operator to map between model and observation space,\n",
    "    assuming linearity and model space observations.\n",
    "\n",
    "    Args:\n",
    "        K: spatial dimension of the model\n",
    "        l_obs: spatial positions of observations on model grid\n",
    "        t_obs: time positions of observations\n",
    "        i_t: the timestep of the current DA cycle\n",
    "\n",
    "    Returns:\n",
    "        Operator matrix (K * observation_density, K)\n",
    "    \"\"\"\n",
    "    n = l_obs.shape[-1]\n",
    "    H = np.zeros((n, K))\n",
    "    H[range(n), l_obs[t_obs == i_t]] = 1\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gaspari_cohn(distance, radius):\n",
    "    \"\"\"Compute the appropriate distance dependent weighting of a\n",
    "    covariance matrix, after Gaspari & Cohn, 1999 (https://doi.org/10.1002/qj.49712555417)\n",
    "\n",
    "    Args:\n",
    "        distance: the distance between array elements\n",
    "        radius: localization radius for DA\n",
    "\n",
    "    Returns:\n",
    "        distance dependent weight of the (i,j) index of a covariance matrix\n",
    "    \"\"\"\n",
    "    if distance == 0:\n",
    "        weight = 1.0\n",
    "    else:\n",
    "        if radius == 0:\n",
    "            weight = 0.0\n",
    "        else:\n",
    "            ratio = distance / radius\n",
    "            weight = 0.0\n",
    "            if ratio <= 1:\n",
    "                weight = (\n",
    "                    -(ratio**5) / 4\n",
    "                    + ratio**4 / 2\n",
    "                    + 5 * ratio**3 / 8\n",
    "                    - 5 * ratio**2 / 3\n",
    "                    + 1\n",
    "                )\n",
    "            elif ratio <= 2:\n",
    "                weight = (\n",
    "                    ratio**5 / 12\n",
    "                    - ratio**4 / 2\n",
    "                    + 5 * ratio**3 / 8\n",
    "                    + 5 * ratio**2 / 3\n",
    "                    - 5 * ratio\n",
    "                    + 4\n",
    "                    - 2 / 3 / ratio\n",
    "                )\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def localize_covariance(B, loc=0):\n",
    "    \"\"\"Localize the model climatology covariance matrix, based on\n",
    "    the Gaspari-Cohn function.\n",
    "\n",
    "    Args:\n",
    "        B: Covariance matrix over a long model run 'M_truth' (K, K)\n",
    "        loc: spatial localization radius for DA\n",
    "\n",
    "    Returns:\n",
    "        Covariance matrix scaled to zero outside distance 'loc' from diagonal and\n",
    "        the matrix of weights which are used to scale covariance matrix\n",
    "    \"\"\"\n",
    "    M, N = B.shape\n",
    "    X, Y = np.ix_(np.arange(M), np.arange(N))\n",
    "    dist = np.vectorize(get_dist)(X, Y, M)\n",
    "    W = np.vectorize(gaspari_cohn)(dist, loc)\n",
    "    return B * W, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def running_average(X, N):\n",
    "    \"\"\"Compute running mean over a user-specified window.\n",
    "\n",
    "    Args:\n",
    "        X: Input vector of arbitrary length 'n'\n",
    "        N: Size of window over which to compute mean\n",
    "\n",
    "    Returns:\n",
    "        X averaged over window N\n",
    "    \"\"\"\n",
    "    if N % 2 == 0:\n",
    "        N1, N2 = -N / 2, N / 2\n",
    "    else:\n",
    "        N1, N2 = -(N - 1) / 2, (N + 1) / 2\n",
    "    X_sum = np.zeros(X.shape)\n",
    "    for i in np.arange(N1, N2):\n",
    "        X_sum = X_sum + np.roll(X, int(i), axis=0)\n",
    "    return X_sum / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_obs(loc, obs, t_obs, l_obs, period):\n",
    "    \"\"\"NOTE: This function is for plotting purposes only.\"\"\"\n",
    "    t_period = np.where((t_obs[:, 0] >= period[0]) & (t_obs[:, 0] < period[1]))\n",
    "    obs_period = np.zeros(t_period[0].shape)\n",
    "    obs_period[:] = np.nan\n",
    "    for i in np.arange(len(obs_period)):\n",
    "        if np.any(l_obs[t_period[0][i]] == loc):\n",
    "            obs_period[i] = obs[t_period[0][i]][l_obs[t_period[0][i]] == loc]\n",
    "    return obs_period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Initializing the Lorenz 1996 Model Parameters\n",
    "\n",
    "Let's define the parameters of the Lorenz 1996 model that match those of Wilks, 2005; K=8, J=32, F=18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # fmt: off\n",
    "    K = 8                               # Dimension of L96 `X` variables\n",
    "    J = 32                              # Dimension of L96 `Y` variables\n",
    "    obs_freq = 10                       # Observation frequency (number of sampling intervals (si) per observation)\n",
    "    F_truth = 18                        # F for truth signal\n",
    "    F_fcst = 18                         # F for forecast (DA) model\n",
    "    GCM_param = np.array([0, 0, 0, 0])  # Polynomial coefficicents for GCM parameterization\n",
    "    ns_da = 4000                        # Number of time samples for DA\n",
    "    ns = 4000                           # Number of time samples for truth signal\n",
    "    ns_spinup = 200                     # Number of time samples for spin up\n",
    "    dt = 0.005                          # Model timestep\n",
    "    si = 0.005                          # Truth sampling interval\n",
    "    B_loc = 0.0                         # Spatial localization radius for DA\n",
    "    DA = \"EnKF\"                         # DA method\n",
    "    nens = 50                           # Number of ensemble members for DA\n",
    "    inflate_opt = \"relaxation\"          # Method for DA model covariance inflation\n",
    "    inflate_factor = 0.86               # Inflation factor\n",
    "    obs_density = 1.0                   # Fraction of spatial gridpoints where observations are collected\n",
    "    DA_freq = 10                        # Assimilation frequency (number of sampling intervals (si) per assimilation step)\n",
    "    obs_sigma = 0.1                     # Observational error standard deviation\n",
    "    initial_spread = 0.1                # Initial spread added to initial conditions\n",
    "    # fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Suggestions for Modifying the L96 Model Paramters\n",
    "\n",
    "If you want to modify the default model parameters given above, you can use the suggestions below for alternate values depending upon the desired behaviour.\n",
    "\n",
    "**Less certain observations**\n",
    "- `obs_sigma`: 1.0\n",
    "- `initial_spread`: 1.0\n",
    "- `inflate_factor`: 0.5\n",
    "\n",
    "**Less frequent observations**\n",
    "- `obs_freq`: 50\n",
    "- `DA_freq`: 50\n",
    "- `inflate_factor`: 0.4\n",
    "\n",
    "**Very infrequent observations**\n",
    "- `obs_freq`: 200\n",
    "- `DA_freq`: 200\n",
    "- `inflate_factor`: 0.5\n",
    "\n",
    "**More frequent observations**\n",
    "- `obs_freq`: 5\n",
    "- `DA_freq`: 5\n",
    "- `inflate_factor`: 0.9\n",
    "\n",
    "**Very frequent observations**\n",
    "- `obs_freq`: 1\n",
    "- `DA_freq`: 1\n",
    "- `inflate_factor`: 0.98\n",
    "\n",
    "**Very frequent observations but less accurate**\n",
    "- `obs_freq`: 1\n",
    "- `DA_freq`: 1\n",
    "- `obs_sigma`: 1.0\n",
    "- `initial_spread`: 1.0\n",
    "- `inflate_factor`: 0.9\n",
    "\n",
    "**Different time-scale**\n",
    "- `F_fcst`: 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Generate `Truth` Run from Two Time-Scale L96 Model\n",
    "\n",
    "The L96 two time-scale model acts as the **real world** using which we obtain the unobserved `truth` field from which our observations will be derived.\n",
    "\n",
    "We begin by spinning-up a state and then record a series of $X$ and $Y$ at time $t$ in arrays `X_truth`, `Y_truth` and `t_truth` respectively. The initial state $X(t=0$ is recorded in `X_init` (and equal to `X_truth[0]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the \"truth\" 2-scale L96 model\n",
    "M_truth = L96(Config.K, Config.J, F=Config.F_truth, dt=Config.dt)\n",
    "M_truth.set_state(rng.standard_normal((Config.K)), 0 * M_truth.j)\n",
    "\n",
    "# The model runs for `ns_spinup` timesteps to spin-up\n",
    "X_spinup, Y_spinup, t_spinup = M_truth.run(Config.si, Config.si * Config.ns_spinup)\n",
    "\n",
    "# Generate the initial conditions of X and Y\n",
    "X_init = X_spinup[-1, :]\n",
    "Y_init = Y_spinup[-1, :]\n",
    "\n",
    "# Using the initial conditions, generate the truth\n",
    "M_truth.set_state(X_init, Y_init)\n",
    "X_truth, Y_truth, t_truth = M_truth.run(Config.si, Config.si * Config.ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Generate Synthetic Observations\n",
    "\n",
    "Now we create some **observations** of the **real world** by sampling at `obs_freq` intervals and adding some noise (observational error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the \"truth\" to generate observations at certain times (t_obs) and locations (l_obs)\n",
    "t_obs = np.tile(\n",
    "    np.arange(Config.obs_freq, Config.ns_da + Config.obs_freq, Config.obs_freq),\n",
    "    [int(Config.K * Config.obs_density), 1],\n",
    ").T\n",
    "\n",
    "l_obs = np.zeros(t_obs.shape, dtype=\"int\")\n",
    "for i in range(l_obs.shape[0]):\n",
    "    l_obs[i, :] = rng.choice(\n",
    "        Config.K, int(Config.K * Config.obs_density), replace=False\n",
    "    )\n",
    "\n",
    "X_obs = X_truth[t_obs, l_obs] + Config.obs_sigma * rng.standard_normal(l_obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculated observation covariance matrix, assuming independent observations\n",
    "R = Config.obs_sigma**2 * np.eye(int(Config.K * Config.obs_density))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 6], dpi=150)\n",
    "plt.plot(t_truth[:], X_truth[:, 0], label=\"truth\")\n",
    "plt.scatter(\n",
    "    t_truth[t_obs[1:, 0]],\n",
    "    find_obs(0, X_obs, t_obs, l_obs, [t_obs[0, 0], t_obs[-1, 0]]),\n",
    "    color=\"k\",\n",
    "    label=\"obs\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time t\")\n",
    "plt.ylabel(\"X(t)\")\n",
    "plt.title(\"Observations at k=0\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Apply Localization to the Background Model Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We run the model in forward mode for 5000 steps to calculate the background covariance. The model is the **GCM** function defined above which integrates forward.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F\n",
    "\\end{align}\n",
    "\n",
    "The absence of the coupling term to the $Y$ equations makes this a model with **missing physics** that we hope the Ensemble Kalman Filter will correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate climatological background covariance for 1-scale L96 model\n",
    "X1_clim, _ = GCM(X_init, Config.F_fcst, Config.dt, 5000)\n",
    "B_clim1 = np.cov(X1_clim.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-calculated climatological background covariance matrix from a long simulation\n",
    "B_loc, W_clim = localize_covariance(B_clim1, loc=Config.B_loc)\n",
    "\n",
    "B_corr1 = np.zeros(B_clim1.shape)\n",
    "for i in range(B_clim1.shape[0]):\n",
    "    for j in range(B_clim1.shape[1]):\n",
    "        B_corr1[i, j] = B_clim1[i, j] / np.sqrt(B_clim1[i, i] * B_clim1[j, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6), dpi=150)\n",
    "plt.subplot(131)\n",
    "plt.contourf(B_corr1, cmap=\"bwr\", extend=\"both\", levels=np.linspace(-0.95, 0.95, 20))\n",
    "plt.colorbar()\n",
    "plt.title(\"Background correlation matrix: 1-scale L96\")\n",
    "plt.subplot(132)\n",
    "plt.contourf(B_loc)\n",
    "plt.colorbar()\n",
    "plt.title(\"B_loc\")\n",
    "plt.subplot(133)\n",
    "plt.contourf(W_clim)\n",
    "plt.colorbar()\n",
    "plt.title(\"W_clim\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Run Data Assimilation\n",
    "\n",
    "The algorithms steps through segments of time (**DA cycles**), launching an ensemble of short forecasts from the posterior estimate of the preceding segment, each perturbed by noise in their initial condition (inflation).\n",
    "\n",
    "Each ensemble trajectory is stored in `ensX`. The increment added to correct the prior is in `X_inc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up array to store DA increments\n",
    "X_inc = np.zeros((int(Config.ns_da / Config.DA_freq), Config.K, Config.nens))\n",
    "if Config.DA == \"3DVar\":\n",
    "    X_inc = np.squeeze(X_inc)\n",
    "t_DA = np.zeros(int(Config.ns_da / Config.DA_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize ensemble with perturbations\n",
    "i_t = 0\n",
    "ensX = X_init[None, :, None] + rng.standard_normal((1, Config.K, Config.nens))\n",
    "X_post = ensX[0, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W = W_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cycle in np.arange(0, Config.ns_da / Config.DA_freq, dtype=int):\n",
    "    # Set up array to store model forecast for each DA cycle\n",
    "    ensX_fcst = np.zeros((Config.DA_freq + 1, Config.K, Config.nens))\n",
    "\n",
    "    # Model forecast for next DA cycle\n",
    "    for n in range(Config.nens):\n",
    "        ensX_fcst[..., n] = GCM(\n",
    "            X_post[0 : Config.K, n],\n",
    "            Config.F_fcst,\n",
    "            Config.dt,\n",
    "            Config.DA_freq,\n",
    "            Config.GCM_param,\n",
    "        )[0]\n",
    "\n",
    "    i_t = i_t + Config.DA_freq\n",
    "\n",
    "    # Get prior/background from the forecast\n",
    "    X_prior = ensX_fcst[-1, ...]\n",
    "\n",
    "    # Call DA\n",
    "    t_DA[cycle] = t_truth[i_t]\n",
    "    if Config.DA == \"EnKF\":\n",
    "        H = observation_operator(Config.K, l_obs, t_obs, i_t)\n",
    "\n",
    "        # Augment state vector with parameters when doing parameter estimation\n",
    "        B_ens = np.cov(X_prior)\n",
    "        B_ens_loc = B_ens * W[0 : Config.K, 0 : Config.K]\n",
    "        X_post = DA_methods.EnKF(X_prior, X_obs[t_obs == i_t], H, R, B_ens_loc)\n",
    "        X_post[0 : Config.K, :] = DA_methods.ens_inflate(\n",
    "            X_post[0 : Config.K, :],\n",
    "            X_prior[0 : Config.K, :],\n",
    "            Config.inflate_opt,\n",
    "            Config.inflate_factor,\n",
    "        )\n",
    "    elif Config.DA == \"None\":\n",
    "        X_post = X_prior\n",
    "\n",
    "    if not Config.DA == \"None\":\n",
    "        # Get current increments\n",
    "        X_inc[cycle, ...] = (\n",
    "            np.squeeze(X_post[0 : Config.K, ...]) - X_prior[0 : Config.K, ...]\n",
    "        )\n",
    "\n",
    "    # Reset initial conditions for next DA cycle\n",
    "    ensX_fcst[-1, :, :] = X_post[0 : Config.K, :]\n",
    "    ensX = np.concatenate((ensX, ensX_fcst[1:None, ...]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Post Processing and Visualization\n",
    "\n",
    "`meanX` is the ensemble mean forecast, averaging over all the ensemble members. It has discontinuities due to the increment addition between each DA segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meanX = np.mean(ensX, axis=-1)\n",
    "clim = np.max(np.abs(meanX - X_truth[0 : (Config.ns_da + 1), :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "ch = axes[0, 0].contourf(\n",
    "    M_truth.k,\n",
    "    t_truth[0 : (Config.ns_da + 1)],\n",
    "    meanX - X_truth[0 : (Config.ns_da + 1), :],\n",
    "    cmap=\"bwr\",\n",
    "    levels=np.arange(-6.5, 7, 1),\n",
    "    extend=\"both\",\n",
    ")\n",
    "plt.colorbar(ch, ax=axes[0, 0], orientation=\"horizontal\")\n",
    "axes[0, 0].set_xlabel(\"s\")\n",
    "axes[0, 0].set_ylabel(\"t\")\n",
    "axes[0, 0].set_title(\"X - X_truth\")\n",
    "axes[0, 1].plot(\n",
    "    t_truth[0 : (Config.ns_da + 1)],\n",
    "    np.sqrt(((meanX - X_truth[0 : (Config.ns_da + 1), :]) ** 2).mean(axis=-1)),\n",
    "    label=\"RMSE\",\n",
    ")\n",
    "axes[0, 1].plot(\n",
    "    t_truth[0 : (Config.ns_da + 1)],\n",
    "    np.mean(np.std(ensX, axis=-1), axis=-1),\n",
    "    label=\"Spread\",\n",
    ")\n",
    "axes[0, 1].plot(\n",
    "    t_truth[0 : (Config.ns_da + 1)],\n",
    "    Config.obs_sigma * np.ones((Config.ns_da + 1)),\n",
    "    label=\"Obs error\",\n",
    ")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_xlabel(\"time\")\n",
    "axes[0, 1].set_title(\"RMSE (X - X_truth)\")\n",
    "axes[0, 1].grid(which=\"both\", linestyle=\"--\")\n",
    "\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k,\n",
    "    np.sqrt(((meanX - X_truth[0 : (Config.ns_da + 1), :]) ** 2).mean(axis=0)),\n",
    "    label=\"RMSE\",\n",
    ")\n",
    "X_inc_ave = X_inc / Config.DA_freq / Config.si\n",
    "axes[0, 2].plot(M_truth.k, X_inc_ave.mean(axis=(0, -1)), label=\"Inc\")\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k, running_average(X_inc_ave.mean(axis=(0, -1)), 7), label=\"Inc Ave\"\n",
    ")\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k,\n",
    "    np.ones(M_truth.k.shape) * (Config.F_fcst - Config.F_truth),\n",
    "    label=\"F_bias\",\n",
    ")\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k,\n",
    "    np.ones(M_truth.k.shape) * (X_inc / Config.DA_freq / Config.si).mean(),\n",
    "    \"k:\",\n",
    "    label=\"Ave Inc\",\n",
    ")\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].set_xlabel(\"s\")\n",
    "axes[0, 2].set_title(\"Increments\")\n",
    "axes[0, 2].grid(which=\"both\", linestyle=\"--\")\n",
    "\n",
    "plot_start, plot_end = 200, 800\n",
    "plot_start_DA, plot_end_DA = int(plot_start / Config.DA_freq), int(\n",
    "    plot_end / Config.DA_freq\n",
    ")\n",
    "plot_x = 0\n",
    "axes[1, 0].plot(\n",
    "    t_truth[plot_start:plot_end], X_truth[plot_start:plot_end, plot_x], label=\"truth\"\n",
    ")\n",
    "axes[1, 0].plot(\n",
    "    t_truth[plot_start:plot_end], meanX[plot_start:plot_end, plot_x], label=\"forecast\"\n",
    ")\n",
    "axes[1, 0].scatter(\n",
    "    t_DA[plot_start_DA - 1 : plot_end_DA - 1],\n",
    "    find_obs(plot_x, X_obs, t_obs, l_obs, [plot_start, plot_end]),\n",
    "    label=\"obs\",\n",
    ")\n",
    "axes[1, 0].grid(which=\"both\", linestyle=\"--\")\n",
    "axes[1, 0].set_xlabel(\"time\")\n",
    "axes[1, 0].set_title(\"k=\" + str(plot_x + 1) + \" truth and forecast\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 1].axis(\"off\")\n",
    "\n",
    "axes[1, 2].text(\n",
    "    0.1,\n",
    "    0.1,\n",
    "    \"RMSE={:3f}\\nSpread={:3f}\\nGCM param={}\\nDA={},{}\\nDA_freq={}\\nB_loc={}\\ninflation={},{}\\nobs_density={}\\nobs_sigma={}\\nobs_freq={}\".format(\n",
    "        np.sqrt(((meanX - X_truth[0 : (Config.ns_da + 1), :]) ** 2).mean()),\n",
    "        np.mean(np.std(ensX, axis=-1)),\n",
    "        Config.DA,\n",
    "        Config.GCM_param,\n",
    "        Config.nens,\n",
    "        Config.DA_freq,\n",
    "        Config.B_loc,\n",
    "        Config.inflate_opt,\n",
    "        Config.inflate_factor,\n",
    "        Config.obs_density,\n",
    "        Config.obs_sigma,\n",
    "        Config.obs_freq,\n",
    "    ),\n",
    "    fontsize=15,\n",
    ")\n",
    "axes[1, 2].axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Examining the Relationship between the Members and their Increments\n",
    "\n",
    "Converting the increment `X_inc` into a tendency, we can examine the relationship between $\\dot{X}$ (due to the missing physics) and the state of the model at the beginning of each DA segment.\n",
    "\n",
    "If we are properly correcting the absence of the coupling term then this structure should look like the parameterization of the coupling term, as done in Wilks, 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Individual Ensemble Members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jj = np.abs(X_inc_ave[0:, :].flatten()) > -1e-7\n",
    "\n",
    "# The offset by DA_freq looks at the previous posterior\n",
    "x_input = ensX[t_obs[0:, 0] - Config.DA_freq, :].flatten()[jj]\n",
    "\n",
    "# Mid-point of trajectory\n",
    "x_input = 0.5 * (x_input + ensX[t_obs[0:, 0], :].flatten()[jj])\n",
    "\n",
    "xinc_output = X_inc_ave[0:, :].flatten()[jj]\n",
    "\n",
    "x = np.linspace(-8, 15, 100)\n",
    "p = np.polyfit(x_input, xinc_output, 4)\n",
    "p18 = [0.000707, -0.0130, -0.0190, 1.59, 0.275]  # Polynomial from Wilks, 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5), dpi=150)\n",
    "plt.suptitle(\"All time, all individial k, and all ensemble members\")\n",
    "plt.subplot(121)\n",
    "plt.plot(x_input, xinc_output, \"k.\")\n",
    "plt.grid()\n",
    "plt.plot(x, -np.polyval(p18, x), label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)$\")\n",
    "plt.xlabel(\"Ensemble member $X_i(k,t)$\")\n",
    "plt.ylabel(\"Ensemble member increment $\\dot{X}$\")\n",
    "plt.subplot(122)\n",
    "plt.hist2d(\n",
    "    x_input, xinc_output, bins=(np.linspace(-10, 15, 50), np.linspace(-25, 20, 150))\n",
    ")\n",
    "plt.plot(x, -np.polyval(p18, x), label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)$\")\n",
    "plt.xlabel(\"Ensemble member $X_i(k,t)$\")\n",
    "plt.ylabel(\"Ensemble member increment $\\dot{X}$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xl = 8, 10\n",
    "k = 0\n",
    "e = 19\n",
    "si = Config.DA_freq\n",
    "\n",
    "l = (l_obs == k).max(axis=1)\n",
    "\n",
    "plt.figure(figsize=(14, 5), dpi=150)\n",
    "plt.suptitle(\"Ensemble member %i , k = %i\" % (e, k))\n",
    "plt.plot(t_truth, X_truth[:, k], \"--\", label=\"Truth\")\n",
    "plt.plot(t_truth, ensX[:, k, e], label=\"Ensemble member forecast\")\n",
    "plt.plot(\n",
    "    t_truth[t_obs[l, 0]],\n",
    "    ensX[si::si, k, e][l] - X_inc[:, :, e][l, k],\n",
    "    \".\",\n",
    "    label=\"Ensemble member prior\",\n",
    ")\n",
    "plt.plot(t_truth[t_obs[l, 0]], ensX[si::si, k, e][l], \".\", label=\"Ensemble member post\")\n",
    "plt.xlim(xl)\n",
    "plt.xlabel(\"Time, t\")\n",
    "plt.ylabel(\"$X(t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Mean over Ensemble Members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jj = np.abs(X_inc_ave.mean(axis=-1).flatten()) > -1e-7\n",
    "\n",
    "# The offset by DA_freq looks at the previous posterior\n",
    "x_input = meanX[t_obs[0:, 0] - Config.DA_freq].flatten()[jj]\n",
    "\n",
    "# Mid-point of trajectory\n",
    "x_input = 0.5 * (x_input + meanX[t_obs[0:, 0]].flatten()[jj])\n",
    "\n",
    "xinc_output = X_inc_ave.mean(axis=-1).flatten()[jj]\n",
    "\n",
    "x = np.linspace(-8, 15, 100)\n",
    "p = np.polyfit(x_input, xinc_output, 4)\n",
    "p18 = [0.000707, -0.0130, -0.0190, 1.59, 0.275]  # Polynomial from Wilks, 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5), dpi=150)\n",
    "plt.suptitle(\"All time, all individial k, mean over ensemble members\")\n",
    "plt.subplot(121)\n",
    "plt.plot(x_input, xinc_output, \"k.\")\n",
    "plt.grid()\n",
    "plt.plot(x, -np.polyval(p18, x), label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)\")\n",
    "plt.xlabel(\"Ensemble member $X_i(k,t)$\")\n",
    "plt.ylabel(\"Ensemble member increment $\\dot{X}$\")\n",
    "plt.subplot(122)\n",
    "plt.hist2d(\n",
    "    x_input, xinc_output, bins=(np.linspace(-10, 15, 50), np.linspace(-25, 20, 150))\n",
    ")\n",
    "plt.plot(x, -np.polyval(p18, x), label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)\")\n",
    "plt.xlabel(\"Ensemble member $X_i(k,t)$\")\n",
    "plt.ylabel(\"Ensemble member increment $\\dot{X}$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = 8, 10\n",
    "k = 0\n",
    "si = Config.DA_freq\n",
    "\n",
    "l = (l_obs == k).max(axis=1)\n",
    "\n",
    "plt.figure(figsize=(14, 5), dpi=150)\n",
    "plt.suptitle(\"Ensemble mean, k = %i\" % (k))\n",
    "plt.plot(t_truth, X_truth[:, k], \"--\", label=\"Truth\")\n",
    "plt.fill_between(\n",
    "    t_truth,\n",
    "    meanX[:, k] - ensX[:, k, :].std(axis=-1),\n",
    "    meanX[:, k] + ensX[:, k, :].std(axis=-1),\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=\"Ensemble spread\",\n",
    ")\n",
    "plt.plot(t_truth, meanX[:, k], label=\"Ensemble mean forecast\")\n",
    "plt.plot(\n",
    "    t_truth[t_obs[l, 0]],\n",
    "    meanX[si::si, k][l] - X_inc.mean(axis=-1)[l, k],\n",
    "    \".\",\n",
    "    label=\"Ensemble mean prior\",\n",
    ")\n",
    "plt.plot(t_truth[t_obs[l, 0]], meanX[si::si, k][l], \".\", label=\"Ensemble mean post\")\n",
    "plt.xlim(xl)\n",
    "plt.xlabel(\"Time, t\")\n",
    "plt.ylabel(\"$X(t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "With this, we have successfully **created the DA increments**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Learning the DA Increments\n",
    "\n",
    "With the dataset for DA Increments created above, we now build a neural network which will try to model that dataset so that we can use it for unseen observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_inc = t_truth[\n",
    "    t_obs[:, 0]\n",
    "]  # These are the times in the \"real world\" when observations were made\n",
    "dt_inc = np.diff(t_inc)[0]  # Time interval between increments\n",
    "\n",
    "dt = np.diff(t_truth)[0]  # Time-step of \"real world\" model\n",
    "da_interval = int(dt_inc / dt)\n",
    "\n",
    "# Data from DA system (increments for individual ensemble members)\n",
    "x_input = ensX[:-1:da_interval]\n",
    "X_tend = X_inc / dt_inc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Observing the Dataset\n",
    "\n",
    "As a sanity check, we look at the data for obvious structure. A polyfit to the data will compare well to Wilks 2005, if the data is similar in distribution. We show Wilks 2005 and the 4th order polyfit for reference but neither are used or needed for training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple scatter plot of x_tend against x_input\n",
    "x = np.linspace(-7, 14, 100)\n",
    "p = np.polyfit(x_input.flatten(), X_tend.flatten(), 4)\n",
    "p18 = [0.000707, -0.0130, -0.0190, 1.59, 0.275]  # Polynomial from Wilks, 2005\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(x_input.flatten(), X_tend.flatten(), \"k.\")\n",
    "plt.plot(x, -np.polyval(p18, x), label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PDf of x_tend against x_input\n",
    "plt.figure(dpi=150)\n",
    "plt.hist2d(\n",
    "    x_input.flatten(),\n",
    "    X_tend.flatten(),\n",
    "    bins=(np.linspace(-10, 15, 50), np.linspace(-25, 20, 150)),\n",
    "    cmap=plt.cm.Greys,\n",
    ")\n",
    "plt.plot(x, -np.polyval(p18, x), \"k--\", label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)$ lin. regr.\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X$_k$\")\n",
    "plt.ylabel(\"Missing X$_k$ tendency\")\n",
    "plt.title(\"Conventional linear regression\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Creating the Dataset Split\n",
    "\n",
    "We partition the dataset into **training** (seen by the network during optimization of the weights) and **validation** (used for evaluation) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_input.size // 2\n",
    "train_size = int(0.7 * x_input.size)\n",
    "print(\"Training set size =\", train_size, \"out of\", x_input.size)\n",
    "\n",
    "# Convert the data to type float32\n",
    "x_input, X_tend = x_input.astype(np.float32), X_tend.astype(np.float32)\n",
    "\n",
    "X_train = x_input.flatten()[:train_size]\n",
    "Y_train = X_tend.flatten()[:train_size]\n",
    "X_val = x_input.flatten()[train_size:]\n",
    "Y_val = X_tend.flatten()[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Building the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "\n",
    "# Training Dataset\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(Y_train))\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Validation Dataset\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(Y_val))\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetANN(nn.Module):\n",
    "    def __init__(self, W=16):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(1, W)\n",
    "        self.linear2 = nn.Linear(W, W)\n",
    "        self.linear3 = nn.Linear(W, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### Define the Training and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, criterion, loader, optimizer):\n",
    "    \"\"\"Train the network for one epoch\"\"\"\n",
    "    network.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        # Get predictions\n",
    "        if len(batch_x.shape) == 1:\n",
    "            # This if block is needed to add a dummy dimension if our inputs are 1D\n",
    "            # (where each number is a different sample)\n",
    "            prediction = torch.squeeze(network(torch.unsqueeze(batch_x, 1)))\n",
    "        else:\n",
    "            prediction = network(batch_x)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(prediction, batch_y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backpropagation to compute the gradients and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(network, criterion, loader):\n",
    "    \"\"\"Test the network\"\"\"\n",
    "    network.eval()  # Evaluation mode (important when having dropout layers)\n",
    "\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            # Get predictions\n",
    "            if len(batch_x.shape) == 1:\n",
    "                # This if block is needed to add a dummy dimension if our inputs are 1D\n",
    "                # (where each number is a different sample)\n",
    "                prediction = torch.squeeze(network(torch.unsqueeze(batch_x, 1)))\n",
    "            else:\n",
    "                prediction = network(batch_x)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(prediction, batch_y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        # Get an average loss for the entire dataset\n",
    "        test_loss /= len(loader)\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_model(network, criterion, optimizer, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"Train and validate the network\"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_model(network, criterion, train_loader, optimizer)\n",
    "        val_loss = test_model(network, criterion, val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {int(end_time - start_time)} seconds.\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the network\n",
    "nn_3l = NetANN(W=16)\n",
    "\n",
    "# MSE loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = Adam(nn_3l.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 24\n",
    "train_loss, val_loss = fit_model(\n",
    "    nn_3l, criterion, optimizer, train_loader, val_loader, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Visualizing the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "Comparing the training and validation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"Training loss\")\n",
    "plt.plot(val_loss, \"r\", label=\"Validation loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "Since the NN has one input and one output, we can plot it as a function $nn(X)$ (orange), and compare it to the polyfit (blue) and Wilks 2005 polynomial (black dashed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.hist2d(\n",
    "    x_input.flatten(),\n",
    "    X_tend.flatten(),\n",
    "    bins=(np.linspace(-10, 15, 50), np.linspace(-25, 20, 150)),\n",
    "    cmap=plt.cm.Greys,\n",
    ")\n",
    "plt.plot(x, -np.polyval(p18, x), \"k--\", label=\"$P_4(X_k)$ - Wilks, 2005\")\n",
    "plt.plot(x, np.polyval(p, x), label=\"$P_4(X_k)$ lin. regr.\")\n",
    "plt.plot(\n",
    "    x,\n",
    "    (nn_3l(torch.unsqueeze(torch.from_numpy(x.astype(np.float32)), 1))).data.numpy(),\n",
    "    label=\"NN-3L\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X$_k$\")\n",
    "plt.ylabel(\"Missing X$_k$ tendency\")\n",
    "plt.title(\"NN fit\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
