{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Tuning GCM Parameterizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "The objective of this notebook is to show how GCM closures can be tuned in practice. \n",
    "We will assume a specific formulation of a closure and estimate its parameters through a standard optimisation procedure using a procedure similar to Data Assimilation (DA). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "**Resources** : We have used material from Emmanuel Cosme's nice GitHub [repository](https://github.com/ecosme38/Data-Assimilation-Notebooks). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## The GCM Parameterization Problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Here we quickly reintroduce the problem we are trying to solve, our starting point here is {doc}`gcm-parameterization-problem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "# L96 provides the \"real world\", L96_eq1_xdot is the beginning of rhs of X tendency\n",
    "from L96_model import L96, RK2, RK4, EulerFwd, L96_eq1_xdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed gives us reproducible results\n",
    "np.random.seed(13)\n",
    "\n",
    "# Create a \"real world\" with K=8 and J=32\n",
    "W = L96(8, 32, F=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Run \"real world\" for 3 days to forget initial conditons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (store=True save the final state as an initial condition for the next run)\n",
    "W.run(0.05, 3.0, store=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "From here on we can use `W.X` as perfect initial conditions for a model and sample the real world using `W.run(dt,T)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "The class defined below is just a sophisticated version of the single time-scale L96 gcm analogue that was defined in 'The Lorenz-96 GCM Analog' notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCM:\n",
    "    def __init__(self, F, parameterization, time_stepping=EulerFwd):\n",
    "        \"\"\"\n",
    "        GCM with paramerization\n",
    "        Args:\n",
    "            F: forcing\n",
    "            parameterization: function that takes parameters and returns a tendency\n",
    "            time_stepping: time stepping method\n",
    "        \"\"\"\n",
    "        self.F = F\n",
    "        self.parameterization = parameterization\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: state vector\n",
    "            param: parameters of our closure\n",
    "        \"\"\"\n",
    "        return L96_eq1_xdot(X, self.F) - self.parameterization(param, X)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X0: initial conditions\n",
    "            dt: time increment\n",
    "            nt: number of forward steps to take\n",
    "            param: parameters of our closure\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "We illustrate the concepts here with the help of the simple linear regression parameterization, which was also introduced in the {doc}`gcm-analogue`. Remember that this was not a very good parameterization, but it is used to show how imperfect parameterizations can be tuned in different ways. Here we use parameters that were estimated by fitting the linear regression to the true sub-grid tendencies. Such parameter estimates are sometimes also referred to as \"offline fitting\" estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_parameterization = lambda param, X: np.polyval(param, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "F, dt, T = 18, 0.005, 5.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "# We pass the parameters that were obtained by fitting the linear in 'The Lorenz-96 GCM Analog' notebook.\n",
    "X, t = gcm(W.X, dt, int(T / dt), param=[0.85439536, 0.75218026])\n",
    "# We also simulate a case with no parameterization, to show that even the imperfect parameterization is helpful.\n",
    "X_no_param, t = gcm(W.X, dt, int(T / dt), param=[0.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Now we compare the model with the parameterization and trajectories from the \"real world\" truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This samples the real world with the same time interval as \"dt\" used by the model\n",
    "X_true, _, _ = W.run(dt, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(t, X_true[:, 4], label=\"Truth\")\n",
    "plt.plot(t, X[:, 4], label=\"Model with linear param\")\n",
    "plt.plot(t, X_no_param[:, 4], label=\"Model with no param\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The figure above show that the model with linear parameterization outperforms the model with no parameterization. Next we discuss if these parameter estimates can be made even better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Variational estimation of optimal parameters for a predefined closure\n",
    "\n",
    "We will try here to estimate the parameters of `naive_parameterization` with a variational approach. This is different from the way the parameters used above were estimated, which had used an offline fitting approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - assuming the formulation of the parameterization\n",
    "gcm = GCM(F, naive_parameterization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Estimating parameters based on one initial condition and one time step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### Cost function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "What we will be doing here is very close to what is done with classical variational data assimilation, where we try to estimate the state of the parameters of a model through the minimization of a cost function $J$. This is also very close to what is done when parameterizations are encoded as neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We introduce a cost function $J(p)$ which depends on the parameters of the closure. \n",
    "\n",
    "$$J(p) = ||X_p - X_{true}||_{d}$$\n",
    "\n",
    "where $p=[p1,p2]$, $X_p$ is GCM solution computed with with parameters $p$ and $||\\cdot ||_{d}$ is one of the distances above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_initial_tendency(X1, X2):\n",
    "    T1 = X1[1, :] - X1[0, :]\n",
    "    T2 = X2[1, :] - X2[0, :]\n",
    "    return np.sqrt((T1 - T2) ** 2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(param):\n",
    "    F, dt, T = 18, 0.01, 0.01\n",
    "    X_gcm, t = gcm(W.X, dt, int(T / dt), param=param)\n",
    "    return norm_initial_tendency(X_true, X_gcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Minimization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Since the problem dimension is only two ( $p=[p1,p2]$ ), we can use efficient derivative-free optimization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = np.array([0.85439536, 0.75218026])  #  prior\n",
    "res = opt.minimize(cost_function, prior, method=\"Powell\")\n",
    "opt_param = res[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Optimized parameters = {opt_param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### Let's test the closure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "F, dt, T = 18, 0.01, 100.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "X_optimized, t = gcm(W.X, dt, int(T / dt), param=opt_param)\n",
    "X_prior, t = gcm(W.X, dt, int(T / dt), param=prior)\n",
    "\n",
    "# - ... the true state\n",
    "X_true, _, _ = W.run(dt, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(t[:500], X_true[:500, 4], label=\"Truth\")\n",
    "plt.plot(t[:500], X_prior[:500, 4], label=\"Initial GCM\")\n",
    "plt.plot(t[:500], X_optimized[:500, 4], label=\"Optimized GCM\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "The results are better but not great. This problem is related to the question of *a priori* versus *a posteriori* skill in Large Eddy Simulation (LES) closures. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Estimating parameters which optimize longer trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "F, dt, T = 18, 0.01, 5.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "X_true, _, _ = W.run(dt, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gcm, t = gcm(W.X, dt, int(T / dt), param=[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - pointwise distance :\n",
    "def pointwise(X1, X2, L=1.0):\n",
    "    # computed over some window t<L.\n",
    "    D = (X1 - X2)[np.where(t < L)]\n",
    "    return np.sqrt(D**2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(param):\n",
    "    F, dt, T = 18, 0.01, 5\n",
    "    X_gcm, t = gcm(W.X, dt, int(T / dt), param=param)\n",
    "    return pointwise(X_true, X_gcm, L=5.0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = np.array([0.85439536, 0.75218026])  #  prior\n",
    "res = opt.minimize(cost_function, prior, method=\"Powell\")\n",
    "opt_param = res[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Optimized parameters = {opt_param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "#### Let's test the closure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "F, dt, T = 18, 0.01, 100.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "X_optimized, t = gcm(W.X, dt, int(T / dt), param=opt_param)\n",
    "X_prior, t = gcm(W.X, dt, int(T / dt), param=prior)\n",
    "X_true, _, _ = W.run(dt, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(t[:500], X_true[:500, 4], label=\"Truth\")\n",
    "plt.plot(t[:500], X_prior[:500, 4], label=\"Initial GCM\")\n",
    "plt.plot(t[:500], X_optimized[:500, 4], label=\"Optimized GCM\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "The closure produces better results but it is not clear how this would generalize to unseen initial conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Discussion and possible next steps:\n",
    "\n",
    " - Estimating parameters over one time-step but with an ensemble of initial conditions. \n",
    " - Use neural networks to encode closures.\n",
    " - Explore the feasability of differentiable GCMs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
