{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet and Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Uai4mkDXGYx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "\n",
    "# from torch_lr_finder import LRFinder\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from L96_model import L96, L96_eq1_xdot, integrate_L96_2t, EulerFwd, RK2, RK4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "```{figure} https://www.asimovinstitute.org/wp-content/uploads/2019/04/NeuralNetworkZo19High.png\n",
    ":width: 600px\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ud1XdNrKXNSx"
   },
   "source": [
    "[Lorenz (1996)](https://www.ecmwf.int/en/elibrary/10829-predictability-problem-partly-solved) describes a \"two time-scale\" model in two equations (2 and 3) which are:\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F - \\left( \\frac{hc}{b} \\right) \\sum_{j=0}^{J-1} Y_{j,k}\n",
    "\\\\\n",
    "\\frac{d}{dt} Y_{j,k}\n",
    "&= - cbY_{j+1,k} \\left( Y_{j+2,k} - Y_{j-1,k} \\right) - c Y_{j,k} + \\frac{hc}{b} X_k\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f82UE1jAYXYq"
   },
   "source": [
    "```{figure} https://www.researchgate.net/publication/319201436/figure/fig1/AS:869115023589376@1584224577926/Visualisation-of-a-two-scale-Lorenz-96-system-with-J-8-and-K-6-Global-scale-values.png\n",
    ":width: 400px\n",
    "\n",
    "*Visualisation of a two-scale Lorenz '96 system with J = 8 and K = 6. Global-scale variables ($X_k$) are updated based on neighbouring variables and on the local-scale variables ($Y_{j,k}$) associated with the corresponding global-scale variable. Local-scale variabless are updated based on neighbouring variables and the associated global-scale variable. The neighbourhood topology of both local and global-scale variables is circular. Image from [Exploiting the chaotic behaviour of atmospheric models with reconfigurable architectures - Scientific Figure on ResearchGate.](https://www.researchgate.net/figure/Visualisation-of-a-two-scale-Lorenz-96-system-with-J-8-and-K-6-Global-scale-values_fig1_319201436)*.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74cig9YaV-Qq"
   },
   "source": [
    "## Generating data from L96:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjRYqUxXkDwj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_steps = 20000\n",
    "Forcing, dt, T = 18, 0.01, 0.01 * time_steps\n",
    "\n",
    "# Create a \"real world\" with K=8 and J=32\n",
    "W = L96(8, 32, F=Forcing)\n",
    "\n",
    "# Get training data for the neural network.\n",
    "# - Run the true state and output subgrid tendencies (the effect of Y on X is xytrue):\n",
    "X_true, y, t, xy_true = W.run(dt, T, return_coupling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eBi3GvAkCzt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_size = 4000  # number of time steps for validation\n",
    "# train:\n",
    "X_true_train = X_true[\n",
    "    :-val_size, :\n",
    "]  # Flatten because we first use single input as a sample\n",
    "subgrid_tend_train = xy_true[:-val_size, :]\n",
    "\n",
    "# test:\n",
    "X_true_test = X_true[-val_size:, :]\n",
    "subgrid_tend_test = xy_true[-val_size:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UhunGR0i__o",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create non local training data\n",
    "# Define a data loader (8 inputs, 8 outputs)\n",
    "\n",
    "# Define our X,Y pairs (state, subgrid tendency) for the linear regression local network.local_torch_dataset = Data.TensorDataset(\n",
    "torch_dataset = Data.TensorDataset(\n",
    "    torch.from_numpy(X_true_train).double(),\n",
    "    torch.from_numpy(subgrid_tend_train).double(),\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 1024  # Number of sample in each batch\n",
    "\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define a test dataloader (8 inputs, 8 outputs)\n",
    "\n",
    "torch_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(X_true_test).double(), torch.from_numpy(subgrid_tend_test).double()\n",
    ")\n",
    "\n",
    "loader_test = Data.DataLoader(\n",
    "    dataset=torch_dataset_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vLWTQvBSYGE",
    "user_expressions": []
   },
   "source": [
    "\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F - \\left( \\frac{hc}{b} \\right) \\sum_{j=0}^{J-1} Y_{j,k}\n",
    "\\\\\n",
    "\\frac{d}{dt} Y_{j,k}\n",
    "&= - cbY_{j+1,k} \\left( Y_{j+2,k} - Y_{j-1,k} \\right) - c Y_{j,k} + \\frac{hc}{b} X_k\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPCFNuimR23o",
    "user_expressions": []
   },
   "source": [
    "Goal: Use $X_k$ to predict subgrid terms,  $- \\left( \\frac{hc}{b} \\right) \\sum_{j=0}^{J-1} Y_{j,k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IuYac4orcTB",
    "user_expressions": []
   },
   "source": [
    "## Janni's fully connected, 3-layer Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xUUyYZO621N",
    "user_expressions": []
   },
   "source": [
    "```{figure} https://miro.medium.com/max/720/1*VHOUViL8dHGfvxCsswPv-Q.png\n",
    ":width: 400px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRNfi_iTW01J",
    "user_expressions": []
   },
   "source": [
    "K=8 and J=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8CXFv5TfyKs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net_ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_ANN, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs, 16 neurons for first hidden layer\n",
    "        self.linear2 = nn.Linear(16, 16)  # 16 neurons for second hidden layer\n",
    "        self.linear3 = nn.Linear(16, 8)  # 8 outputs\n",
    "\n",
    "    #         self.lin_drop = nn.Dropout(0.1) #regularization method to prevent overfitting.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lv7KfTKqotrx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(net, criterion, trainloader, optimizer):\n",
    "    net.train()\n",
    "    test_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(trainloader):  # for each training step\n",
    "        b_x = Variable(batch_x)  # Inputs\n",
    "        b_y = Variable(batch_y)  # outputs\n",
    "        if (\n",
    "            len(b_x.shape) == 1\n",
    "        ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "            prediction = torch.squeeze(\n",
    "                net(torch.unsqueeze(b_x, 1))\n",
    "            )  # input x and predict based on x\n",
    "        else:\n",
    "            prediction = net(b_x)\n",
    "        loss = criterion(prediction, b_y)  # Calculating loss\n",
    "        optimizer.zero_grad()  # clear gradients for next train\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients to update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAJ8y0lsoyhQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(net, criterion, trainloader, optimizer, text=\"validation\"):\n",
    "    net.eval()  # Evaluation mode (important when having dropout layers)\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(\n",
    "            trainloader\n",
    "        ):  # for each training step\n",
    "            b_x = Variable(batch_x)  # Inputs\n",
    "            b_y = Variable(batch_y)  # outputs\n",
    "            if (\n",
    "                len(b_x.shape) == 1\n",
    "            ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "                prediction = torch.squeeze(\n",
    "                    net(torch.unsqueeze(b_x, 1))\n",
    "                )  # input x and predict based on x\n",
    "            else:\n",
    "                prediction = net(b_x)\n",
    "            loss = criterion(prediction, b_y)  # Calculating loss\n",
    "            test_loss = test_loss + loss.data.numpy()  # Keep track of the loss\n",
    "        test_loss /= len(trainloader)  # dividing by the number of batches\n",
    "    #         print(len(trainloader))\n",
    "    # disabling prints to only show graphs\n",
    "    # print(text + ' loss:',test_loss)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bbQAWn2iI6T",
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()  # MSE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7isegnHh0fa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn_3l = Net_ANN().double()\n",
    "# optimizer = optim.Adam(nn_3l.parameters(), lr=1e-7)\n",
    "# lr_finder = LRFinder(nn_3l, optimizer, criterion)\n",
    "# lr_finder.range_test(loader, end_lr=100, num_iter=200)\n",
    "# lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "# lr_finder.reset() # to reset the model and optimizer to their initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 959
    },
    "id": "o02WYIpQlw57",
    "outputId": "aa5e5add-866c-4d9b-a34e-4110b24f20d8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 20  # Number of epocs\n",
    "optimizer = optim.Adam(nn_3l.parameters(), lr=0.01)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(nn_3l, criterion, loader, optimizer)\n",
    "    train_loss.append(test_model(nn_3l, criterion, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model(nn_3l, criterion, loader_test, optimizer))\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend(fontsize=7)\n",
    "\n",
    "final_losses = np.zeros(4)\n",
    "final_losses[0] = validation_loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "CYoJoMUBp-eA",
    "outputId": "81651f93-e26b-45b1-a0f3-480a4f7f1218",
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds22 = nn_3l(torch.from_numpy(X_true_test[:, :]).double())\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(preds22.detach().numpy()[0:1000, 1], label=\"NN Predicted values\")\n",
    "plt.plot(subgrid_tend_test[:1000, 1], label=\"True values\")\n",
    "\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCwwt6bysNfl",
    "user_expressions": []
   },
   "source": [
    "3-layer ANN is (too) good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Bn_9aswsS45",
    "user_expressions": []
   },
   "source": [
    "## ResNet\n",
    "\n",
    "ResNet is based on residual blocks, depicted below  \n",
    "Residual blocks are based on skip connections  \n",
    "Skip connections add output from one layer to that of a deeper layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgKr2RhCf2Eh",
    "user_expressions": []
   },
   "source": [
    "\n",
    "```{figure} https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/10/22125000/Image-recreation-Sep-15-1-1-640x420.jpg\n",
    ":width: 600px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRAdV1k-sv00",
    "user_expressions": []
   },
   "source": [
    "ResNet is famously applied to *image recognition* using NN that are *very deep*  \n",
    "\n",
    "\n",
    "> He, Kaiming, et al. “Deep Residual Learning for Image Recognition.” ArXiv.org, 2015, arxiv.org/abs/1512.03385. Accessed 5 Oct. 2021.  \n",
    "\n",
    "\n",
    "Pretrained image recognition ResNet models are available with tens or >100 layers  \n",
    "This is not our use case for Lorenz '96 \n",
    "\n",
    "\n",
    "> However, adaptation for dynamic systems do exist, e.g. https://towardsdatascience.com/neural-odes-breakdown-of-another-deep-learning-breakthrough-3e78c7213795\\\\\n",
    "\n",
    "\n",
    "Here, we'll look at skip connections and residual blocks, rather than ResNet   \n",
    "First, need to 'break' Janni's model by adding too many layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHGr85pTuJT4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net_deepNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_deepNN, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs, 16 neurons for first hidden layer\n",
    "        self.linear2 = nn.Linear(16, 16)  # 16 neurons for second hidden layer\n",
    "        self.linear3 = nn.Linear(16, 16)\n",
    "        self.linear4 = nn.Linear(16, 16)\n",
    "        self.linear5 = nn.Linear(16, 16)\n",
    "        self.linear6 = nn.Linear(16, 8)  # 8 outputs\n",
    "\n",
    "    #         self.lin_drop = nn.Dropout(0.1) #regularization method to prevent overfitting.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = F.relu(self.linear4(x))\n",
    "        x = F.relu(self.linear5(x))\n",
    "        x = self.linear6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APC8JO46ntQT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn_deep = Net_deepNN().double()\n",
    "# optimizer = optim.Adam(nn_deep.parameters(), lr=1e-7)\n",
    "# lr_finder = LRFinder(nn_deep, optimizer, criterion)\n",
    "# lr_finder.range_test(loader, end_lr=100, num_iter=200)\n",
    "# lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "# lr_finder.reset() # to reset the model and optimizer to their initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3TmNXy-fn8Qa",
    "outputId": "41d461ba-00ef-4893-ec0f-dd3a12af87a9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 40  # Number of epocs\n",
    "optimizer = optim.Adam(nn_deep.parameters(), lr=0.01)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(nn_deep, criterion, loader, optimizer)\n",
    "    train_loss.append(test_model(nn_deep, criterion, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model(nn_deep, criterion, loader_test, optimizer))\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend(fontsize=7)\n",
    "\n",
    "final_losses[1] = validation_loss[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwJ7ODPuu5ZD",
    "user_expressions": []
   },
   "source": [
    "Worse loss with twice the training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "_8ApZP9GvGu6",
    "outputId": "a48e4b75-0eab-40df-e25a-224f97e93caf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_deep = nn_deep(torch.from_numpy(X_true_test[:, :]).double())\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(preds22.detach().numpy()[0:100, 1], label=\"3 layer Predictions\")\n",
    "plt.plot(subgrid_tend_test[:100, 1], label=\"True values\")\n",
    "plt.plot(preds_deep.detach().numpy()[0:100, 1], label=\"deepNN Predictions\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ma86AJSRnOox",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net_deepNN_withSkips(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_deepNN_withSkips, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs, 16 neurons for first hidden layer\n",
    "        self.linear2 = nn.Linear(16, 16)  # 16 neurons for second hidden layer\n",
    "        self.linear3 = nn.Linear(16, 16)\n",
    "        self.linear4 = nn.Linear(16, 16)\n",
    "        self.linear5 = nn.Linear(16, 16)\n",
    "        self.linear6 = nn.Linear(16, 8)  # 8 outputs\n",
    "\n",
    "    #         self.lin_drop = nn.Dropout(0.1) #regularization method to prevent overfitting.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.linear1(x))\n",
    "        x2 = F.relu(self.linear2(x1))\n",
    "        x3 = F.relu(self.linear3(x2)) + x1\n",
    "        x4 = F.relu(self.linear4(x3))\n",
    "        x5 = F.relu(self.linear5(x4)) + x3\n",
    "        x6 = self.linear6(x5)\n",
    "        return x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2K62ND04zHi0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn_deep_skips = Net_deepNN_withSkips().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 959
    },
    "id": "ziyE32LbzIBk",
    "outputId": "48d05a40-f334-41a9-aaab-89541333bd8a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 20  # Number of epocs\n",
    "optimizer = optim.Adam(nn_deep_skips.parameters(), lr=0.01)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(nn_deep_skips, criterion, loader, optimizer)\n",
    "    train_loss.append(test_model(nn_deep_skips, criterion, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model(nn_deep_skips, criterion, loader_test, optimizer))\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend(fontsize=7)\n",
    "\n",
    "# final_losses=np.append(final_losses,validation_loss[-1])\n",
    "final_losses[2] = validation_loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "aJKvu_mS2WBN",
    "outputId": "447fa50c-c284-484f-cf21-6c0e03ed6e46",
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_deep_skips = nn_deep_skips(torch.from_numpy(X_true_test[:, :]).double())\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(preds22.detach().numpy()[0:100, 1], label=\"3 layer Predictions\")\n",
    "plt.plot(subgrid_tend_test[:100, 1], label=\"True values\")\n",
    "plt.plot(preds_deep.detach().numpy()[0:100, 1], label=\"deepNN Predictions\")\n",
    "plt.plot(\n",
    "    preds_deep_skips.detach().numpy()[0:100, 1], label=\"deepNN w/ skips Predictions\"\n",
    ")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Adding skips led to two improvements  \n",
    "* Improved loss, lower than that of deep network without skips and the original 3-layer NN\n",
    "* Faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuSgIFLHNz6l",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net_3L_withSkip(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_3L_withSkip, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs, 16 neurons for first hidden layer\n",
    "        self.linear2 = nn.Linear(16, 16)  # 16 neurons for second hidden layer\n",
    "        self.linear3 = nn.Linear(16, 8)  # 8 outputs\n",
    "\n",
    "    #         self.lin_drop = nn.Dropout(0.1) #regularization method to prevent overfitting.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x)) + x\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9R5rAiVO8lo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn_3l_skip = Net_3L_withSkip().double()\n",
    "# optimizer = optim.Adam(nn_3l_res.parameters(), lr=1e-7)\n",
    "# lr_finder = LRFinder(nn_3l_res, optimizer, criterion)\n",
    "# lr_finder.range_test(loader, end_lr=100, num_iter=200)\n",
    "# lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "# lr_finder.reset() # to reset the model and optimizer to their initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 959
    },
    "id": "3_Pq6ntZPCdo",
    "outputId": "db458eb8-8746-4466-e8b2-5bb2048d3cb1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 20  # Number of epocs\n",
    "optimizer = optim.Adam(nn_3l_skip.parameters(), lr=0.01)\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "# time0 = time()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_model(nn_3l_skip, criterion, loader, optimizer)\n",
    "    train_loss.append(test_model(nn_3l_skip, criterion, loader, optimizer, \"train\"))\n",
    "    validation_loss.append(test_model(nn_3l_skip, criterion, loader_test, optimizer))\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"validation loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend(fontsize=7)\n",
    "\n",
    "final_losses[3] = validation_loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Va1f2qqwPHg3",
    "outputId": "699e3277-28a7-4559-a4e1-451e13d06426",
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_3l_skip = nn_3l_skip(torch.from_numpy(X_true_test[:, :]).double())\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(preds22.detach().numpy()[0:100, 1], label=\"3 layer Predictions\")\n",
    "plt.plot(subgrid_tend_test[:100, 1], label=\"True values\")\n",
    "plt.plot(preds_deep.detach().numpy()[0:100, 1], label=\"deepNN Predictions\")\n",
    "plt.plot(\n",
    "    preds_deep_skips.detach().numpy()[0:100, 1], label=\"deepNN w/ skips Predictions\"\n",
    ")\n",
    "plt.plot(preds_3l_skip.detach().numpy()[0:100, 1], label=\"3 layer w/ skip Predictions\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "FCGsrysp5_SU",
    "outputId": "4b63680a-2fb8-416e-a4cc-1091b74c944a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_names = [\"3 layer\", \"deep\", \"deep w/ skips\", \"3 layer w/ skip\"]\n",
    "df = pd.DataFrame(final_losses, index=model_names, columns=[\"loss\"])\n",
    "df[\"epochs\"] = [20, 40, 20, 20]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Main points\n",
    " - Skip connections are easy to program in pytorch  \n",
    " - Deeper isn't always better  \n",
    " - Residual structure can lower loss, and lead to faster training  \n",
    " - Even shallow NN can benefit from using a residual block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOjZww_ObQzt",
    "user_expressions": []
   },
   "source": [
    "## Recurrent Neural Networks (RNN)\n",
    "#What is RNN? >  Recurrent Neural Network.\n",
    "The hidden state (hidden layer) is used in the next time step. Useful in operations which involve recurrence. \n",
    "In the image below, $x_t$ is the vector of $X$ and $o_t$ is the vector of tendency terms. The network uses $x_t$ and hidden state $s_{t-1}$ as inputs to obtain the tendency term $o_t$. A normal fully connected ANN will only use $x_t$ to obtain $o_t$ but RNN uses $s_{t-1}$ in addition to $x_t$ to get the output, $o_t$. \n",
    "\n",
    "```{figure} https://upload.wikimedia.org/wikipedia/commons/0/05/RNN.png\n",
    ":width: 600px\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0P4byyMiEE-D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Applying RNN to L96 model:\n",
    "\n",
    "# preparing data for training\n",
    "\n",
    "x1 = torch.Tensor(X_true)  # xk's w.r.t to time\n",
    "x2 = torch.Tensor(xy_true)  # tendency terms\n",
    "\n",
    "\n",
    "class L96_RNN(nn.Module):\n",
    "    def __init__(self, in_n, hid, out_n):\n",
    "        super(L96_RNN, self).__init__()\n",
    "        self.h_size = hid\n",
    "        self.linear1 = nn.Linear(\n",
    "            in_n + hid, hid\n",
    "        )  # note that X_k + hidden state is the input\n",
    "        self.linear2 = nn.Linear(hid, out_n)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # forward code for RNN is different than ANN. Hidden state is also given as input to forward function\n",
    "        x2 = self.linear1(torch.cat((x, hidden), 1))  # input layer to 1st layer\n",
    "        x3 = torch.relu(\n",
    "            x2\n",
    "        )  # x2 is the hidden state which will be used as input in the next iteration\n",
    "        x4 = self.linear2(x3)  # x4 is the output layer, tendency terms in L96 case.\n",
    "\n",
    "        return (\n",
    "            x4,\n",
    "            x3,\n",
    "        )  # x4 is the output layer, and x3 is the hidden layer returned for using in the next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "LhxVdWq4EI6X",
    "outputId": "4fc76fe7-1327-4dbc-812a-79a6f658360b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### deletes the model if it already exists\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "###\n",
    "\n",
    "\n",
    "m = 1  # no. of time steps to use in RNN. untested, code might break if changed.\n",
    "\n",
    "in_n, hid, out_n = (\n",
    "    8,\n",
    "    8,\n",
    "    8,\n",
    ")  # X_k, k=1 to 8 and corresponding tendency terms. hid layer nodes can be chaned\n",
    "\n",
    "model = L96_RNN(in_n, hid, out_n)  # model initialize\n",
    "\n",
    "l_r = 1e-03  # learning rate\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_r)  # using Adam optimiser\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"mean\")  # loss function\n",
    "\n",
    "epochs = 20  # epochs\n",
    "\n",
    "loss_array = np.zeros([epochs, 2])  # to store loss for plotting purposes\n",
    "\n",
    "for p in range(epochs):\n",
    "    for i in range(time_steps - 1):\n",
    "        hidd = Variable(torch.randn((1, model.h_size)))\n",
    "        inp = x1[i : i + m, :].reshape([1, 8])\n",
    "        outp = x2[i : i + m, :].reshape([1, 8])\n",
    "        pred, hidd = model.forward(inp, hidd)\n",
    "        loss = loss_fn(pred, outp)\n",
    "        # loss.backward()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_array[p, 0] = p\n",
    "    loss_array[p, 1] = loss.detach().numpy()\n",
    "    # print('epoch no. >',p, 'loss >',loss.detach().numpy())\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(loss_array[:, 1])\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPhzyyWdEMYA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checking tendency term output from L96 RNN:\n",
    "\n",
    "hidd = Variable(\n",
    "    torch.randn((1, model.h_size))\n",
    ")  # hidden state needs to be initialized for the 1st iteration\n",
    "\n",
    "pred = torch.zeros([time_steps, 8])  # tendency prediction\n",
    "\n",
    "for j in range(time_steps - 1):\n",
    "    inp = x1[j : j + m, :].reshape([1, 8])\n",
    "    pred[j, :], hidd = model.forward(inp, hidd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "fJ3If9ZeFAr5",
    "outputId": "46cf08c1-f80d-4647-c969-5a61385a37ae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "l = 1  # k value. l=k. l goes from 1 to 8 for this example\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(t, x2[:, l], \"-\", label=\"actual\")\n",
    "plt.plot(t[0:-1], pred[:, l].detach().numpy(), \"-\", label=\"RNN\")\n",
    "plt.xlim([0, 10])  # looking only till t=10\n",
    "plt.legend(fontsize=7)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"tendency\")\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ComparingSimpleANN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
