{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# GCM parameterizations, skill metrics, and other sources of uncertainity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "In the last notebook we provided a very quick overview of the problem we run into when trying to simulate a real system (e.g. the atmosphere) on a computer using a General Circulation Model (GCM), even when the exact equations to be solved are known in principle. The problem of limited computational resources translates into an inability to resolve all scales of motion in a GCM, and the unresolved scales need to be parameterized. \n",
    "\n",
    "The objective of this notebook is to introduce some more of the key aspects of parameterizations in GCMs, illustrating the deterministic vs stochastic approaches, the interplay with numerical errors, and how to measure the skill of a parameterization. We also provide a comprehensive discussion of the different sources of errors that are present in GCMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## The need for GCM parameterizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Let's first quickly review some concepts from the last notebook, using a slightly modified framing that might benefit some readers. \n",
    "We will assume from now on that the readers are familiar  with the {cite}`Lorenz1995` two-time scale model and its numerical implementation in the `L96_model` module, which was discussed in {doc}`L96-two-scale-description`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "from L96_model import L96, RK2, RK4, EulerFwd, L96_eq1_xdot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Here, `L96` serves as the **\"real world\"** or two time-scale model, whereas `L96_eq1_xdot` represents the **beginning of rhs of X tendency** or the tendency in the single time-scale model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting the seed gives us reproducible results\n",
    "np.random.seed(13)\n",
    "\n",
    "# Create a \"real world\" with K = 8 and J = 32\n",
    "W = L96(8, 32, F=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Since we start the model with a random initial condition, there is no reason to expect that these initial conditions are an actual solution to the model. These arbitrary states can result in initial shocks to the system, which will are unrealistic features but get dissipated after some time.\n",
    "So we *run the \"real world\" for 3 days in order to forget the initial conditons*, and settle into a state that is an actual solution to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `store=True` saves the final state as an initial condition for the next run.\n",
    "W.run(0.05, 3.0, store=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "From here on we can use `W.X` as perfect initial conditions for a model and sample the \"real world\" using `W.run(dt, T)`.\n",
    "\n",
    "**How to think about the real world vs models:**\n",
    "\n",
    "Let's call $Z(t)$ the trajectory of the full complexity physical system (say planet earth). Because in practice, for computational or observational reasons, we cannot afford describing and predicting $Z(t)$, we will only focus on a projection of $Z(t)$ in some lower dimension space. Let's call this reduced dimension state $X(t)$.\n",
    "\n",
    "In our L96 toy model (analog to the real world),  $Z(t)=(X(t),Y(t))$ is the full complexity physical system, while $X(t)$ is the lower dimension reduction (single time-scale models is analog to the GCM). In real world situations or more complex models (e.g. actual atmosphere or ocean models), the lower dimension representation of the real system may be thought of as a coarse-grained or a subsampled description of the full-scale system.\n",
    "\n",
    "Now, a GCM is simply a numerical machine which intends to predict the trajectory $X(t)$ from knowledge of $X(t=0)$ only. A GCM is generally built from first principle physical laws, by discretizing partial differential equations.\n",
    "\n",
    "In what follows, we therefore assume that we know a fraction of the terms that govern the evolution of $X$. We also assume that we do not know what governs the evolution of $Y$ nor how $Y$ may affect $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**GCM without parameterization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GCM_no_parameterization(X0, F, dt, nt):\n",
    "    \"\"\"GCM without parameterization\n",
    "\n",
    "    Args:\n",
    "        X0: initial conditions\n",
    "        dt: time increment\n",
    "        nt: number of forward steps to take\n",
    "    \"\"\"\n",
    "    time, hist, X = (\n",
    "        dt * np.arange(nt + 1),\n",
    "        np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "        X0.copy(),\n",
    "    )\n",
    "    hist[0] = X\n",
    "\n",
    "    for n in range(nt):\n",
    "        X = X + dt * (L96_eq1_xdot(X, F))\n",
    "        hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "    return hist, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "This GCM is unstable due to Euler forward time stepping scheme, so we don't integrate it for too long and compare it to the real world with the same time interval as `dt` used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "F, dt, T = 18, 0.01, 3.0\n",
    "X, t = GCM_no_parameterization(W.X, F, dt, int(T / dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare with the real world\n",
    "X_true, _, _ = W.run(dt, T)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(t, X_true[:, 4], label=\"Truth (L96)\")\n",
    "plt.plot(t, X[:, 4], label=\"Model (GCM)\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "There are several reasons why the above model (i.e. single time-scale model) differs from truth (i.e. L96 two time-scale model), which will be discussed below in {ref}`other-sources-affect-gcm`. One of these reasons is missing physics.\n",
    "\n",
    "One way, discussed in the previous notebook, to reduce the differences between the Model and the Truth, is to add a *parameterization*: an extra term to the rhs of the Model evolution operator in order to reduce the Model error as compared to the Truth. It may account for missing processes that are present in the truth, but are not included in the reduced model. The missing processed may be a result of unresolved scales (sub-grid processes) or due to physical processes that could not be encoded into the full equations.\n",
    "*Parameterizations* are also commonly refered to as *closures*, in particular when they encode explicit physical assumptions on how non-represented variables (e.g. $Y$) impact represented variables (e.g. $X$). \n",
    "\n",
    "Parameterizations usually involve free parameters that need to be adjusted. The form of the parameterization may be dictated by physical laws, but generally it is unknown as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**GCM with parameterization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we introduce a class to solve for the one time-scale problem,\n",
    "# which can take arbitrary parameterizations and time-stepping schemes as input.\n",
    "class GCM:\n",
    "    \"\"\"GCM with parameterization in rhs of equation for tendency\"\"\"\n",
    "\n",
    "    def __init__(self, F, parameterization, time_stepping=EulerFwd):\n",
    "        self.F = F\n",
    "        self.parameterization = parameterization\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        return L96_eq1_xdot(X, self.F) - self.parameterization(param, X)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X0: initial conditions\n",
    "            dt: time increment\n",
    "            nt: number of forward steps to take\n",
    "            param: parameters of our closure\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "As a first step, we illustrate introducing a polynomial parameterization to GCM and then compare the model to the true trajectories obtained from the real world with the same time interval as `dt` used by the model. This is the same as what was done in the previous notebook, but is shown again for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "naive_parameterization = lambda param, X: np.polyval(param, X)\n",
    "F, dt, T = 18, 0.01, 5.0\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "X, t = gcm(W.X, dt, int(T / dt), param=[0.85439536, 0.75218026])\n",
    "# we use the parameters for the linear polynomial parameterization that were learnt in the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare with the real world\n",
    "X_true, _, _ = W.run(dt, T)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(t, X_true[:, 4], label=\"Truth\")\n",
    "plt.plot(t, X[:, 4], label=\"Model\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "While the GCM with parameterization is better than the GCM without parameterization, it is still not very good at reproducing the true evolution of the full system. It also remains to find the most appropriate coefficients of the polynomial parameterization to make the Model as close as possible to the Truth.\n",
    "\n",
    "In summary, the parameterization problem boils down to defining the functional form and finding the best parameters in order to minimize the distance between the true trajectory and the model trajectory.\n",
    "**With M2LINES, we are approaching this problem as a Machine Learning problem. We want to learn parameterizations from objective measures of their skills through an optimization procedure.**\n",
    "But we are not only interested in learning the parameters of existing functional forms. More generally, we would like to learn the functional forms too.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Should parameterizations  be deterministic or stochastic ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The `naive_parameterization` above has no particular physical nor mathematical justification. Most importantly, it relies on a very strong assumption, that the time rate of change of $X$ at time $t$ is a function of $X(t)$. This assumption implies that the future evolution of the reduced dimension system $X(t)$ is *deterministically* related to the initial reduced dimension state $X(0)$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "But this is not a good assumption because the two identical reduced dimension states ($X$, macro-state) can be associated with very different fine scale states ($Y$, micro-state). This can also be seen visually by considering the [plot](sub-grid-hist-label) in the previous notebook, which shows that the for each value of $X$ there is a range of possible values for the sub-grid effects.\n",
    "Given the non-linearity of the evolution equation for $Z$, the two large scale trajectories will diverge at some point due to these small differences in the un-observed states. Let's illustrate that with L96 alone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Randomising the initial Ys\n",
    "np.random.seed(13)\n",
    "\n",
    "# Duplicating L96 to create perturbed versions that include random perturbations in Y\n",
    "Wp1 = W.copy()\n",
    "Yp1 = W.Y.std() * np.random.rand(Wp1.Y.size)\n",
    "Wp1.set_state(W.X, Yp1)\n",
    "\n",
    "Wp2 = W.copy()\n",
    "Yp2 = W.Y + 0.0001 * np.random.rand(Wp2.Y.size)\n",
    "Wp2.set_state(W.X, Yp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Running L96 and perturbed versions to compare results\n",
    "X_true, _, _ = W.run(dt, T)\n",
    "X_pert1, _, _ = Wp1.run(dt, T)\n",
    "X_pert2, _, _ = Wp2.run(dt, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(t, X_true[:, 4], label=\"Truth\")\n",
    "plt.plot(t, X_pert1[:, 4], label=\"Perturbed 1\")\n",
    "plt.plot(t, X_pert2[:, 4], label=\"Perturbed 2\")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "So even very small uncertainties in the micro-state ($Y$) of L96 can lead to large scale changes (i.e. of the variable $X$) over short time.\n",
    "\n",
    "In a Model that does not know anything about micro-state $Y$, it is possible to introduce this uncertainty through a stochastic form in the parameterization.\n",
    "\n",
    "\n",
    "In addition, with this illustration, we also highlight that there is a horizon after which pointwise comparisons of the model with the truth are meaningless, hence there is some needed discussion on how to measure the skill of a parameterization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## How to measure parameterization skill ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We would like to build our closures by systematically measuring their skills, so that we can compare different fomulations using these \"skill scores\". \n",
    "\n",
    "Since we are interested in matching the evolution of the \"real world\" using a GCM, we define skill scores that measure the distance between the evolution of the true state $X_{true}$ and the simulated state $X_{gcm}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining our GCM\n",
    "F, dt, T = 18, 0.005, 1000.0\n",
    "# Use Runge-Kutta time stepping scheme because forward Euler would lead to instabilities\n",
    "gcm = GCM(F, naive_parameterization, time_stepping=RK4)\n",
    "\n",
    "# Evaluate the GCMs\n",
    "X_gcm, t = gcm(W.X, dt, int(T / dt), param=[0.85439536, 0.75218026])\n",
    "X_gcm_no_param, _ = gcm(W.X, dt, int(T / dt), param=[0, 0])\n",
    "\n",
    "# Evaluate the true state\n",
    "X_true, _, _ = W.run(dt, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(dpi=150)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(t[:500], X_true[:500, 4], label=\"Truth\", color=\"k\")\n",
    "plt.plot(t[:500], X_gcm_no_param[:500, 4], label=\"GCM without param\", color=\"tab:green\")\n",
    "plt.plot(t[:500], X_gcm[:500, 4], label=\"GCM with param\", color=\"tab:orange\")\n",
    "\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4(t)$\")\n",
    "plt.legend(fontsize=7)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(\n",
    "    t[:500],\n",
    "    (X_true[:500, 4] - X_gcm_no_param[:500, 4]),\n",
    "    label=\"GCM without param\",\n",
    "    color=\"tab:green\",\n",
    ")\n",
    "plt.plot(\n",
    "    t[:500],\n",
    "    (X_true[:500, 4] - X_gcm[:500, 4]),\n",
    "    label=\"GCM with param\",\n",
    "    color=\"tab:orange\",\n",
    ")\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_4^{true}(t) - X_4^{gcm}(t)$\")\n",
    "plt.legend(fontsize=7)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**Error metric based on model evolution**:\n",
    "\n",
    "Clearly, the absolute error between the true and gcm trajectory grows with time, and we would like measure how the error cumulates over time.\n",
    "A simple error metric is the point-wise root mean square error, which is averaged over time:\n",
    "\n",
    "\\begin{equation}\n",
    "E (t) = \\frac{1}{t} \\int_0^t |X_{true}(t) - X_{gcm}(t)| dt\n",
    "\\end{equation}\n",
    "\n",
    "This can be computed for each $X_k$ separately, or averaged over all $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def error_model_evolution(X1, X2, L, dt=dt):\n",
    "    \"\"\"Model evolution error computed over some window t < L.\"\"\"\n",
    "    D = np.cumsum(\n",
    "        np.sqrt((X1[: int(L / dt), :] - X2[: int(L / dt), :]) ** 2), axis=0\n",
    "    ) / (np.expand_dims(np.arange(1, int(L / dt) + 1), axis=1))\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist = error_model_evolution(X_true, X_gcm, T)\n",
    "dist_no_param = error_model_evolution(X_true, X_gcm_no_param, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting how this distance grows with the length of the window\n",
    "# for all the components of X\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(\n",
    "    t[1:],\n",
    "    np.mean(dist_no_param, 1),\n",
    "    linewidth=2,\n",
    "    label=\"GCM without param\",\n",
    "    color=\"tab:green\",\n",
    ")\n",
    "plt.plot(\n",
    "    t[1:], np.mean(dist, 1), linewidth=2, label=\"GCM with param\", color=\"tab:orange\"\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"$||X_{true}-X_{gcm}||$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The error grows with time, but saturates to some a constant after the truth and GCM have gotten completely decorrelated. This constant is equal to the sum of the variance of the truth and GCM states. Also, as expected, the error grows much more rapidly without a parameterization, showing that adding the parameterization has resulted in a quantitative improvement in the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**Climatology based error metric:**\n",
    "\n",
    "Knowing from the above discussion that the system is not predictable after some time, we may as well have decided to measure how well the model captures the mean state. With this we expect to measure the \"climate\" of the system instead of the \"weather\". \n",
    "\n",
    "\\begin{equation}\n",
    "E_{clim}  = | \\overline{X_{true}}  - \\overline{X_{gcm}}|\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\overline{(.)}$ is some suitably defined average that may be variable in time (like a seasonal climatology). \n",
    "\n",
    "This metric will likely converge to some non-trivial values which are indicative of how well our model captures the \"climate\" of the system, and the difference indicating the bias in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(np.mean(X_true, 0), marker=\"o\", linewidth=2, color=\"k\", label=\"Truth\")\n",
    "plt.plot(\n",
    "    np.mean(X_gcm_no_param, 0),\n",
    "    marker=\"o\",\n",
    "    linewidth=2,\n",
    "    color=\"tab:green\",\n",
    "    label=\"GCM without param\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.mean(X_gcm, 0),\n",
    "    marker=\"o\",\n",
    "    linewidth=2,\n",
    "    color=\"tab:orange\",\n",
    "    label=\"GCM with param\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"$\\overline{X_{k}}$\")\n",
    "plt.ylim([2, 4])\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(\n",
    "    np.sqrt((np.mean(X_gcm_no_param, 0) - np.mean(X_true, 0)) ** 2),\n",
    "    marker=\"o\",\n",
    "    linewidth=2,\n",
    "    color=\"tab:green\",\n",
    "    label=\"GCM without param\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.sqrt((np.mean(X_gcm, 0) - np.mean(X_true, 0)) ** 2),\n",
    "    marker=\"o\",\n",
    "    linewidth=2,\n",
    "    color=\"tab:orange\",\n",
    "    label=\"GCM with param\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"$E_{clim}$\")\n",
    "plt.ylim([0, 0.8])\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "As seen in the first figure, the long term mean state of the true solution is different from the two GCM solutions. Surprisingly, the GCM without parameterization has a climatology that is slightly closer to the true climatology than the climatology of the GCM with parameterization. In other words, the error $E_{clim}$ is larger for the GCM with parameterization. This is shown in the second figure.\n",
    "\n",
    "The above result is counter-intuitive, and suggests that $E_{clim}$ is not a very powerful metric. This metric takes into account only the means of the underlying probability distributions. Let's look at the full probability distributions next!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "**Distribution based error metric**:\n",
    "\n",
    "We want to measure differences between the empirical distributions of the true solution vs. GCM solution. There are several options how to define distributional differences, but here we consider the earth mover's distance or Wasserstein distance:\n",
    "\\begin{align}\n",
    "E_{distr}  & = \\int_{-\\infty}^\\infty |P(X^{true}\\leq x) - P(X^{GCM}\\leq x) \\, | dx, \\\\\n",
    "\\end{align}\n",
    "where $P(X\\leq x)$ is the cumulative distribution function of $X$.\n",
    "\n",
    "Before we compute the differences between the distribution $E_{distr}$, let's first look at the empirical distributions themselves. For simplicity, we show the probability density functions (rather than the cumulative distribution functions, where the latter are used in the computation of $E_{distr}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "\n",
    "sns.kdeplot(X_true.ravel(), color=\"k\", linewidth=2, label=\"Truth\")\n",
    "plt.axvline(X_true.mean(), color=\"k\", linewidth=1, label=\"_none\")\n",
    "\n",
    "sns.kdeplot(\n",
    "    X_gcm_no_param.ravel(), color=\"tab:green\", linewidth=2, label=\"GCM without param\"\n",
    ")\n",
    "plt.axvline(X_gcm_no_param.mean(), color=\"tab:green\", linewidth=1, label=\"_none\")\n",
    "\n",
    "sns.kdeplot(X_gcm.ravel(), color=\"tab:orange\", linewidth=2, label=\"GCM with param\")\n",
    "plt.axvline(X_gcm.mean(), color=\"tab:orange\", linewidth=1, label=\"_none\")\n",
    "\n",
    "plt.xlabel(\"$X$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "The vertical lines in the figure above indicate the means of the respective empirical distributions. As we saw earlier, the mean of the GCM without parameterization is slightly closer to the real mean than the mean of the GCM with parameterization. However, if we take into account the entire distribution, the GCM with parameterization seems to be a closer match to the true distribution. Our eyeball estimate is quantitatively confirmed by the next figure, which computes the distributional error, $E_{distr}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "\n",
    "def error_distribution(X1, X2):\n",
    "    \"\"\"Distribution error measured by the Wasserstein distance.\"\"\"\n",
    "\n",
    "    diff = wasserstein_distance(X1, X2)\n",
    "\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "E_distr_no_param = [error_distribution(X_true[:, k], X_gcm_no_param[:, k]) for k in W.k]\n",
    "E_distr_param = [error_distribution(X_true[:, k], X_gcm[:, k]) for k in W.k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(\n",
    "    W.k,\n",
    "    E_distr_no_param,\n",
    "    marker=\"o\",\n",
    "    color=\"tab:green\",\n",
    "    label=\"GCM without param\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.plot(\n",
    "    W.k,\n",
    "    E_distr_param,\n",
    "    marker=\"o\",\n",
    "    color=\"tab:orange\",\n",
    "    label=\"GCM with param\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"$E_{distr}$\")\n",
    "plt.ylim([0, 2.1])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**Tendency based error metric**:\n",
    "\n",
    "It is also very common to formulate closures based on databases of initial tendencies.  In the Large Eddy Simulation  community, this is sometimes refered to as *a priori* skill, because you don't need to run the full model to compute it.\n",
    "\n",
    "This is the sort of game that several of us have been playing, trying for instance to estimate subgrid fluxes from knowledge of the large scale quantities  $$ \\nabla\\cdot \\mathbf{s} =   \\nabla\\cdot\\big(\\overline{\\mathbf{u}\\,\\Phi} - \\overline{\\mathbf{u}}\\,\\overline{\\Phi}\\big) \\simeq f(\\overline{\\mathbf{u}},\\overline{\\Phi})$$\n",
    "\n",
    "*Note that this is not exactly the same problem as the `a priori` LES problem, because of the interplay with time-discretization. Let's neglect that for the moment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def norm_initial_tendency(X1, X2):\n",
    "    T1 = X1[1, :] - X1[0, :]\n",
    "    T2 = X2[1, :] - X2[0, :]\n",
    "    return np.sqrt((T1 - T2) ** 2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Because this metric is cheap to evaluate, as we do not need to integrate the GCM more than 1 time-step, we can start a sensitivity analysis in order to identify good optimal parameters for the specific formulation `naive_parameterization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "F, dt = 18, 0.01\n",
    "\n",
    "# Let's define again the true state\n",
    "# But only run for 1 time step\n",
    "X_true, _, _ = W.run(dt, dt)\n",
    "\n",
    "# and an ensemble of trajectories\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "\n",
    "n = 100\n",
    "\n",
    "_p1 = np.linspace(-20, 20, n + 1)\n",
    "_p2 = np.linspace(-20, 20, n + 1)\n",
    "xp1, yp2 = np.meshgrid(_p1, _p2)\n",
    "\n",
    "score = np.zeros((n + 1, n + 1))\n",
    "\n",
    "for i, p1 in enumerate(_p1):\n",
    "    for j, p2 in enumerate(_p2):\n",
    "        X_gcm, t = gcm(W.X, dt, 1, param=[p1, p2])  # run gcm for 1 time step\n",
    "        score[i, j] = norm_initial_tendency(X_true, X_gcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.contourf(xp1, yp2, score)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"$p_2$\")\n",
    "plt.ylabel(\"$p_1$\")\n",
    "plt.title(\"Score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "From this analysis, we see that the optimisation problem is probably well posed as the cost function appears pretty smooth. One can also see that the parameter $p_1$ is more important than $p_2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "(other-sources-affect-gcm)=\n",
    "## Sources of model error\n",
    "\n",
    "Missing physics is not the only source of error in GCMs, and many other factors can contribute to the GCM state diverging from the real world. \n",
    "Here, we consider the following five sources of error:\n",
    "\n",
    "1. Missing physics: which is modeled with a GCM without parameterization that corresponds to the one time-scale Lorenz-96 system without any the coupling term.\n",
    "2. Poorly parameterized unresolved physics: which is studied by considering a first-order and third-order polynomial approximations of the coupling terms:\n",
    "    \\begin{equation*}\n",
    "    P_4 \\rightarrow P_1\n",
    "    \\end{equation*}\n",
    "3. Unknown forcing: which is modeled by adding an error to the forcing term:\n",
    "    \\begin{equation*}\n",
    "    F \\rightarrow F + error\n",
    "    \\end{equation*}\n",
    "4. Numerical approximation: which is studied by increasing the time-step:\n",
    "    \\begin{equation*}\n",
    "    \\Delta t \\rightarrow 10 \\Delta t\n",
    "    \\end{equation*}\n",
    "5. Initialization error: which is modeled by adding an error to the initial condition:\n",
    "    \\begin{equation*}\n",
    "    X(t=0) \\rightarrow X(t=0) + error\n",
    "    \\end{equation*}\n",
    "  \n",
    "The first two sources have already been discussed in some detail previously, and included here to contrast against other error sources.\n",
    "\n",
    "The next code estimates these sources of error and the figure shows their relative contributions. For reference, we also plot the error of the GCM using {cite}`Wilks2005` polynomial coupling term and without any of the sources of error listed above. All errors are evaluated by comparing the GCMs to the \"truth\" model goverened by the full two time-scale Lorenz-96 system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generic GCM parameters\n",
    "F, dt, T = 18, 0.002, 40.0\n",
    "X0 = W.X\n",
    "\n",
    "# Remember the optimized parameters from previous notebook\n",
    "p1 = [0.85439536, 0.75218026]\n",
    "p4 = [\n",
    "    0.000707,\n",
    "    -0.0130,\n",
    "    -0.0190,\n",
    "    1.59,\n",
    "    0.275,\n",
    "]\n",
    "\n",
    "# Sampling real world over a longer period of time\n",
    "X_true, _, t = W.run(dt, T)\n",
    "\n",
    "# GCM with different parameterizations\n",
    "gcm = GCM(F, naive_parameterization)\n",
    "X_no_param, t = gcm(W.X, dt, int(T / dt), param=[0])  # Missing physics\n",
    "X_p1, _ = gcm(W.X, dt, int(T / dt), param=p1)  # Simpler but poorer parameterization\n",
    "X_p4, _ = gcm(W.X, dt, int(T / dt), param=p4)  # More complex parameterization\n",
    "\n",
    "# GCM with perturbed forcing\n",
    "gcm_pert_F = GCM(F + 1.0, naive_parameterization)\n",
    "X_frc, _ = gcm_pert_F(W.X, dt, int(T / dt), param=p4)  # Perturbed forcing\n",
    "\n",
    "# GCM with perturbed IC\n",
    "X_IC, _ = gcm(W.X + 0.5, dt, int(T / dt), param=p4)  # Perturbed IC\n",
    "\n",
    "# GCM with changed dt\n",
    "X_dt, t_dt = gcm(W.X, 5 * dt, int(T / (5 * dt)), param=p4)  # Larged dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "\n",
    "plt.plot(t, X_true[:, 4], label=\"Truth\")\n",
    "plt.plot(t, X_no_param[:, 4], label=\"GCM without param\")\n",
    "plt.plot(t, X_p1[:, 4], label=\"GCM with linear param\")\n",
    "plt.plot(t, X_p4[:, 4], label=\"GCM with 4th order param\")\n",
    "plt.plot(t, X_frc[:, 4], label=\"GCM with perturbed forcing\")\n",
    "plt.plot(t, X_IC[:, 4], label=\"GCM with pertured IC\")\n",
    "plt.plot(t_dt, X_dt[:, 4], label=\"GCM with increased dt\")\n",
    "\n",
    "plt.xlim([0, 4])\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$X_{4}(t)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Clearly perturbing the gcm in different ways results in different solutions. Below we quantify these effects using the `error_model_evolution metric` that was defined earlier. We present the results as an average over all k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dist(X1, X2, T, dt=dt):\n",
    "    return np.mean(error_model_evolution(X1, X2, T, dt), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist_no_param = dist(X_true, X_no_param, T, dt)\n",
    "dist_p1 = dist(X_true, X_p1, T, dt)\n",
    "dist_p4 = dist(X_true, X_p4, T, dt)\n",
    "dist_frc = dist(X_true, X_frc, T, dt)\n",
    "dist_IC = dist(X_true, X_IC, T, dt)\n",
    "dist_dt = dist(X_true[::5], X_dt, T, 5 * dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(t[1:], dist_no_param, label=\"GCM without param\")\n",
    "plt.plot(t[1:], dist_p1, label=\"GCM with linear param\")\n",
    "plt.plot(t[1:], dist_p4, label=\"GCM with 4th order param\")\n",
    "plt.plot(t[1:], dist_frc, label=\"GCM with perturbed forcing\")\n",
    "plt.plot(t[1:], dist_IC, label=\"GCM with pertured IC\")\n",
    "plt.plot(t_dt[1:], dist_dt, label=\"GCM with increased dt\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$E(t)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Under the perturbations considered above, the lack of missing physics contributes the most error to the GCM.  \n",
    "This can be fixed by adding parameterizations, which have different contributions to error at different times. Specifically, the error grows more rapidly for the higher order parameterization relative to the linear parameterization, but saturates to smaller error at longer time. This might partly be because we optimized the parameters for the linear parameterization, but used parameter estimates for the higher order parameterization from the literature.  \n",
    "The second largest error source is the numerical approximation (or changed dt), suggesting that we need to be careful about the design and choice of the numerical schemes.\n",
    "The errors due to the forcing and initial condition reult in smaller errors, but will likely grow as the perturbations to these change. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter:\n",
    "- We first reintroduced the GCM parameterization problem, discussing it in terms of limited observability of GCMs. \n",
    "- Then we discussed the need for stochastic parameterizations, which arises because of the possible differences in the unresolved or hidden states.\n",
    "- Then we discussed how skill can be measured, and defined four different metrics. \n",
    "- Finally we discussed the different sources of error that may be presend in numerical GCMs. \n",
    "\n",
    "This chapter has made it clear that even models with some degree of tuning will diverge from the true solution, as there are multiple sources of errors. In the next chapter we discuss if and how GCMs and their parameterizations can be further tuned to enhance their range of predictability (which is known to not be in finite)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
