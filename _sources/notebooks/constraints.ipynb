{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Adding constraints to Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Outline\n",
    "\n",
    "The goal of this notebook is to show how a physical or mathematical constraint can be built in to the learning process to try and nudge the ML model towards more consistent solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Physical constraints in L96\n",
    "\n",
    "The **$1^{st}$ order equation L96 model**, \n",
    "\\begin{equation}\n",
    "\\frac{d}{dt} X_k\n",
    "= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F \n",
    "\\end{equation}\n",
    "conserves energy, \n",
    "$$E_X = \\frac{1}{2}<X_K^2>,$$ \n",
    "in the unforced and undamped form (last 2 terms on the RHS are zero), which results due to the form of the non-linearity. \n",
    "\n",
    "However, the **$2^{nd}$ order equation L96 model**\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F - \\left( \\frac{hc}{b} \\right) \\sum_{j=0}^{J-1} Y_{j,k}\n",
    "\\\\\n",
    "\\frac{d}{dt} Y_{j,k}\n",
    "&= - cbY_{j+1,k} \\left( Y_{j+2,k} - Y_{j-1,k} \\right) - c Y_{j,k} + \\frac{hc}{b} X_k\n",
    "\\end{align}\n",
    "does not have the same conservation property. Instead, for the unforced and undamped system the total energy, \n",
    "$$E = E_X + E_Y = \\frac{1}{2}(<X_K^2> + <Y_j^2>),$$ \n",
    "is conserved, with the \"subgrid tendencies\" being the exchange terms ($<X_kY_{j,k}>$) between the large scale ($\\frac{1}{2}<X_K^2>$) and small scale energy reservoirs $\\frac{1}{2}<Y_j^2>$. \n",
    "\n",
    "Using energetic arguments to contrain a parameterization is non-trivial, but can prove to be very beneficial. For example the Gent-McWilliams scheme guarantees that the sub-grid parameterization will reduce APE. The GEOMETRIC parameterization keeps track of sub-grid energy and the exchange between the resolved and sub-grid energy, in an attempt to have total energy conservation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## L96 model with momentum conserving term\n",
    "As shown above, the L96 model conserves total energy that is contained over both X and Y variables. However, for demonstration purposes we created a modified L96, where the coupling term is momentum conserving. \n",
    "This is a bit silly to do, as the L96 model does not conserve momentum (but no one can stop you!). This also changes the energetics of the original L96. The main reason for this choice was because it was the easiest to implement. \n",
    "\n",
    "The equations of the new model are: \n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F - \\left( \\frac{hc}{b} \\right) \\left(\\sum_{j=0}^{J-1} Y_{j,k-1} - \\sum_{j=0}^{J-1} Y_{j,k+1}\\right)\n",
    "\\\\\n",
    "\\frac{d}{dt} Y_{j,k}\n",
    "&= - cbY_{j+1,k} \\left( Y_{j+2,k} - Y_{j-1,k} \\right) - c Y_{j,k} + \\frac{hc}{b} X_k\n",
    "\\end{align}\n",
    "\n",
    "The small scale forcing from the small scale to the large scale is:\n",
    "\\begin{equation}\n",
    "G_K = S_{k-1} - S_{k+1} = - \\left( \\frac{hc}{b} \\right) \\left(\\sum_{j=0}^{J-1} Y_{j,k-1} - \\sum_{j=0}^{J-1} Y_{j,k+1}\\right)\n",
    "\\end{equation}\n",
    " \n",
    "It is clear that summing $G_k$ over the full domain is identically zero, since it is in a flux form. This term does not input any net momentum into the system, $<G_k>=0$. We will use this as a contrain in this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules for the model\n",
    "from L96_conservative_model import (\n",
    "    L96,\n",
    "    L96_eq1_xdot,\n",
    "    integrate_L96_2t,\n",
    ")  # L96_model_XYtend Adds the option to ouptput the subgrid tendencies (effect of Y on X)\n",
    "from L96_conservative_model import EulerFwd, RK2, RK4\n",
    "\n",
    "# Note: We are using a slightly different L96 model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams[\"font.size\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the world\n",
    "\n",
    "time_steps = 20000\n",
    "Forcing, dt, T = 12, 0.01, 0.01 * time_steps\n",
    "\n",
    "# Create a \"real world\" with K=8 and J=32\n",
    "K = 8\n",
    "J = 32\n",
    "W = L96(K, J, F=Forcing)\n",
    "\n",
    "# Get training data for the neural network.\n",
    "# - Run the true state and output subgrid tendencies (the effect of Y on X is xytrue):\n",
    "X_true, y, T_true, S_true = W.run(dt, T, store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_k = S_(k-1) - S_(k+1)\n",
    "G_true = np.roll(S_true, 1, axis=1) - np.roll(S_true, -1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to xarray for convenience\n",
    "\n",
    "X_true_xr = xr.DataArray(\n",
    "    X_true, dims=[\"time\", \"K\"], coords={\"time\": T_true, \"K\": np.arange(K)}\n",
    ").rename(\"X\")\n",
    "Y_xr = xr.DataArray(\n",
    "    y, dims=[\"time\", \"JK\"], coords={\"time\": T_true, \"JK\": np.arange(K * J)}\n",
    ").rename(\"Y\")\n",
    "S_true_xr = xr.DataArray(\n",
    "    S_true, dims=[\"time\", \"K\"], coords={\"time\": T_true, \"K\": np.arange(K)}\n",
    ").rename(\"S\")\n",
    "G_true_xr = xr.DataArray(\n",
    "    G_true, dims=[\"time\", \"K\"], coords={\"time\": T_true, \"K\": np.arange(K)}\n",
    ").rename(\"G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10), dpi=150)\n",
    "\n",
    "plt.subplot(141)\n",
    "X_true_xr.sel(time=slice(0, 30)).plot.contourf(vmin=-16, levels=21)\n",
    "\n",
    "plt.subplot(142)\n",
    "Y_xr.sel(time=slice(0, 30)).plot.contourf(vmin=-2, levels=21)\n",
    "\n",
    "plt.subplot(143)\n",
    "G_true_xr.sel(time=slice(0, 30)).plot.contourf(levels=21)\n",
    "\n",
    "plt.subplot(144)\n",
    "S_true_xr.sel(time=slice(0, 30)).plot.contourf(levels=21)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The modified system looks fairly organized and chaotic. So the essence of the original L96 still remains, which is what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3), dpi=150)\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(X_true_xr.sel(K=0), S_true_xr.sel(K=1), \".\", markersize=1)\n",
    "plt.xlabel(\"$X_{k-1}$\")\n",
    "plt.ylabel(\"$S_{k}$\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(X_true_xr.sel(K=1), S_true_xr.sel(K=1), \".\", markersize=1)\n",
    "plt.xlabel(\"$X_{k}$\")\n",
    "plt.ylabel(\"$S_{k}$\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(X_true_xr.sel(K=2), S_true_xr.sel(K=1), \".\", markersize=1)\n",
    "plt.xlabel(\"$X_{k+1}$\")\n",
    "plt.ylabel(\"$S_{k}$\")\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3), dpi=150)\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(X_true_xr.sel(K=0), G_true_xr.sel(K=1), \".\", markersize=1)\n",
    "plt.xlabel(\"$X_{k-1}$\")\n",
    "plt.ylabel(\"$G_{k}$\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(X_true_xr.sel(K=1), G_true_xr.sel(K=1), \".\", markersize=1)\n",
    "plt.xlabel(\"$X_{k}$\")\n",
    "plt.ylabel(\"$G_{k}$\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(X_true_xr.sel(K=2), G_true_xr.sel(K=1), \".\", markersize=1)\n",
    "plt.xlabel(\"$X_{k+1}$\")\n",
    "plt.ylabel(\"$G_{k}$\")\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Since the dependence of Y on X remains the same, G now depends on the neighbouring points (and not the K itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4), dpi=150)\n",
    "plt.subplot(121)\n",
    "S_true_xr.mean(\"K\").plot()\n",
    "plt.ylabel(\"$<S>$\")\n",
    "# plt.title('Averaged subgrid term')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(122)\n",
    "G_true_xr.mean(\"K\").plot()\n",
    "plt.ylabel(\"$<G>$\")\n",
    "# plt.title('Averaged subgrid term')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The momentum input from the subgrid term is essentially zero (to numerical precision), as should be. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**What should we learn using ML?**  \n",
    "We will do three types of learning (all fully non-local):  \n",
    "a) Learn $G_k$ directly as a function of $X_k$s by minimizing a MSE loss.  \n",
    "b) Learn $G_k$ directly as a function of $X_k$s by minimizing a MSE loss + minimizing $<G_k>$.   \n",
    "c) Learn $S_k$ as a function of $X_k$s, and then $<G_k>$ will be zero by design.  \n",
    "d) Learn $S_k$ but with $G_k$ being the goal. (This will introduce the Gauge problem).\n",
    "\n",
    "*We chose MSE as a loss function because as shown earlier, the learning for L96 is not sensitive to the choice of loss function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split into Training and testing data\n",
    "\n",
    "val_size = 4000  # number of time steps for validation\n",
    "\n",
    "# train:\n",
    "X_true_train = X_true[\n",
    "    :-val_size, :\n",
    "]  # Flatten because we first use single input as a sample\n",
    "subgrid_G_tend_train = G_true[:-val_size, :]\n",
    "subgrid_S_tend_train = S_true[:-val_size, :]\n",
    "\n",
    "# test:\n",
    "X_true_test = X_true[-val_size:, :]\n",
    "subgrid_G_tend_test = G_true[-val_size:, :]\n",
    "subgrid_S_tend_test = S_true[-val_size:, :]\n",
    "\n",
    "# Create non local training data\n",
    "# Define a data loader (8 inputs, 8 outputs)\n",
    "\n",
    "### For the cases where will learn G_k\n",
    "\n",
    "# Define our X,Y pairs (state, subgrid tendency) for the linear regression local network.local_torch_dataset = Data.TensorDataset(\n",
    "torch_dataset = Data.TensorDataset(\n",
    "    torch.from_numpy(X_true_train).double(),\n",
    "    torch.from_numpy(subgrid_G_tend_train).double(),\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 1024  # Number of sample in each batch\n",
    "\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define a test dataloader (8 inputs, 8 outputs)\n",
    "\n",
    "torch_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(X_true_test).double(),\n",
    "    torch.from_numpy(subgrid_G_tend_test).double(),\n",
    ")\n",
    "\n",
    "loader_test = Data.DataLoader(\n",
    "    dataset=torch_dataset_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# For the cases where will learn S_k\n",
    "\n",
    "# Define our X,Y pairs (state, subgrid tendency) for the linear regression local network.local_torch_dataset = Data.TensorDataset(\n",
    "torch_dataset = Data.TensorDataset(\n",
    "    torch.from_numpy(X_true_train).double(),\n",
    "    torch.from_numpy(subgrid_S_tend_train).double(),\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 1024  # Number of sample in each batch\n",
    "\n",
    "loader_S = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define a test dataloader (8 inputs, 8 outputs)\n",
    "\n",
    "torch_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(X_true_test).double(),\n",
    "    torch.from_numpy(subgrid_S_tend_test).double(),\n",
    ")\n",
    "\n",
    "loader_S_test = Data.DataLoader(\n",
    "    dataset=torch_dataset_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the neural network architecture\n",
    "class Net_ANN(nn.Module):\n",
    "    def __init__(self, name=\"Default\"):\n",
    "        super(Net_ANN, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs, 16 neurons for first hidden layer\n",
    "        self.linear2 = nn.Linear(16, 16)  # 16 neurons for second hidden layer\n",
    "        self.linear3 = nn.Linear(16, 8)  # 8 outputs\n",
    "        self.name = name\n",
    "\n",
    "    #         self.lin_drop = nn.Dropout(0.1) #regularization method to prevent overfitting.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup function to take one step in the learning\n",
    "def train_model(net, criterion, trainloader, optimizer):\n",
    "    # set model in to training mode\n",
    "    net.train()\n",
    "\n",
    "    # Loop through all the training subsets (notice we batched in size of 1024)\n",
    "    for step, (batch_x, batch_y) in enumerate(trainloader):  # for each training step\n",
    "        b_x = Variable(batch_x)  # Inputs\n",
    "        b_y = Variable(batch_y)  # outputs\n",
    "\n",
    "        if (\n",
    "            len(b_x.shape) == 1\n",
    "        ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "            prediction = torch.squeeze(\n",
    "                net(torch.unsqueeze(b_x, 1))\n",
    "            )  # input x and predict based on x\n",
    "        else:\n",
    "            prediction = net(b_x)\n",
    "\n",
    "        loss = criterion(prediction, b_y)  # Calculating loss\n",
    "        optimizer.zero_grad()  # clear gradients for next train\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients to update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup function that evaluates a number of metrics at each training step\n",
    "def test_model(\n",
    "    net, criterion, metric_fun, trainloader, optimizer, text=\"validation\", print_flag=0\n",
    "):\n",
    "    net.eval()  # Evaluation mode (important when having dropout layers)\n",
    "\n",
    "    test_loss = 0\n",
    "    test_metric = 0\n",
    "    test_metric_2 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(\n",
    "            trainloader\n",
    "        ):  # for each training step\n",
    "            b_x = Variable(batch_x)  # Inputs\n",
    "            b_y = Variable(batch_y)  # outputs\n",
    "            if (\n",
    "                len(b_x.shape) == 1\n",
    "            ):  # If is needed to add a dummy dimension if our inputs are 1D (where each number is a different sample)\n",
    "                prediction = torch.squeeze(\n",
    "                    net(torch.unsqueeze(b_x, 1))\n",
    "                )  # input x and predict based on x\n",
    "            else:\n",
    "                prediction = net(b_x)\n",
    "            loss = criterion(prediction, b_y)  # Calculating loss\n",
    "            metric = metric_fun(prediction, b_y)\n",
    "\n",
    "            test_loss = test_loss + loss.data.numpy()  # Keep track of the loss\n",
    "            test_metric = test_metric + metric.data.numpy()  # Keep track of the loss\n",
    "            test_metric_2 = (\n",
    "                test_metric_2\n",
    "                + torch.mean(torch.abs(torch.mean(prediction, axis=1))).data.numpy()\n",
    "            )\n",
    "\n",
    "        test_loss /= len(trainloader)  # dividing by the number of batches\n",
    "        test_metric /= len(trainloader)  # dividing by the number of batches\n",
    "        test_metric_2 /= len(trainloader)\n",
    "    #         print(len(trainloader))\n",
    "    # print(test_loss)\n",
    "\n",
    "    if print_flag == 1:\n",
    "        print(\n",
    "            f\"{net.name} {text} loss: {test_loss}, metric_1: {test_metric}, metric_2: {test_metric_2}\"\n",
    "        )\n",
    "\n",
    "    return test_loss, test_metric, test_metric_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ML models\n",
    "\n",
    "lr = 0.001  # the learning rate\n",
    "\n",
    "# (a) Learn G_k using MSE\n",
    "criterion_a = torch.nn.MSELoss()\n",
    "nn_a = Net_ANN(\"Learn Gk\").double()\n",
    "optimizer_a = optim.Adam(nn_a.parameters(), lr=lr)\n",
    "\n",
    "# (b) learn G_k using MSE + condition of reducing momentum forcing\n",
    "\n",
    "alpha = 100  # This is the regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(Ypred, Ytrue):\n",
    "    # loss = MSE + net momentum forcing penalty\n",
    "    loss = torch.mean((Ypred - Ytrue) ** 2) + alpha * torch.mean(\n",
    "        torch.abs(torch.mean(Ypred, axis=1))\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_b = custom_loss\n",
    "nn_b = Net_ANN(\"Learn Gk w/ constrain\").double()\n",
    "optimizer_b = optim.Adam(nn_b.parameters(), lr=lr)\n",
    "\n",
    "# (c) Learn S_k using MSE\n",
    "criterion_c = torch.nn.MSELoss()\n",
    "nn_c = Net_ANN(\"Learn Sk\").double()\n",
    "optimizer_c = optim.Adam(nn_c.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d) Learn S_k but with G_k as the objective\n",
    "def custom_loss2(Spred, Gtrue):\n",
    "    # Gtrue = np.roll(Strue, 1, axis=1) - np.roll(Strue, -1, axis=1)\n",
    "    Gpred = torch.roll(Spred, 1, dims=1) - torch.roll(Spred, -1, dims=1)\n",
    "    loss = torch.mean((Gpred - Gtrue) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_d = custom_loss2\n",
    "nn_d = Net_ANN(\"Learn Sk from Gk\").double()\n",
    "optimizer_d = optim.Adam(nn_d.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The custom loss function: \n",
    "$$\n",
    "loss = (G_{pred} - G_{true})^2 + \\alpha |<G_k>|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "In model (b) we have to pick a value for the degree of regularization $\\alpha$. Here we use a constant value, but this value itself needed to be determined. We determined it using some rough testing with the L-curve criterion (read about it more [here](https://www.sintef.no/globalassets/project/evitameeting/2005/lcurve.pdf)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the training\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "# Containers\n",
    "validation_stat_a = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "train_stat_a = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "\n",
    "validation_stat_b = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "train_stat_b = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "\n",
    "validation_stat_c = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "train_stat_c = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "\n",
    "validation_stat_d = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}\n",
    "train_stat_d = {\n",
    "    \"loss\": np.zeros((n_epochs)),\n",
    "    \"metric\": np.zeros((n_epochs)),\n",
    "    \"metric2\": np.zeros((n_epochs)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_fun(nn_sel, criterion, metric, loader, loader_test, optimizer):\n",
    "    train_model(nn_sel, criterion, loader, optimizer)\n",
    "\n",
    "    return test_model(\n",
    "        nn_sel, criterion, metric, loader, optimizer, text=\"train\"\n",
    "    ), test_model(nn_sel, criterion, metric, loader_test, optimizer, print_flag=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "    # print(\"--------------\")\n",
    "    # print(\"Epoch num : \" + str(epoch))\n",
    "\n",
    "    # (a)\n",
    "    (\n",
    "        validation_stat_a[\"loss\"][epoch],\n",
    "        validation_stat_a[\"metric\"][epoch],\n",
    "        validation_stat_a[\"metric2\"][epoch],\n",
    "    ), (\n",
    "        train_stat_a[\"loss\"][epoch],\n",
    "        train_stat_a[\"metric\"][epoch],\n",
    "        train_stat_a[\"metric2\"][epoch],\n",
    "    ) = helper_fun(\n",
    "        nn_a, criterion_a, criterion_a, loader, loader_test, optimizer_a\n",
    "    )\n",
    "\n",
    "    # (b)\n",
    "    (\n",
    "        validation_stat_b[\"loss\"][epoch],\n",
    "        validation_stat_b[\"metric\"][epoch],\n",
    "        validation_stat_b[\"metric2\"][epoch],\n",
    "    ), (\n",
    "        train_stat_b[\"loss\"][epoch],\n",
    "        train_stat_b[\"metric\"][epoch],\n",
    "        train_stat_b[\"metric2\"][epoch],\n",
    "    ) = helper_fun(\n",
    "        nn_b, criterion_b, criterion_a, loader, loader_test, optimizer_b\n",
    "    )\n",
    "\n",
    "    # (c)\n",
    "    (\n",
    "        validation_stat_c[\"loss\"][epoch],\n",
    "        validation_stat_c[\"metric\"][epoch],\n",
    "        validation_stat_c[\"metric2\"][epoch],\n",
    "    ), (\n",
    "        train_stat_c[\"loss\"][epoch],\n",
    "        train_stat_c[\"metric\"][epoch],\n",
    "        train_stat_c[\"metric2\"][epoch],\n",
    "    ) = helper_fun(\n",
    "        nn_c, criterion_c, criterion_a, loader_S, loader_S_test, optimizer_c\n",
    "    )\n",
    "\n",
    "    # (d)\n",
    "    # notice that this learns Sk (what is predicted), but the MSE is relative to Gk.\n",
    "    (\n",
    "        validation_stat_d[\"loss\"][epoch],\n",
    "        validation_stat_d[\"metric\"][epoch],\n",
    "        validation_stat_d[\"metric2\"][epoch],\n",
    "    ), (\n",
    "        train_stat_d[\"loss\"][epoch],\n",
    "        train_stat_d[\"metric\"][epoch],\n",
    "        train_stat_d[\"metric2\"][epoch],\n",
    "    ) = helper_fun(\n",
    "        nn_d, criterion_d, criterion_d, loader, loader_test, optimizer_d\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5), dpi=150)\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "ax.plot(range(n_epochs), train_stat_a[\"metric\"], label=nn_a.name)\n",
    "ax.plot(range(n_epochs), train_stat_b[\"metric\"], label=nn_b.name)\n",
    "ax.plot(range(n_epochs), train_stat_c[\"metric\"], label=nn_c.name)\n",
    "ax.plot(range(n_epochs), train_stat_d[\"metric\"], label=nn_d.name)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "# ax.set_xlim([0, 20])\n",
    "ax.set_ylabel(\"MSE in $G$ or $S$\")\n",
    "ax.set_xlabel(\"Epoch number\")\n",
    "ax.set_title(\"Training\")\n",
    "\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "ax.plot(range(n_epochs), validation_stat_a[\"metric\"], label=nn_a.name)\n",
    "ax.plot(range(n_epochs), validation_stat_b[\"metric\"], label=nn_b.name)\n",
    "ax.plot(range(n_epochs), validation_stat_c[\"metric\"], label=nn_c.name)\n",
    "ax.plot(range(n_epochs), validation_stat_d[\"metric\"], label=nn_d.name)\n",
    "plt.yscale(\"log\")\n",
    "ax.legend(fontsize=7)\n",
    "ax.set_xlabel(\"Epoch number\")\n",
    "# ax.set_xlim([0, 20])\n",
    "ax.set_title(\"Validation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Looking at the plots above we see that the training went well for all the cases, as the loss function is reduced. Note tht we should not compare the MSE of the cases where G_k is learnt with the cases where S_k is learnt, since the learning objective was different. \n",
    "\n",
    "We also see that adding a constrain to minimize net momentum input when learning G_k, made the MSE for the model a bit higher, as it is trying to reduce two things at once. \n",
    "\n",
    "It is interesting to note that even the model without the constraint is trying to reduce the momentum input over training epochs, which shows that it is something that the learning is trying to learn without help. (there might be situations with limited data where this help is important, but not super helpful for L96)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do the models that directly learn Gk do at mom cons of that term.\n",
    "fig = plt.figure(dpi=150)\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(range(n_epochs), train_stat_a[\"metric2\"], label=nn_a.name + \" Train\")\n",
    "ax.plot(range(n_epochs), train_stat_b[\"metric2\"], label=nn_b.name + \" Train\")\n",
    "\n",
    "ax.plot(range(n_epochs), validation_stat_a[\"metric2\"], \"--\", label=nn_a.name + \" Test\")\n",
    "ax.plot(range(n_epochs), validation_stat_b[\"metric2\"], \"--\", label=nn_b.name + \" Test\")\n",
    "ax.legend(fontsize=7)\n",
    "plt.yscale(\"log\")\n",
    "# ax.set_xlim([0, 20])\n",
    "ax.set_ylabel(\"$<G_k>$\")\n",
    "ax.set_xlabel(\"Epoch number\")\n",
    "ax.set_title(\"Training\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at predictions\n",
    "nn_a.eval()\n",
    "nn_b.eval()\n",
    "nn_c.eval()\n",
    "nn_d.eval()\n",
    "\n",
    "pred_G_a = nn_a(torch.from_numpy(X_true_test)).detach().numpy()\n",
    "pred_G_b = nn_b(torch.from_numpy(X_true_test)).detach().numpy()\n",
    "pred_S_c = nn_c(torch.from_numpy(X_true_test)).detach().numpy()\n",
    "pred_G_c = np.roll(pred_S_c, 1, axis=1) - np.roll(\n",
    "    pred_S_c, -1, axis=1\n",
    ")  # Since the last model predicts the S, we need to calculate G.\n",
    "pred_S_d = nn_d(torch.from_numpy(X_true_test)).detach().numpy()\n",
    "pred_G_d = np.roll(pred_S_d, 1, axis=1) - np.roll(\n",
    "    pred_S_d, -1, axis=1\n",
    ")  # Since the last model predicts the S, we need to calculate G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "k = 2\n",
    "\n",
    "T_test = T_true[-val_size:]\n",
    "\n",
    "plt.figure(figsize=(10, 4), dpi=150)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(T_test[:n], subgrid_G_tend_test[:n, k], label=\"True\")\n",
    "plt.plot(T_test[:n], pred_G_a[:n, k], label=nn_a.name)\n",
    "plt.plot(T_test[:n], pred_G_b[:n, k], label=nn_b.name)\n",
    "plt.plot(T_test[:n], pred_G_c[:n, k], label=nn_b.name)\n",
    "plt.plot(T_test[:n], pred_G_d[:n, k], label=nn_b.name)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"$G_k$\")\n",
    "\n",
    "plt.legend(fontsize=7)\n",
    "\n",
    "plt.subplot(122)\n",
    "# plt.plot(Xtrue_test[:,k],  subgrid_tend_test[:,k], '.', markersize=1)\n",
    "plt.plot(\n",
    "    subgrid_G_tend_test[:, k], pred_G_a[:, k], \".\", markersize=2.5, label=nn_a.name\n",
    ")\n",
    "plt.plot(\n",
    "    subgrid_G_tend_test[:, k], pred_G_b[:, k], \".\", markersize=2.5, label=nn_b.name\n",
    ")\n",
    "plt.plot(\n",
    "    subgrid_G_tend_test[:, k], pred_G_c[:, k], \".\", markersize=2.5, label=nn_c.name\n",
    ")\n",
    "plt.plot(\n",
    "    subgrid_G_tend_test[:, k], pred_G_d[:, k], \".\", markersize=2.5, label=nn_c.name\n",
    ")\n",
    "\n",
    "plt.xlabel(\"True G\")\n",
    "plt.ylabel(\"Predicted G\")\n",
    "plt.legend(fontsize=7)\n",
    "# plt.plot(Xtrue_test[:,k],  pred_2.detach().numpy()[:,k], '.', markersize=1.5)\n",
    "# plt.plot(Xtrue_test[:,k],  pred_3.detach().numpy()[:,k], '.', markersize=1.5)\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_a = np.mean((subgrid_G_tend_test - pred_G_a) ** 2)\n",
    "MSE_b = np.mean((subgrid_G_tend_test - pred_G_b) ** 2)\n",
    "MSE_c = np.mean((subgrid_G_tend_test - pred_G_c) ** 2)\n",
    "MSE_d = np.mean((subgrid_G_tend_test - pred_G_d) ** 2)\n",
    "\n",
    "MSE_S_c = np.mean((subgrid_S_tend_test - pred_S_c) ** 2)\n",
    "MSE_S_d = np.mean((subgrid_S_tend_test - pred_S_d) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_names = [nn_a.name, nn_b.name, nn_c.name, nn_d.name]\n",
    "MSEs = [MSE_a, MSE_b, MSE_c, MSE_d]\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.bar(nn_names, MSEs)\n",
    "plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment=\"right\")\n",
    "plt.ylabel(\"MSE in $G_k$\");\n",
    "# plt.xlabel('Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_names = [nn_c.name, nn_d.name]\n",
    "MSEs = [MSE_S_c, MSE_S_d]\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.bar(nn_names, MSEs)\n",
    "plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment=\"right\")\n",
    "plt.ylabel(\"MSE in $S_k$\");\n",
    "# plt.xlabel('Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4), dpi=150)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(T_test, np.mean(subgrid_G_tend_test, axis=1), \"--\", label=\"True\")\n",
    "plt.plot(T_test, np.mean(pred_G_a, axis=1), label=nn_a.name)\n",
    "plt.plot(T_test, np.mean(pred_G_b, axis=1), label=nn_b.name)\n",
    "plt.plot(T_test, np.mean(pred_G_c, axis=1), \"-.\", label=nn_c.name)\n",
    "plt.plot(T_test, np.mean(pred_G_d, axis=1), \"-.\", label=nn_c.name)\n",
    "plt.xlim([160, 175])\n",
    "plt.legend(fontsize=7)\n",
    "plt.ylabel(\"$<G_k>$\")\n",
    "plt.xlabel(\"Time step\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(0)\n",
    "plt.hist(np.mean(pred_G_a, axis=1), density=True, label=nn_a.name)\n",
    "plt.hist(np.mean(pred_G_b, axis=1), density=True, label=nn_b.name)\n",
    "\n",
    "plt.legend(fontsize=7)\n",
    "\n",
    "plt.xlabel(\"$<G_k>$\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.xlim([-0.2, 0.2])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "- The models do quite a good job (visually) at capturing the patterns in the subgrid forcing. \n",
    "- The model with the constrain of momentum conservation does a better job of it (pdf width is much smaller), BUT some bias is introduced (median is shifted from 0). \n",
    "- From our experimenting this bias is not always the same, and can vary if trainined multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4), dpi=150)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(T_test, np.mean(subgrid_S_tend_test, axis=1), \"--\", label=\"True\")\n",
    "plt.plot(T_test, np.mean(pred_S_c, axis=1), \"-.\", label=nn_c.name)\n",
    "plt.plot(T_test, np.mean(pred_S_d, axis=1), \"-.\", label=nn_d.name)\n",
    "plt.xlim([160, 175])\n",
    "plt.legend(fontsize=7)\n",
    "plt.ylabel(\"$<S_k>$\")\n",
    "plt.xlabel(\"Time step\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(0, 0)\n",
    "plt.hist(np.mean(pred_S_c, axis=1), density=True, label=nn_c.name)\n",
    "plt.hist(np.mean(pred_S_d, axis=1), density=True, label=nn_d.name)\n",
    "\n",
    "plt.legend(fontsize=7)\n",
    "\n",
    "plt.xlabel(\"$<S_k>$\")\n",
    "plt.ylabel(\"PDF\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**When the model learns Sk from Gk, without knowing anything about Gk. It produces the right Gk, but a Sk that looks very different from the real Sk.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Skill of different ML models in prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - a GCM class including a neural network parameterization in rhs of equation for tendency\n",
    "\n",
    "time_method = RK4\n",
    "\n",
    "# - a GCM class without any parameterization\n",
    "\n",
    "\n",
    "class GCM_no_param:\n",
    "    def __init__(self, F, time_stepping=time_method):\n",
    "        self.F = F\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        return L96_eq1_xdot(X, self.F)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCM_network:\n",
    "    def __init__(self, F, network, time_stepping=time_method):\n",
    "        self.F = F\n",
    "        self.network = network\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        if self.network.linear1.in_features == 1:\n",
    "            X_torch = torch.from_numpy(X).double()\n",
    "            X_torch = torch.unsqueeze(X_torch, 1)\n",
    "        else:\n",
    "            X_torch = torch.from_numpy(np.expand_dims(X, 0)).double()\n",
    "\n",
    "        return L96_eq1_xdot(X, self.F) + np.squeeze(\n",
    "            self.network(X_torch).data.numpy()\n",
    "        )  # Adding NN parameterization\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, histX, histB, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        histX[0] = X\n",
    "        histB[0] = 0.0\n",
    "\n",
    "        for n in range(nt):\n",
    "            # this next if statement is being called twice\n",
    "            if self.network.linear1.in_features == 1:\n",
    "                X_torch = torch.from_numpy(X).double()\n",
    "                X_torch = torch.unsqueeze(X_torch, 1)\n",
    "            else:\n",
    "                X_torch = torch.from_numpy(np.expand_dims(X, 0)).double()\n",
    "\n",
    "            histB[n + 1] = np.squeeze(self.network(X_torch).data.numpy())\n",
    "\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            histX[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return histX, time, histB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCM_network_S:\n",
    "    def __init__(self, F, network, time_stepping=time_method):\n",
    "        self.F = F\n",
    "        self.network = network\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        if self.network.linear1.in_features == 1:\n",
    "            X_torch = torch.from_numpy(X).double()\n",
    "            X_torch = torch.unsqueeze(X_torch, 1)\n",
    "        else:\n",
    "            X_torch = torch.from_numpy(np.expand_dims(X, 0)).double()\n",
    "\n",
    "            predS = self.network(X_torch).data.numpy()\n",
    "            predG = np.roll(predS, 1, axis=1) - np.roll(predS, -1, axis=1)\n",
    "        # return L96_eq1_xdot(X, self.F) + np.squeeze(self.network(X_torch).data.numpy()) # Adding NN parameterization\n",
    "        return L96_eq1_xdot(X, self.F) + np.squeeze(predG)  # Adding NN parameterization\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        # X0 - initial conditions, dt - time increment, nt - number of forward steps to take\n",
    "        # param - parameters of our closure\n",
    "        time, histX, histB, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        histX[0] = X\n",
    "        histB[0] = 0.0\n",
    "\n",
    "        for n in range(nt):\n",
    "            # this next if statement is being called twice\n",
    "            if self.network.linear1.in_features == 1:\n",
    "                X_torch = torch.from_numpy(X).double()\n",
    "                X_torch = torch.unsqueeze(X_torch, 1)\n",
    "            else:\n",
    "                X_torch = torch.from_numpy(np.expand_dims(X, 0)).double()\n",
    "\n",
    "            predS = self.network(X_torch).data.numpy()\n",
    "            predG = np.roll(predS, 1, axis=1) - np.roll(predS, -1, axis=1)\n",
    "            histB[n + 1] = predG  # np.squeeze(self.network(X_torch).data.numpy())\n",
    "\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            histX[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return histX, time, histB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test = 20\n",
    "\n",
    "X_full, _, t, S_full = W.run(dt, T_test)  # Full model\n",
    "G_full = np.roll(S_full, 1, axis=1) - np.roll(S_full, -1, axis=1)\n",
    "init_cond = X_true[-1, :]\n",
    "\n",
    "gcm_a = GCM_network(Forcing, nn_a)\n",
    "X_a, t, G_a = gcm_a(init_cond, dt, int(T_test / dt))\n",
    "\n",
    "gcm_b = GCM_network(Forcing, nn_b)\n",
    "X_b, t, G_b = gcm_b(init_cond, dt, int(T_test / dt))\n",
    "\n",
    "gcm_c = GCM_network_S(Forcing, nn_c)\n",
    "X_c, t, G_c = gcm_c(init_cond, dt, int(T_test / dt))\n",
    "\n",
    "gcm_d = GCM_network_S(Forcing, nn_d)\n",
    "X_d, t, G_d = gcm_d(init_cond, dt, int(T_test / dt))\n",
    "\n",
    "\n",
    "gcm_no_param = GCM_no_param(Forcing)\n",
    "X_no_param, t = gcm_no_param(init_cond, dt, int(T_test / dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to xarray for convenience\n",
    "X_full_xr = xr.DataArray(\n",
    "    X_full, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"X_true\")\n",
    "G_full_xr = xr.DataArray(\n",
    "    G_full, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"G_true\")\n",
    "\n",
    "X_a_xr = xr.DataArray(\n",
    "    X_a, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"X_a\")\n",
    "G_a_xr = xr.DataArray(\n",
    "    G_a, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"G_a\")\n",
    "\n",
    "X_b_xr = xr.DataArray(\n",
    "    X_b, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"X_b\")\n",
    "G_b_xr = xr.DataArray(\n",
    "    G_b, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"G_b\")\n",
    "\n",
    "X_c_xr = xr.DataArray(\n",
    "    X_c, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"X_c\")\n",
    "G_c_xr = xr.DataArray(\n",
    "    G_c, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"G_c\")\n",
    "\n",
    "X_d_xr = xr.DataArray(\n",
    "    X_d, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"X_d\")\n",
    "G_d_xr = xr.DataArray(\n",
    "    G_d, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"G_d\")\n",
    "\n",
    "X_noparam_xr = xr.DataArray(\n",
    "    X_no_param, dims=[\"time\", \"K\"], coords={\"time\": t, \"K\": np.arange(K)}\n",
    ").rename(\"X_noparam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10), dpi=150)\n",
    "\n",
    "plt.subplot(161)\n",
    "X_full_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-15, 15, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Full model\")\n",
    "\n",
    "plt.subplot(162)\n",
    "X_a_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-15, 15, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Learn Gk\")\n",
    "\n",
    "plt.subplot(163)\n",
    "X_b_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-15, 15, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Lrn Gk w/ con\")\n",
    "\n",
    "plt.subplot(164)\n",
    "X_c_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-15, 15, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Lrn Sk\")\n",
    "\n",
    "plt.subplot(165)\n",
    "X_d_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-15, 15, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Lrn Sk f/ Gk\")\n",
    "\n",
    "plt.subplot(166)\n",
    "X_noparam_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-15, 15, 11),\n",
    "    extend=\"both\",\n",
    "    cbar_kwargs={\"aspect\": 20, \"shrink\": 0.2},\n",
    ")\n",
    "plt.title(\"No param\")\n",
    "\n",
    "plt.suptitle(\"X\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10), dpi=150)\n",
    "\n",
    "plt.subplot(161)\n",
    "G_full_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-20, 20, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Full model\")\n",
    "\n",
    "plt.subplot(162)\n",
    "G_a_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-20, 20, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Learn Gk\")\n",
    "\n",
    "plt.subplot(163)\n",
    "G_b_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-20, 20, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Lrn Gk w/ con\")\n",
    "\n",
    "plt.subplot(164)\n",
    "G_c_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-20, 20, 11), extend=\"both\", add_colorbar=False\n",
    ")\n",
    "plt.title(\"Lrn Sk\")\n",
    "\n",
    "plt.subplot(165)\n",
    "G_d_xr.sel(time=slice(0, 15)).plot.contourf(\n",
    "    levels=np.linspace(-20, 20, 11),\n",
    "    extend=\"both\",\n",
    "    add_colorbar=True,\n",
    "    cbar_kwargs={\"aspect\": 20, \"shrink\": 0.2},\n",
    ")\n",
    "plt.title(\"Lrn Sk\")\n",
    "\n",
    "plt.subplot(166)\n",
    "# X_noparam_xr.sel(time=slice(0, 15)).plot.contourf(levels=np.linspace(-15, 15, 9), extend='both', cbar_kwargs={'aspect':5})\n",
    "plt.title(\"No param\")\n",
    "plt.suptitle(\"G\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Given the simplicity of the parameterization problem in L96 model, all the different ML parameterizations performed relatively well. We expect to see much larger gains by adding constraints in more complex realistic problems, particularly the ones that are more data limited. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Summary \n",
    "\n",
    "In this notebook we learnt how to add constraints to the learning process, and studied the impacts that this has on the trained models. \n",
    "\n",
    "With this notebook we wrap up the section on using neural networks to parameterize missing effects in the L96 model. In the next section we will be learning a different approach to improving models - data assimilation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
