{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Outline:\n",
    "\n",
    "In this notebook we introduce the basics of machine learning, show how to optimize parameters in simple models (linear regression), and finally demonstrate a basic neural network training case. There are many excellent tutorials on machine learning available on the internet, and our goal here is not to be comprehensive but rather provide a gentle introduction that seems necessary to what comes later in the book. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to machine learning \n",
    "\n",
    "Machine learning can be defined as : \"Machine learning is a way for computers to learn and improve from experience without being explicitly programmed.\" This definition was generated with the help of an AI agent called [ChatGPT](https://chat.openai.com/share/4cb73dc4-e372-445c-8aca-94c6199540eb).  \n",
    "\n",
    "It has proven to be a useful tool to the scientific community by being able to make advancements in a range of problems, which had traditionally been very challenging or impossible. In our project, [M2LInES](https://m2lines.github.io/), we are improving simulations of the climate system by targeting the parameterization or sub-grid closure problem with the help of ML. \n",
    "\n",
    "In simpler terms, ML for our context can be thought of as a sophisticated way to approximate or estimate function. This can be done in a way where the structure of the functions is prescribed and constrained to very limited space of functions (e.g. linear regression) or in a way where the the structure of the function is left relatively free (e.g. neural networks). Regardless of this choice, all ML tasks can be broken down into a few general pieces:\n",
    "- Data : All ML tasks need data to learn patterns.\n",
    "- Model : Some choice needs to be made about the model architecture (e.g. neural network, gaussian process, etc). \n",
    "- Loss functions : The way a ML model gets closer to the true function is by minimizing some appropriately chosen loss function, a measure of error between truth and ML prediction.\n",
    "- Optimizers : To minize the error or loss function some method is needed to estimate optimal parameters. \n",
    "\n",
    "Since all this is being done on a computer, some software architecture is needed. Possible, one of the major reasons for large scale adoption of machine learning has been the availability of easy to use, well documented, open source and general purpose machine learning libraries. Many such library exist, e.g. [Tensorflow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/), [JAX](https://jax.readthedocs.io/en/latest/index.html), etc. In this book we will primarily use PyTorch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [PyTorch](https://github.com/pytorch/pytorch)\n",
    "\n",
    "In short, PyTorch is a Python-based machine learning framework that can be used for scientific machine learning tasks. There are many good reasons to use PyTorch, including that it can be used as a replacement for NumPy if we want to do computations on GPUs, it is extremely flexible and fast, but primarily that it has a large community and excellent tutorials to learn from.\n",
    "\n",
    "To understand and install PyTorch, we recommend going through the [tutorials](https://pytorch.org/tutorials/) and [blogs](https://pytorch.org/blog/).\n",
    "\n",
    "\n",
    "To follow along a single blogpost that goes through most concepts, we recommend [this](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/). \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having understood how the tools work, we can now use them to model our data. To train any machine learning model, there is a general recipe followed. This recipe can be summarized in three steps:\n",
    "1. Define a model representation.\n",
    "2. Choose a suitable loss function that tells us how far apart our model predictions are from the real data\n",
    "3. Update the parameters in model representation using an optimization algorithm to minimize the loss function.\n",
    "\n",
    "In this notebook, we will be working a linear regression model and a neural network. We will measure loss using the Mean Squared Error (MSE) function and will try to optimize or reduce the loss value using a popular optimization algorithm, gradient descent.\n",
    "\n",
    "To read further on how to train machine learning models, refer to Prof. Fund's [lecture notes](https://github.com/ffund/intro-ml-tss21) or Prof. Hegde's [blogposts](https://chinmayhegde.github.io/dl-notes/notes/lecture03/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably, the simplest \"machine learning\" model is [linear regression](https://en.wikipedia.org/wiki/Linear_regression). This model models the relationship between the input and output data as a linear function, which for a single input and single output dimension is equivalent to fitting a line through the data:\n",
    "$$\n",
    "y = w_1x + w_0,\n",
    "$$\n",
    "where $w_1$ (slope or kernel) and $w_0$ (bias) are parameters that need to best estimated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "For multiple inputs and single output the model can be visually represented as follows:\n",
    "```{image} https://miro.medium.com/v2/resize:fit:1032/1*WswH2fPx0bf_JFRMm8V-HA.gif\n",
    ":alt: nn-output-computation\n",
    ":width: 500px\n",
    "```\n",
    "\n",
    "*Image is taken from [here](https://blog.insightdatascience.com/a-quick-introduction-to-vanilla-neural-networks-b0998c6216a1).* Similarly, this linear regression could be modified to have multiple inputs and multiple outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to estimate optimal parameters for linear regression, and here we will show how this can be done using PyTorch and gradient descent. \n",
    "\n",
    "The best prediction is defined by minimizing some cost or loss function. For linear regression, generally the mean sequare error between the expected output $y$ and the prediction ($\\tilde{y}$), defined as mean of $(y-\\tilde{y})^2$ over all output dimensions and samples, is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Most of this lesson follows a tutorial from Jeremy Howard's fast.ai [lesson zero](https://www.youtube.com/watch?v=ACU-T9L4_lI)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn the parameters $w_0$ and $w_1$ of a line using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import machine-learning packages\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Import plotting packages\n",
    "from IPython.display import Image, HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import base64\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation for linear regression\n",
    "\n",
    "Since this is a toy problem, we first choose the true parameters so that we can later easily verify the success of our approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.as_tensor([3.0, 2])\n",
    "w\n",
    "# where 3 is the interesection point (sometimes also called bias) and 2 is the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we are using tensors here, which is a data structure used by PyTorch to easily deploy computations to CPUs or GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create some data points x and y which are related to each other through the linear relationship and parameters chosen above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create 100 data points. \n",
    "n = 100\n",
    "x = torch.ones(n, 2)\n",
    "\n",
    "# uniformly sample x points between -1 and 1.\n",
    "# Underscore functions in pytorch means replace the value (update)\n",
    "x[:, 1].uniform_(-1.0, 1)\n",
    "\n",
    "x[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above that for each sample we have created two numbers. \n",
    "The first is 1, to be used with $w_0$, and the second is x, to be used with $w_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we generate output data using the equation for linear regression, but also add some random noise to be in the more real world setting where there is always some noise or if linear regression is not the perfect model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output data.\n",
    "y = x @ w + torch.rand(n)  # @ is a matrix product (similar to matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.scatter(x[:, 1], y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "We define our loss function $L$ as Mean Square Error loss as:\n",
    "\\begin{equation*}\n",
    "L_{MSE} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} (y_i - \\tilde y_i)^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written in terms of $w_0$ and $w_1$, our **loss function** $L$ is:\n",
    "\n",
    "\\begin{equation*}\n",
    "L_{MSE} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} (y_i - (w_0 + \\mathbf{w_1} \\cdot x_i))^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters with some randomly guessed values. \n",
    "w_real = torch.as_tensor([-3.0, -5])\n",
    "y_hat = x @ w_real\n",
    "# Initial mean-squared error\n",
    "mse(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we chose randomly guessed initial parameter choices. While this choice is not consequential here, in more complex problems these choice prior parameter choice can be extremely important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.scatter(x[:, 1], y, label=\"y\")\n",
    "plt.scatter(x[:, 1], y_hat, label=\"$\\\\tilde{y}$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.title('Prediction of model using non-optimized model')\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameter obect that will be optimized.\n",
    "w = nn.Parameter(w_real)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent and optimization\n",
    "\n",
    "So far, we have specified the *model* (linear regression) and the *evaluation criteria* (or *loss function*). Now we need to handle *optimization*; that is, how do we find the best values for weights ($w_0$, $w_1$) such that they best fit the linear regression parameters?\n",
    "\n",
    "This optimization is done using gradient descent. In this process the gradient of the loss function is estimated relative to the unknown parameters, and then the parameters are updated in a direction that reduces the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image file\n",
    "with open(\"figs/Gradient_descent2.png\", \"rb\") as f:\n",
    "    image_data = f.read()\n",
    "\n",
    "# Create the HTML code to display the image\n",
    "html = f'<div style=\"text-align:center\"><img src=\"data:image/png;base64,{base64.b64encode(image_data).decode()}\" style=\"max-width:700px;\"/></div>'\n",
    "\n",
    "# Display the HTML code in the notebook\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To know how to change $w_0$ and $w_1$ to reduce the loss, we compute the derivatives (or gradients):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L_{MSE}}{\\partial w_0} = \\frac{1}{n}\\sum_{i=1}^{n} -2\\cdot [y_i - (w_0 + \\mathbf{w_1}\\cdot x_i)]\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L_{MSE}}{\\partial \\mathbf{w_1}} = \\frac{1}{n}\\sum_{i=1}^{n} -2\\cdot [y_i - (w_0 + \\mathbf{w_1}\\cdot x_i)] \\cdot x_i\n",
    "\\end{equation*}\n",
    "\n",
    "Since we know that we can iteratively take little steps down along the gradient to reduce the loss, aka, *gradient descent*, the size of the step is determined by the learning rate ($\\eta$):\n",
    "\n",
    "\\begin{equation*}\n",
    "w_0^{new} = w_0^{current} -   \\eta \\cdot \\frac{\\partial L_{MSE}}{\\partial w_0}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{w_1^{new}} = \\mathbf{w_1^{current}} -  \\eta \\cdot \\frac{\\partial L_{MSE}}{\\partial \\mathbf{w_1}}\n",
    "\\end{equation*}\n",
    "\n",
    "The gradients needed above are computed using the ML library's [autograd feature](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html), and described more [here](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(lr):\n",
    "    y_hat = x @ w\n",
    "    loss = mse(y, y_hat)\n",
    "    # calculate the gradient of a tensor! It is now stored at w.grad\n",
    "    loss.backward()\n",
    "\n",
    "    # To prevent tracking history and using memory\n",
    "    # (code block where we don't need to track the gradients but only modify the values of tensors)\n",
    "    with torch.no_grad():\n",
    "        # lr is the learning rate. Good learning rate is a key part of Neural Networks.\n",
    "        w.sub_(lr * w.grad)\n",
    "        # We want to zero the gradient before we are re-evaluate it.\n",
    "        w.grad.zero_()\n",
    "\n",
    "    return loss.detach().item(), y_hat.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: In PyTorch, we need to set the gradients to zero before starting to do back propragation because PyTorch accumulates the gradients on subsequent backward passes. This is convenient while training Recurrent Neural Networks (RNNs). So, the default action is to accumulate or sum the gradients on every `loss.backward()` call.\n",
    "Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the minimum (or maximum, in case of maximization objectives).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.as_tensor([-2.0, -3])\n",
    "w = nn.Parameter(w)\n",
    "lr = 0.1\n",
    "losses = [float(\"inf\")]\n",
    "y_hats = []\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model and perform gradient descent\n",
    "for _ in range(epoch):\n",
    "    loss, y_hat = step(lr)\n",
    "    losses.append(loss)\n",
    "    y_hats.append(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(np.array(losses))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale('log')\n",
    "plt.title(\"Loss vs Iteration\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above the loss function is reduced as more optimization steps are taken. The animation below shows how the prediction gets closer to the data as more steps are taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), dpi=80)\n",
    "axs[0].scatter(x[:, 1], y, label=\"y\")\n",
    "scatter_yhat = axs[0].scatter(x[:, 1], y_hat, label=\"$\\\\tilde{y}$\")\n",
    "axs[0].set_xlabel(\"$x$\")\n",
    "axs[0].legend(fontsize=7)\n",
    "\n",
    "(line,) = axs[1].plot(range(len(losses)), np.array(losses))\n",
    "axs[1].set_xlabel(\"Iteration\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "axs[1].set_title(\"Loss vs Iteration\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    axs[0].set_title(\"Loss = %.2f\" % losses[i])\n",
    "    scatter_yhat.set_offsets(np.c_[[], []])\n",
    "    scatter_yhat.set_offsets(np.c_[x[:, 1], y_hats[i]])\n",
    "    line.set_data(np.array(range(i + 1)), np.array(losses[: (i + 1)]))\n",
    "    return scatter_yhat, line\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "animation = FuncAnimation(fig, animate, frames=epoch, interval=100, blit=True)\n",
    "# let animation load\n",
    "time.sleep(1)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you have some difficulties running the cell below without importing certain packages. Run the following code in a terminal in `L96M2lines` environment.\n",
    "```shell\n",
    "conda install -c conda-forge ffmpeg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f'<div style=\"text-align:center;\">{animation.to_html5_video()}</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks \n",
    "\n",
    "The above discussion focussed on fitting a simple function, a line, through our data. However, data in many problems correspond to underlying functions that are much more complex and their exact form is not known. For these problem we need functions that are much more flexible; neural networks are these more flexible functions. \n",
    "\n",
    "This is shown by the **Universal Approximation Theorm**, which states that neural networks can approximate any continuous function. A visual demonstration that neural nets can compute any function can be seen in [this page](http://neuralnetworksanddeeplearning.com/chap4.html).\n",
    "\n",
    "Now we give a brief overview of neural networks and how to build them using PyTorch. If you want to go through it in depth, check out these resources: [Deep Learning With Pytorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) or [Neural Networks](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We create many data points, as neural networks need lot of data to train from scratch.\n",
    "n = 10000\n",
    "x = torch.ones(n,1)\n",
    "\n",
    "# uniformly sample x points between -1 and 1.\n",
    "# Underscore functions in pytorch means replace the value (update)\n",
    "x = x.uniform_(-1.0, 1)\n",
    "\n",
    "y = torch.sin(x* 2*torch.pi) + 0.1*torch.rand(n, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we create a more wigly function (sin), which can not be approximated as a line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[:,0], y, '.', markersize=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the neural network\n",
    "\n",
    "There are many many flavors of neural networks, as can be seen at the [neural network zoo](https://www.asimovinstitute.org/neural-network-zoo/). Here we use the simplest - a fully connected neural net. \n",
    "\n",
    "Our design consists of 3 hidden dense layers, two of which have a non-linear Relu activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Neural_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Dense1 = nn.Linear(1, 30)  \n",
    "        self.Dense2 = nn.Linear(30,30)\n",
    "        self.Dense3 = nn.Linear(30,1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # This method is automatically executed when\n",
    "        # we call a object of this class\n",
    "        x = self.Dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.Dense2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.Dense3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = Simple_Neural_Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_input = torch.randn(1, 1)\n",
    "out = neural_net(net_input)\n",
    "print(f\"The output of the random input {net_input.item():.4f} from untrained network is: {out.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the Weights using an Optimizer\n",
    "\n",
    "In the linear regression problem above we used a very simple optimization procedure, just adjusting the weights manually. However, in more complex problems it is sometimes more convenient to use the optimization module available in PyTorch as it can allow the use of many advanced options. The implementation of almost every optimizer that we'll ever need can be found in PyTorch itself. The choice of which optimizer we choose might be very important as it will determine how fast the network will be able to learn.\n",
    "\n",
    "In the example below, we show one of the popular optimizers `Adam`.\n",
    "\n",
    "#### Adam Optimizer\n",
    "\n",
    "A popular optimizer that is used in many neural networks is the Adam optimizer. It is an adaptive learning rate method that computes individual learning rates for different parameters. For further reading, check out this [post](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c) about Adam, and this [post](https://www.ruder.io/optimizing-gradient-descent/) about other optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the Adam optimizer.\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.Adam(neural_net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "We use the mean square error as loss function again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss function\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "The training step is defined below, where `optimizer.step()` essentially does the job of updating the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, loss_fn, optimizer):\n",
    "    \n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "\n",
    "    pred = model(x)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = loss.item()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to real training problems, often the data sets are very large and can not be fit into memory. In these cases the data might be broken up into smaller chunks called batches, and the training is done on one batch at a time. A pass over all the batches in the data set is referred to as an epoch. Since our data set is very small we do not break it up into batches here, but still refer to each training step as an epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "Loss = np.zeros(epochs)\n",
    "for t in range(epochs):\n",
    "    Loss[t] = train_step(neural_net, loss_fn, optimizer)\n",
    "    if np.mod(t, 200) == 0:\n",
    "        print(f\"Loss at Epoch {t+1} is \", Loss[t])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(Loss)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale('log')\n",
    "plt.title(\"Loss vs Iteration\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Predictions with Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate some points where the predictions of the model will be tested.\n",
    "# Here we pick the testing domain to be larger than the training domain to check if the model\n",
    "# has any skill at extrapolation. \n",
    "x_test = torch.linspace(-1.5, 1.5, 501).reshape(501,1)\n",
    "\n",
    "# Generate the predictions from the trained model.\n",
    "pred = neural_net(x_test).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "\n",
    "plt.plot(x, y, '.', markersize=0.5, label='Data')\n",
    "plt.plot(x_test, pred, markersize=0.5, label='Predicted fit')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the neural network is able to approximate the sine function relatively well over the range over which the training data is available. \n",
    "However, it should also be noted that the neural network possesses no skill to extrapolate beyond the range of the data that it was trained on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we introduced the basics of machine learning - fitting flexible functions to data. In the next few notebooks we will show how neural networks can be used to model sub-grid forcing in the Lorenz 96 model, and also some more useful topics related to neural network training and interpretation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
