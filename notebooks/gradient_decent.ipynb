{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal approximation theorem - NNs can approximate any continuous function.\n",
    "- A visual demonstration that neural nets can compute any function: http://neuralnetworksanddeeplearning.com/chap4.html\n",
    "- Like any ML algorithm, training a neural netwoek requires minimizing some loss function (for a given structure that maps inputs to outputs). \n",
    "- The minimization is done using an algorithm called gradient descent, or a variation called stochastic/minibach gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using gradient descent in Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest machine learning algorithm is linear regression. We will code up linear regression from scratch with a twist: we will use gradient descent, which is also how neural networks learn. Most of this lesson is pretty much stolen from Jeremy Howard's fast.ai [lesson zero](https://www.youtube.com/watch?v=ACU-T9L4_lI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In linear regression, we assume that $y = w_0 + w_1 x $. \n",
    "- We look for the $w$ coefficients that give the 'best' prediction for the output ($y$). The best prediction is defined by minimizing some cost function. For linear regression in machine learning task it is usually the mean square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=\"figs/linear_regression_as_neural_network2.png\", width=500)\n",
    "# Image is taken from https://blog.insightdatascience.com/a-quick-introduction-to-vanilla-neural-networks-b0998c6216a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression from scratch\n",
    "\n",
    "We will learn the parameters $w_0$ and $\\mathbf{w}_1$ of a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting packages\n",
    "%matplotlib inline\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import machine-learning packages\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch_lr_finder import LRFinder\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is pytorch? (as defined at https://pytorch.org/) \n",
    "\n",
    "Itâ€™s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- A replacement for NumPy to use the power of GPUs\n",
    "- A deep learning research platform that provides flexibility and speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose the true parameters we want to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.as_tensor([3.0, 2])\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some data points x and y which lie on the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = torch.ones(n, 2)\n",
    "x[:, 1].uniform_(\n",
    "    -1.0, 1\n",
    ")  # Underscore functions in pytorch means replace the value (update)\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor is a data structure which is a fundamental building block of PyTorch. Tensors are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU\n",
    "and more importantly for us - they can keep track of its gradients.\n",
    "\n",
    "For further reading, see [here](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x @ w + torch.rand(n)  # @ is a matrix product (similar to matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:, 1], y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_real = torch.as_tensor([-3.0, -5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we could find a way to fit our guess for the coefficients the weights ($w_0$ and $\\mathbf{w}_1$), we could use the exact same method for very complicated tasks (as in image recognition). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written in terms of $w_0$ and $w_1$, our **loss function** is:\n",
    "\n",
    "### $L = \\frac{1}{n}\\sum_{i=1}^n [y_i - (w_0 + w_1 x_i)]^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = x @ w_real\n",
    "mse(y_hat, y)  # Initial mean-squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:, 1], y)\n",
    "plt.scatter(x[:, 1], y_hat);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = nn.Parameter(w_real)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So far, we have specified the *model* (linear regression) and the *evaluation criteria* (or *loss function*). Now we need to handle *optimization*; that is, how do we find the best values for weights ($w_0$, $w_1$)? How do we find the best *fitting* linear regression.\n",
    "\n",
    "To know how to change $w_0$ and $w_1$ to reduce the loss, we compute the derivatives (or gradients).\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_0} = \\frac{1}{n}\\sum_i -2[y_i - (w_0 + w_1 x_i)]$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_1} = \\frac{1}{n}\\sum_i -2[y_i - (w_0 + w_1 x_i)]x_i$\n",
    "\n",
    "#### If we know those we can iteratively take little steps down the gradient to reduce the loss, aka, *gradient descent*. How big our steps are is determined by the *learning rate*.\n",
    "\n",
    "$w_0^{new} = w_0^{old}$ - Learning-Rate $*  \\frac{\\partial L}{\\partial w_0}$\n",
    "\n",
    "$w_1^{new} = w_1^{old}$ - Learning-Rate $*  \\frac{\\partial L}{\\partial w_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update2(iteration):\n",
    "    y_hat = x @ w\n",
    "    loss = mse(y, y_hat)\n",
    "    loss.backward()\n",
    "    # calculate the gradient of a tensor! It is now stored at w.grad\n",
    "\n",
    "    # To prevent tracking history and using memory\n",
    "    # (code block where we don't need to track the gradients but only modify the values of tensors)\n",
    "    with torch.no_grad():\n",
    "        w.sub_(lr * w.grad)\n",
    "        # Under score means inplace.\n",
    "        # lr is the learning rate. Good learning rate is a key part of Neural Networks.\n",
    "\n",
    "        w.grad.zero_()\n",
    "        # We want to zero the gradient before we are re-evaluate it.\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, we need to set the gradients to zero before starting to do back propragation because PyTorch accumulates the gradients on subsequent backward passes. This is convenient while training RNNs. So, the default action is to accumulate (i.e. sum) the gradients on every loss.backward() call.\n",
    "Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the minimum (or maximum, in case of maximization objectives).\n",
    "\n",
    "Explanations about how PyTorch calculates the gradients can be found here (and in many other sources) - https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(w1, w0, x):\n",
    "    return w1 * x + w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.as_tensor([-2.0, -3])\n",
    "w = nn.Parameter(w)\n",
    "lr = 0.001\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(x[:, 0], y)\n",
    "(line,) = ax.plot(\n",
    "    x[:, 0],\n",
    "    lin(w.detach().numpy()[0], w.detach().numpy()[1], x.detach().numpy()[:, 0]),\n",
    "    c=\"firebrick\",\n",
    ")\n",
    "# line, = ax.plot(x[:,0], y, c='firebrick')\n",
    "ax.set_title(\"Loss = 0.00\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    for t in range(100):\n",
    "        l = update2(t)\n",
    "    ax.set_title(\"Loss = %.2f\" % l)\n",
    "    line.set_data(\n",
    "        x.detach().numpy()[:, 0],\n",
    "        lin(w.detach().numpy()[0], w.detach().numpy()[1], x.detach().numpy()[:, 0]),\n",
    "    )\n",
    "    return (line,)\n",
    "\n",
    "\n",
    "anim = FuncAnimation(fig, animate, frames=70, interval=150, blit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might have some difficulties running this cell without importing certain packages.\n",
    "# might need to install: conda install -c conda-forge ffmpeg\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"figs/Gradient_descent2.png\", width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.as_tensor([-2.0, -3])\n",
    "w = nn.Parameter(w)\n",
    "lr = 0.01\n",
    "losses = []\n",
    "for t in range(100):\n",
    "    losses.append(update2(t).detach().numpy())\n",
    "\n",
    "plt.plot(np.array(losses))\n",
    "plt.xlabel(\"Iternation\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Deep learning, we use a variation of gradient descent called [mini-batch gradient descent](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/):\n",
    "Instead of calculating the gradient over the whole training data before changing model weights (coefficients), we take a subset (batch) of our data, and change the values of the weights after we calculated the gradient over this subset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
