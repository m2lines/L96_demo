{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a013986b",
   "metadata": {},
   "source": [
    "# Data Assimilation demo in the Lorenz 96 (L96) two time-scale model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e3435",
   "metadata": {},
   "source": [
    "# What is DA? Why do we do it?\n",
    "\n",
    "Data assimilation is an approach for combining data (e.g. observations) with prior knowledge (e.g. from a physical model) to obtain an estimate of the true state of a system. We use data assimilation to generate accurate initial conditions for weather and climate predictions, and to produce reanalysis products (state estimates) of the atmosphere-ocean-sea ice-land system. \n",
    "\n",
    "Conceptually, data assimilation follows the Bayes' Rule:\n",
    "\n",
    "\\begin{align}\n",
    "\\ P(X|Obs)=\\frac{P(Obs|X)P(X)}{P(Obs)}\n",
    "\\end{align}\n",
    "\n",
    "where $X$ is the state of the dynamic system, $P(X|Obs)$ is the posterior distribution, or the analysis in DA terms, and $P(X)$ is the prior distribution, or the forecast in DA terms. $P(Obs|X)$ is the observation distribution given the prior. Simply speaking, we want to know the best estimate of the current state $X$ of the system given the available observations. Below is a simple schematic representation of one DA step.\n",
    "\n",
    "```{figure} figs/DA_step.png\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a8f14",
   "metadata": {},
   "source": [
    "# Design of the data assimilation experiments in L96\n",
    "\n",
    "We use the 2-scale Lorenz-96 model as our truth, or our representation of the \"real world\". We then use the 1-scale L96 model as our DA model, or our representation of the \"GCM\". Detailed description of this setup can be find here (https://m2lines.github.io/L96_demo/notebooks/gcm-analogue.html). This is what we call an idealized biased-model framework in DA research, in the sense that our DA model is biased compared to the truth, similar to real-world situations where our GCMs are biased against the natural weather or climate systems. \n",
    "\n",
    "We will follow the follwing steps when doing DA in this idealized biased L96 framework:\n",
    "1. Generate a “truth” signal from 2-scale L96 model\n",
    "2. Specify a biased “GCM”model for performing DA\n",
    "    * Choose P (to start, use GCM with P=0).\n",
    "    * Choose F_GCM (to start, use F_GCM=F).\n",
    "3. Collect sparse and noisy observations, sampled from “truth”\n",
    "    * Specify observation density in space, frequency in time\n",
    "    * Specify observational error\n",
    "4. Perform DA using a method of choice\n",
    "    * Choose DA method: EnKF, 3DVAR, Hybrid EnKF\n",
    "    * Specify DA setup: ensemble members, localization, inflation\n",
    "5. Assess DA performance\n",
    "    * RMSE of DA experiment relative to truth\n",
    "6. Analyze DA increments to assess GCM model error (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66584b6f",
   "metadata": {},
   "source": [
    "# 1. Define our \"GCM\" and DA parameters to use throughout notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81b262-6d04-44d8-9a61-ada0ea0c9acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from L96_model import L96, RK4, L96_2t_xdot_ydot, L96_eq1_xdot, L96s\n",
    "import DA_methods\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3e7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # fmt: off\n",
    "    K = 40                              # Dimension of L96 `X` variables\n",
    "    J = 10                              # Dimension of L96 `Y` variables\n",
    "    obs_freq = 10                       # Observation frequency (number of sampling intervals (si) per observation)\n",
    "    F_truth = 10                        # F for truth signal\n",
    "    F_fcst = 10                         # F for forecast (DA) model\n",
    "    GCM_param = np.array([0, 0, 0, 0])  # Polynomial coefficicents for GCM parameterization\n",
    "    ns_da = 2000                        # Number of time samples for DA\n",
    "    ns = 2000                           # Number of time samples for truth signal\n",
    "    ns_spinup = 200                     # Number of time samples for spin up\n",
    "    dt = 0.005                          # Model timestep\n",
    "    si = 0.005                          # Truth sampling interval\n",
    "    seasonal = False                    # Option for adding a seasonal cycle to the forcing in the L96 truth model\n",
    "    B_loc = 5                           # Spatial localization radius for DA in terms of model grid points\n",
    "    DA = \"EnKF\"                         # DA method\n",
    "    nens = 500                          # Number of ensemble members for DA\n",
    "    inflate_opt = \"relaxation\"          # Method for DA model covariance inflation\n",
    "    inflate_factor = 0.65                # Inflation factor\n",
    "    hybrid_factor = 0.1                 # Weighting factor for hybrid EnKF method (weight of static B matrix)\n",
    "    param_DA = False                    # Switch to turn on parameter estimation with DA\n",
    "    param_sd = [0.01, 0.02, 0.1, 0.5]   # Polynomal parameter standard deviation\n",
    "    param_inflate = \"multiplicative\"    # Method for parameter variance inflation\n",
    "    param_inf_factor = 0.02             # Parameter inflation factor\n",
    "    obs_density = 0.2                   # Fraction of spatial gridpoints where observations are collected\n",
    "    DA_freq = 10                        # Assimilation frequency (number of sampling intervals (si) per assimilation step)\n",
    "    obs_sigma = 0.5                     # Observational error standard deviation\n",
    "    save_fig = False                    # Switch to save figure file\n",
    "    save_data = False                   # Switch to save\n",
    "    # fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf223e6-ed8a-425a-b808-55b0cc59c69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GCM(X_init, F, dt, nt, param=[0]):\n",
    "    \"\"\"A toy `General Circulation Model` (GCM) that uses the single time-scale Lorenz 1996 model\n",
    "    with a parameterised coupling term to represent the interaction between the observed coarse\n",
    "    scale processes `X` and unobserved fine scale processes `Y` of the two time-scale model.\n",
    "\n",
    "    Args:\n",
    "        X_init: Initial conditions of X\n",
    "        F: Forcing term\n",
    "        dt: Sampling frequency of the model\n",
    "        nt: Number of timesteps for which to run the model\n",
    "        param: Weights to give to the coupling term\n",
    "\n",
    "    Returns:\n",
    "        Model output for all variables of X at each timestep along with the corresponding time units\n",
    "    \"\"\"\n",
    "    time, hist, X = (\n",
    "        dt * np.arange(nt + 1),\n",
    "        np.zeros((nt + 1, len(X_init))) * np.nan,\n",
    "        X_init.copy(),\n",
    "    )\n",
    "    hist[0] = X\n",
    "\n",
    "    for n in range(nt):\n",
    "        X = X + dt * (L96_eq1_xdot(X, F) - np.polyval(param, X))\n",
    "        hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "\n",
    "    return hist, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3052af",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# 2. Generate `truth` run from two time-scale L96 model\n",
    "\n",
    "We initialise the L96 two time-scale model using a set of random normally distributed values for $X$, and zeros for $Y$, and run it forward for a period `ns_spinup` to allow the model to spinup. The $X$ and $Y$ components of the spinup are then used as initial conditions for another forward run for a time period `ns` to represent the unobserved `truth` field from which our observations will be derived.\n",
    "\n",
    "_Note that in reality we do not observe the fine scale components ($Y$s) and so this is what we aim to represent in the parameterisation in the `GCM()` function._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393d1bd-5838-402d-a381-a617c987d1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the \"truth\" 2-scale L96 model\n",
    "M_truth = L96(Config.K, Config.J, F=Config.F_truth, dt=Config.dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4696c6b-cfbd-4623-9952-bcf7a3e75783",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Generate the initial conditions of $X$ and $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8e02a-2dec-4724-9175-f5809414533e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "M_truth.set_state(rng.standard_normal((Config.K)), 0 * M_truth.j)\n",
    "\n",
    "# The model runs for `ns_spinup` timesteps to spin-up\n",
    "X_spinup, Y_spinup, t_spinup = M_truth.run(Config.si, Config.si * Config.ns_spinup)\n",
    "\n",
    "# The initial conditions for the first forecast (prior to DA)\n",
    "# are the last time sample after spinup\n",
    "X_init = X_spinup[-1, :]\n",
    "Y_init = Y_spinup[-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c402e-e083-4cfe-b678-d4d8cd5e098f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Using the generated initial conditions above, run **L96** to generate the `truth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f3b33-7cf7-47ae-9460-f735f8d46627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "M_truth.set_state(X_init, Y_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916b273-2823-4c6c-b3e4-fd1f3b5ce805",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "If given in `Config`, give `F` a **seasonal cycle** in the truth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceac2dd-543e-4688-99ce-a7eb2bcc4e41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if Config.seasonal:\n",
    "    ann_period = 2000\n",
    "    mon_period = 100\n",
    "    mon_per_ann = ann_period / mon_period\n",
    "    X_truth, Y_truth, t_truth = M_truth.run(Config.si, Config.si * mon_period)\n",
    "    for i in range(1, int(Config.ns / mon_period)):\n",
    "        M_truth.set_state(X_truth[-1, ...], Y_truth[-1, ...])\n",
    "        M_truth.set_param(F=Config.F_truth + 2 * np.sin(2 * np.pi * i / mon_per_ann))\n",
    "        X_step, Y_step, t_step = M_truth.run(Config.si, Config.si * mon_period)\n",
    "        X_truth = np.concatenate((X_truth, X_step[1:None, ...]))\n",
    "        Y_truth = np.concatenate((Y_truth, Y_step[1:None, ...]))\n",
    "        t_truth = np.concatenate((t_truth, t_truth[-1] + t_step[1:None]))\n",
    "else:\n",
    "    X_truth, Y_truth, t_truth = M_truth.run(Config.si, Config.si * Config.ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c0fef-53be-4d0b-9105-9601862f6690",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now, we generate climatological background (temporal) covariance `B_clim2` for the 2-scale L96 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac4faf-ff3f-4152-bfb9-fcdb1f2c2234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B_clim2 = np.cov(X_truth.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df79fe-b945-43be-b08a-b63b93bca400",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Plotting values of $X$ and $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e66be99-a6d1-4134-92a4-e1a2ab8a6916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10), dpi=150)\n",
    "plt.subplot(221)\n",
    "\n",
    "# Snapshot of X[k]\n",
    "plt.plot(M_truth.k, X_truth[-1, :], label=\"X\")\n",
    "plt.plot(M_truth.j / M_truth.J, Y_truth[-1, :], label=\"Y\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"k\")\n",
    "plt.title(\"$X,Y$ @ $t=N\\Delta t$\")\n",
    "plt.plot(M_truth.k, X_truth[0, :], \"k:\")\n",
    "plt.plot(M_truth.j / M_truth.J, Y_truth[0, :], \"k:\")\n",
    "plt.subplot(222)\n",
    "\n",
    "# Sample time-series X[0](t), Y[0](t)\n",
    "plt.plot(t_truth, X_truth[:, 0], label=\"X\")\n",
    "plt.plot(t_truth, Y_truth[:, 0], label=\"Y\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.title(\"$X[0,t]$, $Y[0,t]$\")\n",
    "plt.subplot(223)\n",
    "\n",
    "# Full model history of X\n",
    "plt.contourf(M_truth.k, t_truth, X_truth)\n",
    "plt.colorbar(orientation=\"horizontal\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"t\")\n",
    "plt.title(\"$X(k,t)$\")\n",
    "plt.subplot(224)\n",
    "\n",
    "# Full model history of Y\n",
    "plt.contourf(M_truth.j / M_truth.J, t_truth, Y_truth)\n",
    "plt.colorbar(orientation=\"horizontal\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"t\")\n",
    "plt.title(\"$Y(k,t)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a17c582-090e-48bf-b26a-adbd66fdbefb",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Generating climatological background covariance `B_clim1` for 1-scale L96 model. This climatological covariance will be used by the 3-dimensional variational (3DVar) DA method, or by the hybrid 3DVar-EnKF method. Since we cannot observe every state variable at every possible location, when assimilting sparse observations into the model, we need to use each observation to update multiple model locations, usually around the observed location. The covariance are thus needed to project the model-observation misfits between different model locations in certain DA methods such as 3DVar and EnKF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b5cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "M_1s = L96s(Config.K, F=Config.F_truth, dt=Config.dt, method=RK4)\n",
    "M_1s.set_state(X_init)\n",
    "X1_truth, t1_truth = M_1s.run(Config.si * Config.ns)\n",
    "B_clim1 = np.cov(X1_truth.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f14f72",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# 3. Generate synthetic observations\n",
    "\n",
    "Here we generate a set of sparse observations by sampling from the `X_truth` run and adding some random Gaussian noise. The observations are stored as individual samples, each containing the time, location, and value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01dbe50-741b-4ce5-88ac-4f583e19d11c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample the \"truth\" to generate observations at certain times (t_obs) and locations (l_obs)\n",
    "t_obs = np.tile(\n",
    "    np.arange(Config.obs_freq, Config.ns_da + Config.obs_freq, Config.obs_freq),\n",
    "    [int(Config.K * Config.obs_density), 1],\n",
    ").T\n",
    "\n",
    "l_obs = np.zeros(t_obs.shape, dtype=\"int\")\n",
    "for i in range(l_obs.shape[0]):\n",
    "    l_obs[i, :] = rng.choice(\n",
    "        Config.K, int(Config.K * Config.obs_density), replace=False\n",
    "    )\n",
    "\n",
    "X_obs = X_truth[t_obs, l_obs] + Config.obs_sigma * rng.standard_normal(l_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ee8b4d-0539-4621-8d45-30769452a155",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The covariance matrix `R` for the observations (used to express the uncertainty in the observations during DA) is given as a diagonal matrix with entries defined by the square of the \"observational error standard deviation\". $R$ is a diagonal matrix because we assume that the errors of different observations are independent. The function `find_obs` is used here to find the locations and values of a set of observations given the observed times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc69e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculated observation covariance matrix, assuming independent observations\n",
    "R = Config.obs_sigma**2 * np.eye(int(Config.K * Config.obs_density))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34000d8e-bb3c-4ca7-b760-e186ec7c2f80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 6], dpi=150)\n",
    "plt.plot(range(1000, 1500), X_truth[1000:1500, 0], label=\"truth\")\n",
    "plt.scatter(\n",
    "    t_obs[100:150, 0],\n",
    "    DA_methods.find_obs(0, X_obs, t_obs, l_obs, [t_obs[100, 0], t_obs[150, 0]]),\n",
    "    color=\"k\",\n",
    "    label=\"obs\",\n",
    ")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72bc6d5",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# 4. Check the Model Background Covariances for the variable `X`\n",
    "\n",
    "The climatological covariances of both the \"truth\" (2-scale L96) and \"GCM\" (1-scale L96) are computed a-priori from a long run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c053ebb0-0e36-41cc-bca5-141ddd15a165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B_corr1 = np.zeros(B_clim1.shape)\n",
    "B_corr2 = np.zeros(B_clim2.shape)\n",
    "for i in range(B_clim1.shape[0]):\n",
    "    for j in range(B_clim1.shape[1]):\n",
    "        B_corr1[i, j] = B_clim1[i, j] / np.sqrt(B_clim1[i, i] * B_clim1[j, j])\n",
    "        B_corr2[i, j] = B_clim2[i, j] / np.sqrt(B_clim2[i, i] * B_clim2[j, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e9b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6), dpi=150)\n",
    "plt.subplot(121)\n",
    "plt.contourf(B_corr1, cmap=\"bwr\", extend=\"both\", levels=np.linspace(-0.95, 0.95, 20))\n",
    "plt.colorbar()\n",
    "plt.title(\"Background correlation matrix: 1-scale L96\")\n",
    "plt.subplot(122)\n",
    "plt.contourf(B_corr2, cmap=\"bwr\", extend=\"both\", levels=np.linspace(-0.95, 0.95, 20))\n",
    "plt.colorbar()\n",
    "plt.title(\"Background correlation matrix: 2-scale L96\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a249a",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# 4. Run Data Assimilation\n",
    "\n",
    "During each DA cycle, we produce an updated state estimate for all $K$ grid points, and all `n` number of ensemble members.\n",
    "\n",
    "The inputs are:\n",
    "\n",
    "- prior estimate (the forecast at time t for all K grid points, and all n number of ensemble members)\n",
    "- observations (N observations)\n",
    "- operator matrix (N x K)\n",
    "- covariance over obs (N x N)\n",
    "- covariance over model (K x K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5c9f4-0ae8-4d07-a458-6d2b1ec4af33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Set up array to store DA increments\n",
    "X_inc = np.zeros((int(Config.ns_da / Config.DA_freq), Config.K, Config.nens))\n",
    "if Config.DA == \"3DVar\":\n",
    "    X_inc = np.squeeze(X_inc)\n",
    "\n",
    "t_DA = np.zeros(int(Config.ns_da / Config.DA_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49ce4c-2a11-4207-9bd1-05b16607b2fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize ensemble with perturbations\n",
    "i_t = 0\n",
    "ensX = X_init[None, :, None] + rng.standard_normal((1, Config.K, Config.nens))\n",
    "X_post = ensX[0, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c576b-be9f-4353-9172-455853f08d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Localize the covariance matrix in advance to avoid the calculation at each DA step\n",
    "# The localization weight matrix is also saved for EnKF\n",
    "\n",
    "B_loc, W_clim = DA_methods.localize_covariance(B_clim1, loc=Config.B_loc)\n",
    "\n",
    "if Config.param_DA:\n",
    "    mean_param = np.zeros((int(Config.ns_da / Config.DA_freq), len(Config.GCM_param)))\n",
    "    spread_param = np.zeros((int(Config.ns_da / Config.DA_freq), len(Config.GCM_param)))\n",
    "    param_scale = Config.param_sd\n",
    "    W = np.ones((Config.K + len(Config.GCM_param), Config.K + len(Config.GCM_param)))\n",
    "    W[0 : Config.K, 0 : Config.K] = W_clim\n",
    "else:\n",
    "    W = W_clim\n",
    "    param_scale = np.zeros(Config.GCM_param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e95487-435f-4bd4-bfab-6e0cb6893b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ens_param = np.zeros((len(Config.GCM_param), Config.nens))\n",
    "for i in range(len(Config.GCM_param)):\n",
    "    ens_param[i, :] = Config.GCM_param[i] + rng.normal(\n",
    "        scale=param_scale[i], size=Config.nens\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef8b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for cycle in np.arange(0, Config.ns_da / Config.DA_freq, dtype=int):\n",
    "    # Set up array to store model forecast for each DA cycle\n",
    "    ensX_fcst = np.zeros((Config.DA_freq + 1, Config.K, Config.nens))\n",
    "\n",
    "    # Model forecast for next DA cycle\n",
    "    for n in range(Config.nens):\n",
    "        ensX_fcst[..., n] = GCM(\n",
    "            X_post[0 : Config.K, n],\n",
    "            Config.F_fcst,\n",
    "            Config.dt,\n",
    "            Config.DA_freq,\n",
    "            ens_param[:, n],\n",
    "        )[0]\n",
    "\n",
    "    i_t = i_t + Config.DA_freq\n",
    "\n",
    "    # Get prior/background from the forecast\n",
    "    X_prior = ensX_fcst[-1, ...]\n",
    "\n",
    "    # Call DA\n",
    "    t_DA[cycle] = t_truth[i_t]\n",
    "    if Config.DA == \"EnKF\":\n",
    "        H = DA_methods.observation_operator(Config.K, l_obs, t_obs, i_t)\n",
    "\n",
    "        # Augment state vector with parameters when doing parameter estimation\n",
    "        if Config.param_DA:\n",
    "            H = np.concatenate(\n",
    "                (H, np.zeros((H.shape[0], len(Config.GCM_param)))), axis=-1\n",
    "            )\n",
    "            X_prior = np.concatenate((X_prior, ens_param))\n",
    "        B_ens = np.cov(X_prior)\n",
    "        B_ens_loc = B_ens * W\n",
    "        X_post = DA_methods.EnKF(X_prior, X_obs[t_obs == i_t], H, R, B_ens_loc)\n",
    "\n",
    "        X_post[0 : Config.K, :] = DA_methods.ens_inflate(\n",
    "            X_post[0 : Config.K, :],\n",
    "            X_prior[0 : Config.K, :],\n",
    "            Config.inflate_opt,\n",
    "            Config.inflate_factor,\n",
    "        )\n",
    "\n",
    "        if Config.param_DA:\n",
    "            X_post[-len(Config.GCM_param) :, :] = DA_methods.ens_inflate(\n",
    "                X_post[-len(Config.GCM_param) :, :],\n",
    "                X_prior[-len(Config.GCM_param) :, :],\n",
    "                Config.param_inflate,\n",
    "                Config.param_inf_factor,\n",
    "            )\n",
    "            ens_param = X_post[-len(Config.GCM_param) :, :]\n",
    "\n",
    "    elif Config.DA == \"HyEnKF\":\n",
    "        H = DA_methods.observation_operator(Config.K, l_obs, t_obs, i_t)\n",
    "        B_ens = (\n",
    "            np.cov(X_prior) * (1 - Config.hybrid_factor)\n",
    "            + B_clim1 * Config.hybrid_factor\n",
    "        )\n",
    "        B_ens_loc = B_ens * W\n",
    "        X_post = DA_methods.EnKF(X_prior, X_obs[t_obs == i_t], H, R, B_ens_loc)\n",
    "        X_post = DA_methods.ens_inflate(\n",
    "            X_post, X_prior, Config.inflate_opt, Config.inflate_factor\n",
    "        )\n",
    "\n",
    "    elif Config.DA == \"3DVar\":\n",
    "        X_prior = np.squeeze(X_prior)\n",
    "        H = DA_methods.observation_operator(Config.K, l_obs, t_obs, i_t)\n",
    "        X_post = DA_methods.Lin3dvar(X_prior, X_obs[t_obs == i_t], H, R, B_loc, 3)\n",
    "\n",
    "    elif Config.DA == \"Replace\":\n",
    "        X_post = X_prior\n",
    "        X_post[l_obs[t_obs == i_t]] = X_obs[t_obs == i_t]\n",
    "\n",
    "    elif Config.DA == \"None\":\n",
    "        X_post = X_prior\n",
    "\n",
    "    if not Config.DA == \"None\":\n",
    "        # Get current increments\n",
    "        X_inc[cycle, ...] = (\n",
    "            np.squeeze(X_post[0 : Config.K, ...]) - X_prior[0 : Config.K, ...]\n",
    "        )\n",
    "\n",
    "        # Get posterior info about the estimated parameters\n",
    "        if Config.param_DA:\n",
    "            mean_param[cycle, :] = ens_param.mean(axis=-1)\n",
    "            spread_param[cycle, :] = ens_param.std(axis=-1)\n",
    "\n",
    "    # Reset initial conditions for next DA cycle\n",
    "    ensX_fcst[-1, :, :] = X_post[0 : Config.K, :]\n",
    "    ensX = np.concatenate((ensX, ensX_fcst[1:None, ...]))\n",
    "\n",
    "print(f\"Time to complete DA: {round(time.time() - start_time, 2)} (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d243a69",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# 5. Post Processing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c37a85-433c-4d2e-a49c-1daf4f6a9ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_X = np.mean(ensX, axis=-1)\n",
    "clim = np.max(np.abs(mean_X - X_truth[0 : (Config.ns_da + 1), :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f05d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "ch = axes[0, 0].contourf(\n",
    "    M_truth.k,\n",
    "    t_truth[0 : (Config.ns_da + 1)],\n",
    "    mean_X - X_truth[0 : (Config.ns_da + 1), :],\n",
    "    cmap=\"bwr\",\n",
    "    vmin=-clim,\n",
    "    vmax=clim,\n",
    "    extend=\"neither\",\n",
    ")\n",
    "\n",
    "plt.colorbar(ch, ax=axes[0, 0], orientation=\"horizontal\")\n",
    "axes[0, 0].set_xlabel(\"s\")\n",
    "axes[0, 0].set_ylabel(\"t\", rotation=0)\n",
    "axes[0, 0].set_title(\"X - X_truth\")\n",
    "\n",
    "axes[0, 1].plot(\n",
    "    t_truth[0 : (Config.ns_da + 1)],\n",
    "    np.sqrt(((mean_X - X_truth[0 : (Config.ns_da + 1), :]) ** 2).mean(axis=-1)),\n",
    "    label=\"RMSE\",\n",
    ")\n",
    "axes[0, 1].plot(\n",
    "    t_truth[0 : (Config.ns_da + 1)],\n",
    "    np.mean(np.std(ensX, axis=-1), axis=-1),\n",
    "    label=\"Spread\",\n",
    ")\n",
    "axes[0, 1].plot(\n",
    "    t_truth[0 : (Config.ns_da + 1)],\n",
    "    Config.obs_sigma * np.ones((Config.ns_da + 1)),\n",
    "    label=\"Obs error\",\n",
    ")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_xlabel(\"time\")\n",
    "axes[0, 1].set_title(\"RMSE (X - X_truth)\")\n",
    "axes[0, 1].grid(which=\"both\", linestyle=\"--\")\n",
    "\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k,\n",
    "    np.sqrt(((mean_X - X_truth[0 : (Config.ns_da + 1), :]) ** 2).mean(axis=0)),\n",
    "    label=\"RMSE\",\n",
    ")\n",
    "X_inc_ave = X_inc / Config.DA_freq / Config.si\n",
    "axes[0, 2].plot(M_truth.k, X_inc_ave.mean(axis=(0, -1)), label=\"Inc\")\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k, DA_methods.running_average(X_inc_ave.mean(axis=(0, -1)), 7), label=\"Inc Ave\"\n",
    ")\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k,\n",
    "    np.ones(M_truth.k.shape) * (Config.F_fcst - Config.F_truth),\n",
    "    label=\"F_bias\",\n",
    ")\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k,\n",
    "    np.ones(M_truth.k.shape) * (X_inc / Config.DA_freq / Config.si).mean(),\n",
    "    \"k:\",\n",
    "    label=\"Ave Inc\",\n",
    ")\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].set_xlabel(\"s\")\n",
    "axes[0, 2].set_title(\"Increments\")\n",
    "axes[0, 2].grid(which=\"both\", linestyle=\"--\")\n",
    "\n",
    "plot_start, plot_end = 1000, 1500\n",
    "plot_start_DA, plot_end_DA = int(plot_start / Config.DA_freq), int(\n",
    "    plot_end / Config.DA_freq\n",
    ")\n",
    "plot_x = 0\n",
    "\n",
    "axes[1, 0].plot(\n",
    "    t_truth[plot_start:plot_end], X_truth[plot_start:plot_end, plot_x], label=\"truth\"\n",
    ")\n",
    "axes[1, 0].plot(\n",
    "    t_truth[plot_start:plot_end], mean_X[plot_start:plot_end, plot_x], label=\"forecast\"\n",
    ")\n",
    "axes[1, 0].scatter(\n",
    "    t_DA[plot_start_DA - 1 : plot_end_DA - 1],\n",
    "    DA_methods.find_obs(plot_x, X_obs, t_obs, l_obs, [plot_start, plot_end]),\n",
    "    label=\"obs\",\n",
    ")\n",
    "axes[1, 0].grid(which=\"both\", linestyle=\"--\")\n",
    "axes[1, 0].set_xlabel(\"time\")\n",
    "axes[1, 0].set_title(\"k=\" + str(plot_x + 1) + \" truth and forecast\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "if Config.param_DA:\n",
    "    for i, c in zip(np.arange(len(Config.GCM_param), 0, -1), [\"r\", \"b\", \"g\", \"k\"]):\n",
    "        axes[1, 1].plot(\n",
    "            t_DA,\n",
    "            DA_methods.running_average(mean_param[:, i - 1], 100),\n",
    "            c + \"-\",\n",
    "            label=\"C{} {:3f}\".format(\n",
    "                i - 1, mean_param[int(len(t_DA) / 2) :, i - 1].mean()\n",
    "            ),\n",
    "        )\n",
    "        axes[1, 1].plot(\n",
    "            t_DA,\n",
    "            DA_methods.running_average(mean_param[:, i - 1] + spread_param[:, i - 1], 100),\n",
    "            c + \":\",\n",
    "            label=f\"SD {spread_param[int(len(t_DA) / 2):, i - 1].mean():3f}\",\n",
    "        )\n",
    "        axes[1, 1].plot(\n",
    "            t_DA,\n",
    "            DA_methods.running_average(mean_param[:, i - 1] - spread_param[:, i - 1], 100),\n",
    "            c + \":\",\n",
    "        )\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(which=\"both\", linestyle=\"--\")\n",
    "\n",
    "else:\n",
    "    axes[1, 1].axis(\"off\")\n",
    "\n",
    "axes[1, 2].text(\n",
    "    0.1,\n",
    "    0.2,\n",
    "    (\n",
    "        f\"RMSE={np.sqrt(((mean_X - X_truth[0 : (Config.ns_da + 1), :]) ** 2).mean()):3f}\\n\"\n",
    "        f\"Spread={np.mean(np.std(ensX, axis=-1)):3f}\\n\"\n",
    "        f\"GCM param={Config.GCM_param}\\n\"\n",
    "        f\"DA={Config.DA}, {Config.nens}\\n\"\n",
    "        f\"DA_freq={Config.DA_freq}\\n\"\n",
    "        f\"B_loc={Config.B_loc}\\n\"\n",
    "        f\"inflation={Config.inflate_opt}, {Config.inflate_factor}\\n\"\n",
    "        f\"obs_density={Config.obs_density}\\n\"\n",
    "        f\"obs_sigma={Config.obs_sigma}\\n\"\n",
    "        f\"obs_freq={Config.obs_freq}\\n\"\n",
    "    ),\n",
    "    fontsize=15,\n",
    ")\n",
    "axes[1, 2].axis(\"off\")\n",
    "\n",
    "if Config.save_fig:\n",
    "    output_dir = os.path.join(\".\", \"DA_data\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    exp_number = np.random.randint(1, 10000)\n",
    "    with open(os.path.join(output_dir, f\"config_{exp_number}.txt\"), \"w\") as f:\n",
    "        f.write(\n",
    "            output_dir\n",
    "            + str({k: v for k, v in Config.__dict__.items() if not k.startswith(\"_\")})\n",
    "        )\n",
    "    plt.savefig(os.path.join(output_dir, f\"fig_{exp_number}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63475b82-5e45-4a2c-93f5-f0e089320c7b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Save DA output for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65993585",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.save_data:\n",
    "    output_filename = (\n",
    "        f\"K_{Config.K}_\"\n",
    "        f\"J_{Config.J}_\"\n",
    "        f\"obs_freq_{Config.obs_freq}_\"\n",
    "        f\"F_truth_{Config.F_truth}_\"\n",
    "        f\"F_fcst_{Config.F_fcst}_\"\n",
    "        f\"ns_da_{Config.ns_da}_\"\n",
    "        f\"ns_{Config.ns}_\"\n",
    "        f\"ns_spinup_{Config.ns_spinup}_\"\n",
    "        f\"dt_{Config.dt}_\"\n",
    "        f\"si_{Config.si}_\"\n",
    "        f\"B_loc_{Config.B_loc}_\"\n",
    "        f\"DA_{Config.DA}_\"\n",
    "        f\"nens_{Config.nens}_\"\n",
    "        f\"inflate_opt_{Config.inflate_opt}_\"\n",
    "        f\"inflate_factor_{Config.inflate_factor}_\"\n",
    "        f\"hybrid_factor_{Config.hybrid_factor}_\"\n",
    "        f\"obs_density_{Config.obs_density}_\"\n",
    "        f\"DA_freq_{Config.DA_freq}_\"\n",
    "        f\"obs_sigma_{Config.obs_sigma}.npz\"\n",
    "    )\n",
    "    output_dir = os.path.join(\".\", \"DA_data\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    np.savez(\n",
    "        os.path.join(output_dir, output_filename),\n",
    "        mean_X=mean_X,\n",
    "        X_truth=X_truth,\n",
    "        X_inc_ave=X_inc_ave,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e96ab6d-95db-4882-ab8c-687b0f9c68bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
