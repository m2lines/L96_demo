{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d2675f-796a-4466-a76a-c9a354518d1b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Using Neural Networks for L96 Parameterization: Online Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa918476-598e-4ec0-9d9c-b2fd20d2e699",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Outline:\n",
    "In the previous two notebooks we showed how a parameterization can be developed for the L96 setup using neural networks using a methodology known as offline training. In this notebook we dive into an alternative training philosophy known as online training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f632ad-af84-4958-b940-20b96755460d",
   "metadata": {},
   "source": [
    "## Online vs offline training\n",
    "\n",
    "The type of ML training done in the previous couple of notebooks, where we estimate the sub-grid terms explicitly as the terms corresponding to the missing scales from a full simulations, is known as offline training. In this training procedure it is assumed that know exactly how to estimate the effect of the missing scales. However, this might not be true in general. For the L96 case there can be differences in the numerics of the one time scale and two time scale models, which will not be accounted for in the sub-grid terms we estimate from the two time-scale more. In more general simulations, e.g. turbulent flows, a clear scale separation into scales is not available, and it is not obvious what the right way to filter the high resolution simulation might be. Additionally, the parameterization learnt using the offline procedure may turn out to be unstable because numerical stability of the parameterized model was never imposed as a constraint during the traing procedure. \n",
    "\n",
    "An alternate way to train machine learning models is called online training. In this procedure, rather than training a model that best fits a pre-defined sub-grid tendency, a model is trained which tries to produce a solution of the parameterized low resolution model ($X^{param-LR}$) that is closest to the high resolution model ($X^{HR}$, where this $X$ has been appropriately downscaled to match the resolution of the low resolution model). For the L96 simulation, this would imply that the parameterized one time-scale model match the evolution of the slow variables in the two-time scale model. \n",
    "\n",
    "A corresponding loss function may be defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "L^{Online} = \\frac{1}{T}\\int_0^T |X^{param-LR} - X^{HR}|^2 dt\n",
    "\\end{equation}\n",
    "\n",
    "Contrast this to the loss function offline training:\n",
    "\\begin{equation}\n",
    "L^{Offline} = |P(X_k^{LR}) - U_k|^2,\n",
    "\\end{equation}\n",
    "where $P(X)$ is the parameterization of the sub-grid terms (coupling terms) $U$. *Note that in both of the loss functions above appropriate averages over available samples are taken.*\n",
    "\n",
    "So, let's see how this can be done in practice, and what technical changes we need to make. \n",
    "\n",
    "Note: Much of this notebook has been inspired by Pavel Perezhogin's [repo](https://github.com/Pperezhogin/L96-predictability). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a18f4-f3d5-4503-8c0b-e81895ee2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "from IPython.display import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torch import nn, optim\n",
    "\n",
    "from L96_model import L96, RK2, RK4, EulerFwd, L96_eq1_xdot, integrate_L96_2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79044f9a-ac01-415a-a88a-0385718c8b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring reproducibility\n",
    "np.random.seed(14)\n",
    "torch.manual_seed(14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c9e8fe-9e64-4a58-8cf9-0223192a0d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # You can try this if you have a GPU. However, speed up on GPU is only guaranteed after overhead cost of moving data between CPU and GPU is accounted for. \n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555922c-f58e-4343-8cf0-1ad027046c6a",
   "metadata": {},
   "source": [
    "## Generate the Ground Truth Dataset from the *Real World* \n",
    "\n",
    "Same as the past notebooks, we first generate some instance from the two-time scale (also called our \"real world\") simulation. \n",
    "\n",
    "We initialise the L96 two time-scale model using $K$ (set to 8) values of $X$ and $J$ (set to 32) values of $Y$ for each $X$. The model is run for many timesteps to generate the dataset for training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0738e8-fd6f-4f0c-befa-aa5db97f4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the two time-scale model\n",
    "time_steps = 32000\n",
    "forcing, dt, T = 18, 0.01, 0.01 * time_steps\n",
    "\n",
    "# Create a \"real world\" with K=8 and J=32\n",
    "W = L96(8, 32, F=forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3e6b4-269f-4ac5-82e8-366eb2fb6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the two time-scale model\n",
    "\n",
    "# The effect of Y on X is `xy_true`\n",
    "X_true, _, _, _ = W.run(dt, T, store=True, return_coupling=True)\n",
    "\n",
    "# Change the data type to `float32` in order to avoid doing type conversions later on\n",
    "X_true = X_true.astype(np.float32)\n",
    "\n",
    "# Notice that we only output the model trajectory here, as we don't need the coupling terms for training here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd23807-0b39-4a92-9bef-d8edfcfd2871",
   "metadata": {},
   "source": [
    "We now need to set the number of time steps that the training process will use for every sample (how long of a trajectory are we trying to match). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0367d3f3-037d-4316-a827-bc45f5bb3b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Tstep_train = 128 \n",
    "\n",
    "# We split the simulation into ensembles members without any overlap, \n",
    "# and each of these will be used as a separate sample.\n",
    "N_ens = int(time_steps/ Tstep_train)\n",
    "X_true_ens = np.reshape(X_true[0:N_ens*Tstep_train,:], (N_ens, Tstep_train, 8) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97974dc3-09d8-4a41-8b53-e3fa36ba959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test\n",
    "\n",
    "# Set the number of time series that will be part of test ensemble. \n",
    "test_ens = 32\n",
    "\n",
    "# Training Data\n",
    "X_true_train = X_true_ens[0:-test_ens, :, :]  \n",
    "\n",
    "# Test Data\n",
    "X_true_test = X_true_ens[-test_ens:, :, :]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69869e4a-2cb3-481c-9070-1385e2c34084",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true_train.shape, X_true_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd66c5-c845-42ad-8d53-dc6a77c9f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sample in each batch\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ceba35-c4ae-452a-820b-5f2bd45105c4",
   "metadata": {},
   "source": [
    "Notice that in the training and testing datasets defined below the input to the model is the initial condition, and the output that the model will be evaluated against is a time series from the simulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988c083-3931-474c-bd73-53e8447d24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset\n",
    "# ----------------\n",
    "nlocal_data_train = Data.TensorDataset(\n",
    "    torch.from_numpy(X_true_train[:,0,:]), # expected input is an initial condition\n",
    "    torch.from_numpy(X_true_train) # expected output as a time series\n",
    ")\n",
    "\n",
    "nlocal_loader_train = Data.DataLoader(\n",
    "    dataset=nlocal_data_train, batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "# Test Dataset\n",
    "# ------------\n",
    "nlocal_data_test = Data.TensorDataset(\n",
    "    torch.from_numpy(X_true_test[:,0,:]), \n",
    "    torch.from_numpy(X_true_test)\n",
    ")\n",
    "nlocal_loader_test = Data.DataLoader(\n",
    "    dataset=nlocal_data_test, batch_size=BATCH_SIZE, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c2036-7b57-4172-a6f0-0e164765510e",
   "metadata": {},
   "source": [
    "## Create a differentiable 1 time-scale L96 model \n",
    "\n",
    "One of the key components needed for online training is a differentiable solver. This can be seen by the presence of $X^{param-LR}$ in the loss function, which indicates that derivatives should be able to pass through a function that not only corresponds to the extra terms that are added, but also produces the full solution.\n",
    "\n",
    "This relatively to easy to do with modern machine learning frameworks like PyTorch or JAX, as long as the model is simple enough to be rewritten using these frameworks. \n",
    "\n",
    "Here we will write a differentiable solver for :\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F.\n",
    "\\end{align}\n",
    "\n",
    "Notice below that in this case this task was as simple as using the word `torch` instead of `np`, basically swapping out numpy calls with PyTorch calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ad47f-cec7-446a-ad65-8cd900469341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L96_eq1_xdot_torch(X, F=20):\n",
    "    '''\n",
    "    Compared to older function works on batches,\n",
    "    i.e. dimension is Nbatch x K (single time step for many batches is input)\n",
    "    '''\n",
    "    return torch.roll(X, shifts=1, dims=-1) * (torch.roll(X, shifts=-1, dims=-1) - torch.roll(X, shifts= 2, dims=-1)) - X + F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5406a-9992-430d-a61f-008ef44b380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RK4(fn, dt, X, *kw):\n",
    "    \"\"\"\n",
    "    Calculate the new state X(n+1) for d/dt X = fn(X,t,...) using the fourth order Runge-Kutta method.\n",
    "    Args:\n",
    "        fn     : The function returning the time rate of change of model variables X\n",
    "        dt     : The time step\n",
    "        X      : Values of X variables at the current time, t\n",
    "        kw     : All other arguments that should be passed to fn, i.e. fn(X, t, *kw)\n",
    "    Returns:\n",
    "        X at t+dt\n",
    "    \"\"\"\n",
    "\n",
    "    Xdot1 = fn(X, *kw)\n",
    "    Xdot2 = fn(X + 0.5 * dt * Xdot1, *kw)\n",
    "    Xdot3 = fn(X + 0.5 * dt * Xdot2, *kw)\n",
    "    Xdot4 = fn(X + dt * Xdot3, *kw)\n",
    "    return X + (dt / 6.0) * ((Xdot1 + Xdot4) + 2.0 * (Xdot2 + Xdot3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7788d6-13cd-42ae-9249-0926be461470",
   "metadata": {},
   "source": [
    "## Define a Neural Network for the parameterization\n",
    "\n",
    "Here we use a neural network architecture that is exactly the same as the non-local architecture that was used in the previous notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a5262-39e2-4d50-a8ea-c8b028e5fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN_nonlocal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs\n",
    "        self.linear2 = nn.Linear(16, 16)\n",
    "        self.linear3 = nn.Linear(16, 8)  # 8 outputs\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d72c643-f0ac-42f8-99ee-615e9e2087fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn_online_network = FCNN_nonlocal().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f76d74f-26fa-47ba-b19f-819cafeeee03",
   "metadata": {},
   "source": [
    "## Loss function and optimizer for online training\n",
    "\n",
    "The target of the loss function in online training is not simply to match the sub-grid fluxes, but instead to also track the solution of the two time-scale model using the single time-scale model. \n",
    "\n",
    "The loss function below is where the magic happens. Notice two key elements: (1) the neural network is combined with the numerical solver that produces the tendency at every time step, (2) a time integration is performed over the number some number of time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9fffa9-07fa-4b40-84da-470f8202474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(NN, x_in, y_true):\n",
    "    \"\"\"\n",
    "    NN: the neural network that parameterizes the missing terms\n",
    "    x_in: initial conditions, which are the input. Shape (batch size X K)\n",
    "    y_true: full solution from the two time scale model. Shape (batch size X Nt X K)\n",
    "    \"\"\"\n",
    "    full_xdot = lambda x: NN(x) +  L96_eq1_xdot_torch(x, F=forcing) # make a function that returns tendency with NN as param.\n",
    "\n",
    "    y_pred = 0 * y_true # Use this to store the model prediction we make\n",
    "\n",
    "    y_pred[:,0,:] = x_in # intiailize IC (which is the only input to model).\n",
    "\n",
    "    for j in range(y_true.shape[1]-1): # loop over time steps\n",
    "\n",
    "        y_pred[:,j+1,:] = RK4(full_xdot, dt, y_pred[:,j,:].clone()) # time step forward. \n",
    "\n",
    "    return ((y_pred-y_true)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0ad15-5790-4236-8011-fbff56c55910",
   "metadata": {},
   "source": [
    "Since the only free parameters correspond to the weights of the neural network, they are passed to the optimizer. Notice that even though the loss function is much more complex than the offline training case, the parameters that are being optimized are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc3b3b-46c3-45a5-9d5c-7dab607015fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer. \n",
    "\n",
    "optimizer_fcnn = optim.Adam(fcnn_online_network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea74669-66f7-4637-acbf-8b8457668c08",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "First we define the helper functions for training, testing and fitting, same as we did in previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c342c1-f430-4085-bc10-efcafa1364d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, criterion, loader, optimizer):\n",
    "    \"\"\"Train the network for one epoch\"\"\"\n",
    "    network.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in loader:\n",
    "        \n",
    "        \n",
    "        # Compute the loss (now the predictions are done directly in the loss function).\n",
    "        loss = criterion(network, batch_x.to(device), batch_y.to(device))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backpropagation to compute the gradients and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss / len(loader)\n",
    "\n",
    "def test_model(network, criterion, loader):\n",
    "    \"\"\"Test the network\"\"\"\n",
    "    network.eval()  # Evaluation mode (important when having dropout layers)\n",
    "\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(network, batch_x.to(device), batch_y.to(device))\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        # Get an average loss for the entire dataset\n",
    "        test_loss /= len(loader)\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "def fit_model(network, criterion, optimizer, train_loader, test_loader, n_epochs):\n",
    "    \"\"\"Train and validate the network\"\"\"\n",
    "    train_losses, test_losses = [], []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_model(network, criterion, train_loader, optimizer)\n",
    "        test_loss = test_model(network, criterion, test_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {int(end_time - start_time)} seconds.\")\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329dace-bd5f-4625-9cc3-1b9a6604b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs refer to the number of times we iterate over the entire training data during training.\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea48b6c-cb48-47fd-af13-99c215cf683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "train_loss_online, test_loss_online = fit_model(\n",
    "    fcnn_online_network, compute_loss, optimizer_fcnn, nlocal_loader_train, nlocal_loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6329c-1b1f-4a85-8b4d-ea5387936a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(np.array(train_loss_online), label='Training loss')\n",
    "plt.plot(np.array(test_loss_online), label='Test loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30448682-8d16-48ca-b907-92ffaba2ea9b",
   "metadata": {},
   "source": [
    "The loss curve above shows that online model is training well, and parameters have been optimized in some sense. Let us check below how this online trained model compares against the offline trained model from the previous notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2932cc0-bada-4649-8121-f84b6e0daff3",
   "metadata": {},
   "source": [
    "## Test in a simulation: Online testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f9d38-2320-4490-a4a2-803de8df8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the previously trained non-local offline network.\n",
    "nonlocal_FCNN_weights = torch.load(\"./networks/non_local_FCNN.pth\")\n",
    "\n",
    "fcnn_offline_network = FCNN_nonlocal()\n",
    "fcnn_offline_network.load_state_dict(nonlocal_FCNN_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aeb502-5f07-4966-8cfe-4082757d8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The different GCM classes\n",
    "# ---------------------------\n",
    "\n",
    "class GCM_without_parameterization:\n",
    "    \"\"\"GCM without parameterization\n",
    "\n",
    "    Args:\n",
    "        F: Forcing term\n",
    "        time_stepping: Time stepping method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F, time_stepping=RK4):\n",
    "        self.F = F\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, _):\n",
    "        \"\"\"Compute right hand side of the the GCM equations\"\"\"\n",
    "        return L96_eq1_xdot(X, self.F)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"Run GCM\n",
    "\n",
    "        Args:\n",
    "            X0: Initial conditions of X\n",
    "            dt: Time increment\n",
    "            nt: Number of forward steps to take\n",
    "            param: Parameters of closure\n",
    "\n",
    "        Returns:\n",
    "            Model output for all variables of X at each timestep\n",
    "            along with the corresponding time units\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time\n",
    "    \n",
    "class GCM_network:\n",
    "    \"\"\"GCM with neural network parameterization\n",
    "\n",
    "    Args:\n",
    "        F: Forcing term\n",
    "        network: Neural network\n",
    "        time_stepping: Time stepping method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F, network, time_stepping=RK4):\n",
    "        self.F = F\n",
    "        self.network = network\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, _):\n",
    "        \"\"\"Compute right hand side of the the GCM equations\"\"\"\n",
    "        if self.network.linear1.in_features == 1:\n",
    "            X_torch = torch.from_numpy(X)\n",
    "            X_torch = torch.unsqueeze(X_torch, 1)\n",
    "        else:\n",
    "            X_torch = torch.from_numpy(np.expand_dims(X, 0))\n",
    "\n",
    "        # Adding NN parameterization\n",
    "        return L96_eq1_xdot(X, self.F) + np.squeeze(self.network(X_torch).data.numpy())\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"Run GCM\n",
    "\n",
    "        Args:\n",
    "            X0: Initial conditions of X\n",
    "            dt: Time increment\n",
    "            nt: Number of forward steps to take\n",
    "            param: Parameters of closure\n",
    "\n",
    "        Returns:\n",
    "            Model output for all variables of X at each timestep\n",
    "            along with the corresponding time units\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6a3ac2-d75d-4643-8aa8-33c3b7a353d1",
   "metadata": {},
   "source": [
    "Now let us pick a random point in the simulation as our initial condition, and compare if there is some drastic difference between offline and online parameterization that can be seen visually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d35701-3fb8-4360-af23-01dff0ff28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 2100\n",
    "init_conditions = X_true[start, :]\n",
    "T_test = 10\n",
    "forcing = 18\n",
    "dt = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4586d4-55f9-4c28-b7dc-73ea5284d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcm_no_param = GCM_without_parameterization(forcing)\n",
    "X_no_param, t = gcm_no_param(init_conditions, dt, int(T_test / dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f023ad5-b6b0-4e3e-b75b-834d14daa2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with nonlocal offline FCNN\n",
    "gcm_nonlocal_offline_net = GCM_network(forcing, fcnn_offline_network)\n",
    "Xnn_nonlocal_offline, t = gcm_nonlocal_offline_net(\n",
    "    init_conditions, dt, int(T_test / dt), fcnn_offline_network\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fa095e-9d5d-4d99-aa6b-31b8f7e56df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with nonlocal offline FCNN\n",
    "gcm_nonlocal_online_net = GCM_network(forcing, fcnn_online_network)\n",
    "Xnn_nonlocal_online, t = gcm_nonlocal_online_net(\n",
    "    init_conditions, dt, int(T_test / dt), fcnn_online_network\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ff02e-6acc-4c66-b0a5-461e9358c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_i = 300\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(t[:time_i], X_true[start:start+time_i, 4], label=\"Full L96\", color='k')\n",
    "\n",
    "plt.plot(t[:time_i], X_no_param[:time_i, 4], \"--\", label=\"No parameterization\")\n",
    "\n",
    "plt.plot(t[:time_i], Xnn_nonlocal_offline[:time_i, 4], label=\"Offline parameterization\")\n",
    "\n",
    "plt.plot(t[:time_i], Xnn_nonlocal_online[:time_i, 4], label=\"Online parameterization\")\n",
    "\n",
    "plt.hlines(0, t[0], t[Tstep_train], label=\"time range of online training\", linestyle='-.', color='r')\n",
    "\n",
    "plt.legend(loc=\"lower right\", fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae711913-49b6-43b9-a70b-368fcfcac88e",
   "metadata": {},
   "source": [
    "The above plot shows that both offline and online trained models perform much better than the simulation without any parameterization. However, it is unclear if there is any signficant gain in the online case. To be more precise, we compare the different cases below over many simulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c3563-51d7-4949-b8ff-14df98afed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_noparam, err_offline, err_online = [], [], []\n",
    "T_test = 1\n",
    "\n",
    "for i in range(100):\n",
    "    init_conditions_i = X_true[i * 10, :]\n",
    "\n",
    "    # Evaluate with no parameterization\n",
    "    gcm_no_param = GCM_without_parameterization(forcing)\n",
    "    X_no_param, t = gcm_no_param(init_conditions, dt, int(T_test / dt))\n",
    "\n",
    "    # Evaluate with local FCNN\n",
    "    gcm_nonlocal_offline_net = GCM_network(forcing, fcnn_offline_network)\n",
    "    Xnn_nonlocal_offline, t = gcm_nonlocal_offline_net(\n",
    "                    init_conditions, dt, int(T_test / dt), fcnn_offline_network\n",
    "                                                    )\n",
    "\n",
    "    # Evaluate with nonlocal FCNN\n",
    "    gcm_nonlocal_online_net = GCM_network(forcing, fcnn_online_network)\n",
    "    Xnn_nonlocal_online, t = gcm_nonlocal_online_net(\n",
    "                    init_conditions, dt, int(T_test / dt), fcnn_online_network\n",
    "                                )\n",
    "\n",
    "    \n",
    "    err_noparam.append(\n",
    "        np.sum(np.abs(X_true[i * 10 : i * 10 + T_test * 100 + 1] - X_no_param))\n",
    "    )\n",
    "\n",
    "    err_offline.append(\n",
    "        np.sum(np.abs(X_true[i * 10 : i * 10 + T_test * 100 + 1] - Xnn_nonlocal_offline))\n",
    "    )\n",
    "\n",
    "    err_online.append(\n",
    "        np.sum(np.abs(X_true[i * 10 : i * 10 + T_test * 100 + 1] - Xnn_nonlocal_online))\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Sum of errors for no parameterization: {sum(err_noparam):.2f}\")\n",
    "print(f\"Sum of errors for offline parameterization: {sum(err_offline):.2f}\")\n",
    "print(f\"Sum of errors for online parameterization: {sum(err_online):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336de28-783d-4dfe-ae60-8774f39057bf",
   "metadata": {},
   "source": [
    "This assessment shows that the online parameterization performs about the same as offline parameterzation. However, atleast for the L96 model the gains (if any), which come at the cost of signfiant complexity, are not drastic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab018fe-819f-4dd2-a208-161ea0368d82",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we described how online training differs from offline training, and provided an example of how online training can be done for the L96 model. While for the L96 model the online training procedure did not produce significant improvements, the gains for other models may be much greater. If interested, you may look at this [blog post](https://raspstephan.github.io/blog/lorenz-96-is-too-easy/) arguing that L96 might be too simple a test case, which is why different training procedures do not result in very significant differences.\n",
    "\n",
    "In the next few notebooks we show a few tricks to potentially improve performance of neural networks, how to interpret neural networks, and how physics constraints can be added in their architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b02a1-c8f5-4dbd-91b8-cbe2f8d138aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
