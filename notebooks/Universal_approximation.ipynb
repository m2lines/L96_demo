{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d2675f-796a-4466-a76a-c9a354518d1b",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa918476-598e-4ec0-9d9c-b2fd20d2e699",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "The **Universal Approximation Theorm** states that neural networks can approximate any continuous function. A visual demonstration that neural nets can compute any function can be seen in [this page](http://neuralnetworksanddeeplearning.com/chap4.html).\n",
    "\n",
    "In this notebook, we give a brief overview of neural networks and how to build them using PyTorch. If you want to go through it in depth, check out these resources:\n",
    "- [Deep Learning With Pytorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "- [Neural Networks](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a18f4-f3d5-4503-8c0b-e81895ee2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "from IPython.display import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torch import nn, optim\n",
    "\n",
    "from L96_model import L96, RK2, RK4, EulerFwd, L96_eq1_xdot, integrate_L96_2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79044f9a-ac01-415a-a88a-0385718c8b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring reproducibility\n",
    "np.random.seed(14)\n",
    "torch.manual_seed(14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555922c-f58e-4343-8cf0-1ad027046c6a",
   "metadata": {},
   "source": [
    "### Build the *Real World* to Generate the Ground Truth Dataset\n",
    "\n",
    "We initialise the L96 two time-scale model using $K$ (set to 8) values of $X$ and $J$ (set to 32) values of $Y$ for each $X$. The model is run for 20,000 timesteps to generate the dataset for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0738e8-fd6f-4f0c-befa-aa5db97f4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 20000\n",
    "forcing, dt, T = 18, 0.01, 0.01 * time_steps\n",
    "\n",
    "# Create a \"real world\" with K=8 and J=32\n",
    "W = L96(8, 32, F=forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df4af7-7283-4a64-8a3d-cbb7dada4363",
   "metadata": {},
   "source": [
    "### Getting Training Data\n",
    "\n",
    "Using the *real world* model created above we generate the training data (input and output pairs) for the neural network by running the true state and outputting subgrid tendencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3e6b4-269f-4ac5-82e8-366eb2fb6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The effect of Y on X is `xy_true`\n",
    "X_true, _, _, xy_true = W.run(dt, T, store=True, return_coupling=True)\n",
    "\n",
    "# Change the data type to `float32` in order to avoid doing type conversions later on\n",
    "X_true, xy_true = X_true.astype(np.float32), xy_true.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4586ecb-43a0-4764-9b6f-c968be40eca9",
   "metadata": {},
   "source": [
    "### Split the Data to obtain the Training and Test (Validation) Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f99293a-df6d-4d60-b177-03fc2f8ae361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time steps for validation\n",
    "val_size = 4000\n",
    "\n",
    "# Training Data\n",
    "X_true_train = X_true[\n",
    "    :-val_size, :\n",
    "]  # Flatten because we first use single input as a sample\n",
    "subgrid_tend_train = xy_true[:-val_size, :]\n",
    "\n",
    "# Test Data\n",
    "X_true_test = X_true[-val_size:, :]\n",
    "subgrid_tend_test = xy_true[-val_size:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac52993-712b-4aba-8012-7ca83a4d605e",
   "metadata": {},
   "source": [
    "### Create Dataloaders \n",
    "\n",
    "- `Dataset` and `Dataloader` classes provide a very convenient way of iterating over a dataset while training a deep learning model.\n",
    "\n",
    "- We need to iterate over the data because it is very slow and memory-intensive to hold all the data and to use gradient decent over all the data simultaneously (see more details [here](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) and [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e751231-d506-4bee-982d-c7522a4481e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sample in each batch\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615518a-e6d8-4a9d-ad05-b334fd7f8224",
   "metadata": {},
   "source": [
    "Define the X (state), Y (subgrid tendency) pairs for the linear regression local network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e3982-1791-418e-bf8d-81475da84a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataset = Data.TensorDataset(\n",
    "    torch.from_numpy(np.reshape(X_true_train, -1)),\n",
    "    torch.from_numpy(np.reshape(subgrid_tend_train, -1)),\n",
    ")\n",
    "\n",
    "local_loader = Data.DataLoader(\n",
    "    dataset=local_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ef6f0-9441-49c5-8cea-904c992d75c1",
   "metadata": {},
   "source": [
    "Define the dataloader for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec82d6-91e3-4851-993f-fa28836af5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(np.reshape(X_true_test, -1)),\n",
    "    torch.from_numpy(np.reshape(subgrid_tend_test, -1)),\n",
    ")\n",
    "\n",
    "local_loader_test = Data.DataLoader(\n",
    "    dataset=local_dataset_test, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff24b3-f8c9-4b5e-a36f-82d6b0f177d1",
   "metadata": {},
   "source": [
    "Display a batch of samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527ff9a-01c8-419a-9ca4-c28a38a54b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over the data to get one batch\n",
    "data_iterator = iter(local_loader)\n",
    "X_iter, subgrid_tend_iter = next(data_iterator)\n",
    "\n",
    "print(\"X (State):\\n\", X_iter)\n",
    "print(\"\\nY (Subgrid Tendency):\\n\", subgrid_tend_iter)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(X_iter, subgrid_tend_iter, \".\")\n",
    "plt.xlabel(\"State\", fontsize=20)\n",
    "plt.ylabel(\"Subgrid tendency\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eff251-d0c0-4628-b617-6189915e9daa",
   "metadata": {},
   "source": [
    "### Neural Network Architectures\n",
    "\n",
    "We will try to understand the fully connected networks with the help of Linear regression (and gradient descent).\n",
    "\n",
    "<center>\n",
    "  <img\n",
    "    src=\"https://miro.medium.com/max/720/1*VHOUViL8dHGfvxCsswPv-Q.png\"\n",
    "    width=400\n",
    "  />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e53bd-fb12-4aad-bcff-be495935ef80",
   "metadata": {},
   "source": [
    "### Building a Linear Regression Network\n",
    "\n",
    "First, we will build a linear regression \"network\" and later see how to generalize the linear regression in order to use fully connected neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed1ac73-a213-4d2c-81af-d74b093d2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(1, 1)  # A single input and a single output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This method is automatically executed when\n",
    "        # we call a object of this class\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a3d2f-7f95-4906-a3b0-dd2ecf81822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_network = LinearRegression()\n",
    "linear_network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f6fd5-0055-4583-8677-ae31ac9953e9",
   "metadata": {},
   "source": [
    "### Test forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b236b7f-dd79-4f88-8e8a-e60bfbc2a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_input = torch.randn(1, 1)\n",
    "out = linear_network(net_input)\n",
    "print(f\"The output of the random input is: {out.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471c571-5ee4-48ef-bfce-e2349bcdfe5b",
   "metadata": {},
   "source": [
    "### Defining the Loss Function\n",
    "\n",
    "In order to check how well our network is modeling the dataset, we need to define a loss function. For our task, we choose the *Mean Squared Error* metric as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42de43-27de-4896-91ee-caf695f212df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss function\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c7361-f717-4a74-b296-075f398d7072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input and output pair from the data loader\n",
    "X_tmp = next(iter(local_loader))\n",
    "\n",
    "# Predict the output\n",
    "y_tmp = linear_network(torch.unsqueeze(X_tmp[0], 1))\n",
    "\n",
    "# Calculate the MSE loss\n",
    "loss = criterion(y_tmp, torch.unsqueeze(X_tmp[1], 1))\n",
    "print(f\"MSE Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041270e-2857-4dce-bcd7-05f61694deab",
   "metadata": {},
   "source": [
    "### Calculating gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877e848-2f98-4bb9-b5a2-eb80fdefb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero the gradient buffers of all parameters\n",
    "linear_network.zero_grad()\n",
    "\n",
    "print(\"Gradients before backward:\")\n",
    "print(linear_network.linear1.bias.grad)\n",
    "\n",
    "# Compute the gradients\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "print(\"\\nGradients after backward:\")\n",
    "print(linear_network.linear1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98716e66-ed26-48f4-8509-9f25e0ddeebf",
   "metadata": {},
   "source": [
    "### Updating the Weights using an Optimizer\n",
    "\n",
    "Now in order to make the network learn, we need an algorithm that will update its weights depending on the loss function. This is achieved by using an optimizer. The implementation of almost every optimizer that we'll ever need can be found in PyTorch itself. The choice of which optimizer we choose might be very important as it will determine how fast the network will be able to learn.\n",
    "\n",
    "In the example below, we show one of the popular optimizers `SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed28fd9-441e-4e01-9e3c-7571ff7a311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(linear_network.parameters(), lr=0.003, momentum=0.9)\n",
    "print(\"Before backward pass: \\n\", list(linear_network.parameters())[0].data.numpy())\n",
    "\n",
    "loss.backward(retain_graph=True)\n",
    "optimizer.step()\n",
    "\n",
    "print(\"\\nAfter backward pass: \\n\", list(linear_network.parameters())[0].data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2fefc-1479-43b2-b8de-24321a0dff60",
   "metadata": {},
   "source": [
    "An optimizer usually consists of two major hyperparameters called the **learning rate** and **momentum**. The **learning rate** determines the magnitude with which the weights of the network update thus making it crucial to choose the correct learning rate ($LR$) otherwise the network will either fail to train, or take much longer to converge. To read about **momentum**, check out this [blog post](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d).\n",
    "\n",
    "The  effective value of the gradient $V$ at step $t$ in SGD with momentum ($\\beta$) is determined by\n",
    "\n",
    "\\begin{equation}\n",
    "V_t = \\beta V_{t-1} + (1-\\beta) \\nabla_w L(W,X,y)\n",
    "\\end{equation}\n",
    "\n",
    "and the updates to the weights will be\n",
    "\n",
    "\\begin{equation}\n",
    "w^{new} = w^{old} - LR * V_t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2a2478-e1f7-45db-b9e0-a9a511a4a912",
   "metadata": {},
   "source": [
    "#### Adam Optimizer\n",
    "\n",
    "Another popular optimizer that is used in many neural networks is the Adam optimizer. It is an adaptive learning rate method that computes individual learning rates for different parameters. For further reading, check out this [post](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c) about Adam, and this [post](https://www.ruder.io/optimizing-gradient-descent/) about other optimizers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f28f566-bb81-4d2d-9acc-b24ae80712c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Combining it all Together: Training the Whole Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66f8a8-8c11-4897-b71e-ac06d03271ac",
   "metadata": {},
   "source": [
    "### Define the Training and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800f28f-69c5-40eb-9b0c-cb27059f2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, criterion, loader, optimizer):\n",
    "    \"\"\"Train the network for one epoch\"\"\"\n",
    "    network.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        # Get predictions\n",
    "        if len(batch_x.shape) == 1:\n",
    "            # This if block is needed to add a dummy dimension if our inputs are 1D\n",
    "            # (where each number is a different sample)\n",
    "            prediction = torch.squeeze(network(torch.unsqueeze(batch_x, 1)))\n",
    "        else:\n",
    "            prediction = network(batch_x)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(prediction, batch_y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backpropagation to compute the gradients and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963a83c-c7ba-42a6-9642-002e5d254c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(network, criterion, loader):\n",
    "    \"\"\"Test the network\"\"\"\n",
    "    network.eval()  # Evaluation mode (important when having dropout layers)\n",
    "\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            # Get predictions\n",
    "            if len(batch_x.shape) == 1:\n",
    "                # This if block is needed to add a dummy dimension if our inputs are 1D\n",
    "                # (where each number is a different sample)\n",
    "                prediction = torch.squeeze(network(torch.unsqueeze(batch_x, 1)))\n",
    "            else:\n",
    "                prediction = network(batch_x)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(prediction, batch_y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        # Get an average loss for the entire dataset\n",
    "        test_loss /= len(loader)\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8b24a-3107-48b4-8b52-d26776fa95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(network, criterion, optimizer, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"Train and validate the network\"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_model(network, criterion, train_loader, optimizer)\n",
    "        val_loss = test_model(network, criterion, val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {int(end_time - start_time)} seconds.\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c15e2-c4ea-4de7-9fe6-2f5cbc656f03",
   "metadata": {},
   "source": [
    "### Set Hyperparameters\n",
    "\n",
    "Epochs refer to the number of times we iterate over the entire training data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447887a6-4c7e-4146-b7c9-0962594ad997",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "optimizer = optim.Adam(linear_network.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2052dcbd-a72d-40d8-a9ba-4657630e48ae",
   "metadata": {},
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39cc83-2cb4-4e54-95cf-e11b6240dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = fit_model(\n",
    "    linear_network, criterion, optimizer, local_loader, local_loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b387b0-3f08-4871-a170-1fcb029df63d",
   "metadata": {},
   "source": [
    "### Show the Weights of the Trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cc206-7c55-4813-ab65-4259a9290609",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array(\n",
    "    [\n",
    "        linear_network.linear1.weight.data.numpy()[0][0],\n",
    "        linear_network.linear1.bias.data.numpy()[0],\n",
    "    ]\n",
    ")\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9615e08d-6399-49e4-9c7f-0cbd171d084f",
   "metadata": {},
   "source": [
    "### Compare Predictions with Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aab879-0337-4b5e-8dc9-cdb9497c9103",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = linear_network(\n",
    "    torch.unsqueeze(torch.from_numpy(np.reshape(X_true_test[:, 1], -1)), 1)\n",
    ")\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(predictions.detach().numpy()[0:1000], label=\"Predicted Values\")\n",
    "plt.plot(subgrid_tend_test[:1000, 1], label=\"True Values\")\n",
    "plt.legend(fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c2119b-b4a2-496f-b090-16e65b309af0",
   "metadata": {},
   "source": [
    "## Putting the Simple Linear Parameterization back to the GCM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdd1a56-598f-44e4-ab34-9ffdacd72406",
   "metadata": {},
   "source": [
    "### Defining the GCM Classes\n",
    "\n",
    "In all the GCMs, we set time stepping using the fourth order Runge-Kutta method.\n",
    "\n",
    "To recap, {cite}`Lorenz1995` describes a two-time scale dynamical system using two equations which are:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F - \\left( \\frac{hc}{b} \\right) \\sum_{j=0}^{J-1} Y_{j,k}\n",
    "\\end{gather*}\n",
    "\n",
    "\\begin{gather*}\n",
    "\\frac{d}{dt} Y_{j,k}\n",
    "&= - cbY_{j+1,k} \\left( Y_{j+2,k} - Y_{j-1,k} \\right) - c Y_{j,k} + \\frac{hc}{b} X_k\n",
    "\\end{gather*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a0020-ccfc-44fe-b589-103b954e90ff",
   "metadata": {},
   "source": [
    "#### *Without* Neural Network Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22fac20-3cf9-4aec-b0eb-707c748a393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCM_without_parameterization:\n",
    "    \"\"\"GCM without parameterization\n",
    "\n",
    "    Args:\n",
    "        F: Forcing term\n",
    "        time_stepping: Time stepping method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F, time_stepping=RK4):\n",
    "        self.F = F\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, _):\n",
    "        \"\"\"Compute right hand side of the the GCM equations\"\"\"\n",
    "        return L96_eq1_xdot(X, self.F)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"Run GCM\n",
    "\n",
    "        Args:\n",
    "            X0: Initial conditions of X\n",
    "            dt: Time increment\n",
    "            nt: Number of forward steps to take\n",
    "            param: Parameters of closure\n",
    "\n",
    "        Returns:\n",
    "            Model output for all variables of X at each timestep\n",
    "            along with the corresponding time units\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99fd070-2336-4453-a429-7a409869ea57",
   "metadata": {},
   "source": [
    "#### Linear Parameterization in RHS of Equation for Tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec86952-2c9d-4af0-81ec-aaf8957070cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCM_linear_parameterization:\n",
    "    \"\"\"GCM with linear parameterization\n",
    "\n",
    "    Args:\n",
    "        F: Forcing term\n",
    "        parameterization: Parameterization function\n",
    "        time_stepping: Time stepping method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F, parameterization, time_stepping=RK4):\n",
    "        self.F = F\n",
    "        self.parameterization = parameterization\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, param):\n",
    "        \"\"\"Compute right hand side of the the GCM equations\"\"\"\n",
    "        return L96_eq1_xdot(X, self.F) - self.parameterization(param, X)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"Run GCM\n",
    "\n",
    "        Args:\n",
    "            X0: Initial conditions of X\n",
    "            dt: Time increment\n",
    "            nt: Number of forward steps to take\n",
    "            param: Parameters of closure\n",
    "\n",
    "        Returns:\n",
    "            Model output for all variables of X at each timestep\n",
    "            along with the corresponding time units\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6009cc-e9e1-454a-a92a-3b7bf23e1c70",
   "metadata": {},
   "source": [
    "#### *With* Neural Network Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f6265-8c8f-456b-b951-b84364e98f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCM_network:\n",
    "    \"\"\"GCM with neural network parameterization\n",
    "\n",
    "    Args:\n",
    "        F: Forcing term\n",
    "        network: Neural network\n",
    "        time_stepping: Time stepping method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F, network, time_stepping=RK4):\n",
    "        self.F = F\n",
    "        self.network = network\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, _):\n",
    "        \"\"\"Compute right hand side of the the GCM equations\"\"\"\n",
    "        if self.network.linear1.in_features == 1:\n",
    "            X_torch = torch.from_numpy(X)\n",
    "            X_torch = torch.unsqueeze(X_torch, 1)\n",
    "        else:\n",
    "            X_torch = torch.from_numpy(np.expand_dims(X, 0))\n",
    "\n",
    "        # Adding NN parameterization\n",
    "        return L96_eq1_xdot(X, self.F) + np.squeeze(self.network(X_torch).data.numpy())\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"Run GCM\n",
    "\n",
    "        Args:\n",
    "            X0: Initial conditions of X\n",
    "            dt: Time increment\n",
    "            nt: Number of forward steps to take\n",
    "            param: Parameters of closure\n",
    "\n",
    "        Returns:\n",
    "            Model output for all variables of X at each timestep\n",
    "            along with the corresponding time units\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e909da98-e156-48ed-a31b-50f983c4161e",
   "metadata": {},
   "source": [
    "Driver code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c448fc-ee16-465d-b3ed-4985cec70fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test = 10\n",
    "\n",
    "# Full L96 model\n",
    "X_full, _, _ = W.run(dt, T_test)\n",
    "X_full = X_full.astype(np.float32)\n",
    "\n",
    "init_conditions = X_true[-1, :]\n",
    "\n",
    "# GCM parameterized by the linear network\n",
    "gcm_net = GCM_network(forcing, linear_network)\n",
    "Xnn_1layer, t = gcm_net(init_conditions, dt, int(T_test / dt), linear_network)\n",
    "\n",
    "# GCM parameterized without parameterization\n",
    "gcm_no_param = GCM_without_parameterization(forcing)\n",
    "X_no_param, t = gcm_no_param(init_conditions, dt, int(T_test / dt))\n",
    "\n",
    "# GCM with naive parameterization\n",
    "naive_parameterization = lambda param, X: np.polyval(param, X)\n",
    "gcm = GCM_linear_parameterization(forcing, naive_parameterization)\n",
    "X_param, t = gcm(init_conditions, dt, int(T / dt), param=-weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9642e15b-9b5e-4fd6-b2d3-b508ac508dc3",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acceb41-c31c-4cac-ae72-a770805c75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_i = 200\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(t[:time_i], X_full[:time_i, 4], label=\"Full L96\")\n",
    "plt.plot(t[:time_i], Xnn_1layer[:time_i, 4], \".\", label=\"NN 1 layer\")\n",
    "plt.plot(t[:time_i], X_no_param[:time_i, 4], label=\"No parameterization\")\n",
    "plt.plot(t[:time_i], X_param[:time_i, 4], label=\"linear parameterization\")\n",
    "plt.legend(loc=\"upper left\", fontsize=7);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
