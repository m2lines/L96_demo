{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41242f2",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Symbolic Regression vs. Neural Networks on Multiscale L96\n",
    "\n",
    "To recap from the previous notebooks, [Lorenz (1996)](https://www.ecmwf.int/en/elibrary/10829-predictability-problem-partly-solved) describes a \"two time-scale\" model in two equations which are:\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F - \\left( \\frac{hc}{b} \\right) \\sum_{j=0}^{J-1} Y_{j,k}\n",
    "\\\\\n",
    "\\frac{d}{dt} Y_{j,k}\n",
    "&= - cbY_{j+1,k} \\left( Y_{j+2,k} - Y_{j-1,k} \\right) - c Y_{j,k} + \\frac{hc}{b} X_k\n",
    "\\end{align}\n",
    "\n",
    "This model can be visualized as follows:\n",
    "\n",
    "```{figure} https://www.researchgate.net/publication/319201436/figure/fig1/AS:869115023589376@1584224577926/Visualisation-of-a-two-scale-Lorenz-96-system-with-J-8-and-K-6-Global-scale-values.png\n",
    ":width: 400px\n",
    ":name: l96-equation-figure-symbolicvnn\n",
    "\n",
    "*Visualisation of a two-scale Lorenz '96 system with J = 8 and K = 6. Global-scale variables ($X_k$) are updated based on neighbouring variables and on the local-scale variables ($Y_{j,k}$) associated with the corresponding global-scale variable. Local-scale variabless are updated based on neighbouring variables and the associated global-scale variable. The neighbourhood topology of both local and global-scale variables is circular. Image from [Exploiting the chaotic behaviour of atmospheric models with reconfigurable architectures - Scientific Figure on ResearchGate.](https://www.researchgate.net/figure/Visualisation-of-a-two-scale-Lorenz-96-system-with-J-8-and-K-6-Global-scale-values_fig1_319201436)*.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070565d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import gplearn\n",
    "import gplearn.genetic\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydot\n",
    "import pysindy as ps\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from L96_model import L96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ecc5a-b99e-4dbb-8b46-3920ef9e3513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensuring reproducibility\n",
    "np.random.seed(14)\n",
    "torch.manual_seed(14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d8bd2-e489-44d2-9791-fe7b453c5636",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Dataset Generation\n",
    "\n",
    "Let's generate some data using the same conditions as in the previous notebooks [Introduction to Neural Networks](https://m2lines.github.io/L96_demo/notebooks/Universal_approximation.html) and [Using Neural Networks for L96 Parameterization](https://m2lines.github.io/L96_demo/notebooks/Neural_network_for_Lorenz96.html).\n",
    "\n",
    "### Obtaining the Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1437d-188e-427f-8da9-5e955cdc58b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_steps = 20000\n",
    "forcing, dt, T = 18, 0.01, 0.01 * time_steps\n",
    "\n",
    "W = L96(8, 32, F=forcing)\n",
    "\n",
    "X_true, _, _, subgrid_tend = W.run(dt, T, store=True, return_coupling=True)\n",
    "X_true, subgrid_tend = X_true.astype(np.float32), subgrid_tend.astype(np.float32)\n",
    "\n",
    "print(f\"X_true shape: {X_true.shape}\")\n",
    "print(f\"subgrid_tend shape: {subgrid_tend.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0350f65",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "In this case, although we have 8 different dimensions in $X$, each is derived in the exact same way from neighboring values of $X$ (and local subdimensions $Y$). Therefore, we can actually reduce this problem from an 8-to-8D regression problem down to an 8-to-1D regression problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54892735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Dx = X_true.shape[1]\n",
    "inputs = np.vstack([X_true[:, range(-i, Dx - i)] for i in range(Dx)])\n",
    "targets = np.hstack([subgrid_tend[:, -i] for i in range(Dx)])\n",
    "\n",
    "print(f\"inputs shape: {inputs.shape}\")\n",
    "print(f\"targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d3e5a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4), dpi=150)\n",
    "plt.title(\"Multiscale L96 trajectory\", fontsize=16)\n",
    "for i in range(Dx):\n",
    "    plt.plot(inputs[:, i][:500], label=r\"$x\" + str(i) + \"$\")\n",
    "plt.legend(ncol=2, fontsize=14)\n",
    "plt.xlabel(\"Time\", fontsize=16)\n",
    "plt.ylabel(\"Value\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be290f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6), dpi=150)\n",
    "fig.suptitle(\"2D histograms of features vs. forcing\", fontsize=16)\n",
    "for i in range(Dx):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.hist2d(inputs[:, i], targets, bins=50)\n",
    "    plt.xlabel(f\"$x_{i}$\", fontsize=16)\n",
    "    plt.xticks(np.arange(-6, 13, 2))\n",
    "    plt.xlim(-8, 14)\n",
    "    if i % 4 == 0:\n",
    "        plt.ylabel(\"Forcing\", fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e202f0",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Looking at these marginal histograms of each large-scale variable vs. the subgrid forcing, we can see there's a mostly linear relationship between $x_0$ and its forcing, suggesting that the subgrid terms tend to force small values higher and large values lower. This suggests that an extremely simple linear model only containing $x_0$ with negative slope will do reasonably well. However, the relationship isn't _completely_ linear, and also contains a bifurcation at larger $x_0$.\n",
    "\n",
    "For the other features, the relationships are less straightforward, but there's clearly high mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa935b26-4491-4678-a94a-48675c9dea1b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Split the Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd24ec2c-f8ad-4481-b81e-397af8239330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_idxs = np.arange(len(inputs))\n",
    "np.random.shuffle(data_idxs)\n",
    "\n",
    "num_train_samples = int(len(inputs) * 0.75)\n",
    "train_idxs = data_idxs[:num_train_samples]\n",
    "test_idxs = data_idxs[num_train_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b46ea2a-eb12-4be7-822d-bc0f8f93fb58",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Build the Dataset and the Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288a4a4-078b-44d5-8e41-1e058de861ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "\n",
    "# Training Data\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(inputs[train_idxs]), torch.tensor(targets[train_idxs])\n",
    ")\n",
    "loader_train = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Test Data\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(inputs[test_idxs]), torch.tensor(targets[test_idxs])\n",
    ")\n",
    "loader_test = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745bcc69-4d0f-4b3e-a1de-41e341b4eb6d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Creating and Training the Neural Network\n",
    "\n",
    "Now let's create a neural network and train it on the dataset we generated above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bcfcb9-b79e-4777-8e55-a31373e5241b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Define Functions to Train and Evaluate the Model\n",
    "\n",
    "```{note}\n",
    "The code in this section is taken from the notebook [Introduction to Neural Networks](https://m2lines.github.io/L96_demo/notebooks/Universal_approximation.html).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed1572-d504-47db-b962-98a1300ec4a1",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def train_model(network, criterion, loader, optimizer):\n",
    "    \"\"\"Train the network for one epoch\"\"\"\n",
    "    network.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        # Get predictions\n",
    "        if len(batch_x.shape) == 1:\n",
    "            # This if block is needed to add a dummy dimension if our inputs are 1D\n",
    "            # (where each number is a different sample)\n",
    "            prediction = torch.squeeze(network(torch.unsqueeze(batch_x, 1)))\n",
    "        else:\n",
    "            prediction = network(batch_x)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(prediction, batch_y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backpropagation to compute the gradients and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss / len(loader)\n",
    "\n",
    "\n",
    "def test_model(network, criterion, loader):\n",
    "    \"\"\"Test the network\"\"\"\n",
    "    network.eval()  # Evaluation mode (important when having dropout layers)\n",
    "\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            # Get predictions\n",
    "            if len(batch_x.shape) == 1:\n",
    "                # This if block is needed to add a dummy dimension if our inputs are 1D\n",
    "                # (where each number is a different sample)\n",
    "                prediction = torch.squeeze(network(torch.unsqueeze(batch_x, 1)))\n",
    "            else:\n",
    "                prediction = network(batch_x)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(prediction, batch_y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        # Get an average loss for the entire dataset\n",
    "        test_loss /= len(loader)\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def fit_model(network, criterion, optimizer, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"Train and validate the network\"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_model(network, criterion, train_loader, optimizer)\n",
    "        val_loss = test_model(network, criterion, val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {int(end_time - start_time)} seconds.\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6526b2d4",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Create the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6445e5d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net_ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_ANN, self).__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)\n",
    "        self.linear2 = nn.Linear(16, 16)\n",
    "        self.linear3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)[:, 0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dadd73-44d8-4aa4-b7a7-72352dc9ccaa",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab123e48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn_3l = Net_ANN()\n",
    "\n",
    "n_epochs = 20\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(nn_3l.parameters(), lr=0.01)\n",
    "train_loss, validation_loss = fit_model(\n",
    "    nn_3l, criterion, optimizer, loader_train, loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd373a-2f8e-427e-9ac4-67eb365f9675",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a0eec2-007c-49fa-96ca-8b32ac839d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"Training loss\")\n",
    "plt.plot(validation_loss, \"r\", label=\"Validation loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5965164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_nn = nn_3l(torch.from_numpy(inputs[test_idxs])).detach().numpy()\n",
    "print(\"R2 Score:\", r2_score(preds_nn, targets[test_idxs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1dacc",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The neural network gets an $R^2$ of ~ 0.916, which seems pretty good given the non-determinism of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3aaddc-fcc8-41eb-ab37-cc923a83aeb8",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Comparing with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec28d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(inputs[train_idxs], targets[train_idxs])\n",
    "print(\"R2 Score:\", linear_reg.score(inputs[test_idxs], targets[test_idxs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1677913e",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Linear regression also does fairly well, though about 0.1 lower in terms of $R^2$. Lets look at its coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f9d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.bar(range(8), linear_reg.coef_)\n",
    "plt.xticks(range(8), [f\"x{i}\" for i in range(8)], fontsize=12)\n",
    "plt.ylabel(\"Coefficient value\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb0893",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "This matches the 2D histogram plots from earlier that showed a negative linear relationship between forcing and $x_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a4893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6), dpi=150)\n",
    "plt.title(\"Neural Network vs. Linear Regression\", fontsize=16)\n",
    "\n",
    "preds_nn = nn_3l(torch.from_numpy(inputs)).detach().numpy()\n",
    "preds_lr = linear_reg.predict(inputs)\n",
    "random_idx = np.random.choice(len(targets - 1000))\n",
    "random_slice = slice(random_idx, random_idx + 1000)\n",
    "\n",
    "plt.plot(preds_nn[random_slice], label=\"NN Predicted values\")\n",
    "plt.plot(preds_lr[random_slice], label=\"LR Predicted values\")\n",
    "plt.plot(\n",
    "    targets[random_slice], label=\"True values\", ls=\"--\", color=\"black\", alpha=0.667\n",
    ")\n",
    "plt.xlabel(f\"Time (starting at idx {random_idx})\", fontsize=16)\n",
    "plt.ylabel(\"Subgrid forcing\", fontsize=16)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29faede",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Symbolic regression with genetic programming\n",
    "\n",
    "Let's try using one of the genetic programming libraries to discover a parameterization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947cb34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "symbolic_reg = gplearn.genetic.SymbolicRegressor(\n",
    "    population_size=5000,\n",
    "    generations=100,\n",
    "    p_crossover=0.7,\n",
    "    p_subtree_mutation=0.1,\n",
    "    p_hoist_mutation=0.05,\n",
    "    p_point_mutation=0.1,\n",
    "    max_samples=0.9,\n",
    "    verbose=1,\n",
    "    parsimony_coefficient=0.001,\n",
    "    const_range=(-2, 2),\n",
    "    function_set=(\"add\", \"mul\", \"sub\", \"div\", \"sin\", \"log\"),\n",
    ")\n",
    "\n",
    "symbolic_reg.fit(inputs[train_idxs][:20000], targets[train_idxs][:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd991e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(symbolic_reg._program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd32a51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"R2 Score:\", r2_score(symbolic_reg.predict(inputs[test_idxs]), targets[test_idxs])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510bdd5",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "While the program does slightly better than linear regression, it's extremely complicated (graph visualization below for slightly easier inspection):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88adb60-93a2-44f9-8314-c411e59d43e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "graph = pydot.graph_from_dot_data(symbolic_reg._program.export_graphviz())[0]\n",
    "graph.write_png(\"figs/symbolic_regression.png\")\n",
    "\n",
    "# Display the graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(\"figs/symbolic_regression.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da9e31",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Explore the Pareto frontier\n",
    "\n",
    "We can explore the Pareto frontier of the final population of programs as well, to see if there's a simpler program that still does well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f7f44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(\n",
    "    [\n",
    "        dict(\n",
    "            idx=i,\n",
    "            expr=str(p),\n",
    "            fitness=p.fitness_,\n",
    "            length=p.length_,\n",
    "            r2_score=r2_score(p.execute(inputs[test_idxs]), targets[test_idxs]),\n",
    "        )\n",
    "        for i, p in enumerate(symbolic_reg._programs[-1])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67789b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pareto_frontier = []\n",
    "used_programs = set()\n",
    "\n",
    "for l in range(1, 40):\n",
    "    programs = df[df.length <= l]\n",
    "    best = programs.r2_score.argmax()\n",
    "    idx = programs.idx.values[best]\n",
    "    r2 = programs.r2_score.values[best]\n",
    "    program = symbolic_reg._programs[-1][idx]\n",
    "    if program not in used_programs:\n",
    "        pareto_frontier.append(dict(length=l, r2=r2, expr=str(program)))\n",
    "        used_programs.add(program)\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 200)\n",
    "pareto_frontier = pd.DataFrame.from_dict(pareto_frontier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fca58a-911e-46c3-9a15-4a2ba8590eee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pareto_frontier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905cfb5c",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Unfortunately, nothing really jumps out here as both simple and performant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfecdaba",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Symbolic Regression with Sparse Linear Regression and Polynomial Features\n",
    "\n",
    "We can also apply sparse regression methods to this problem with polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30255fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def learned_expr(coef, names, cutoff=1e-3):\n",
    "    coef_idx = np.argwhere(np.abs(coef) > cutoff)[:, 0]\n",
    "    sort_order = reversed(np.argsort(np.abs(coef[coef_idx])))\n",
    "    return \" + \".join(\n",
    "        [\n",
    "            f\"{coef[coef_idx[i]]:.4f}*{names[coef_idx[i]].replace(' ','*')}\"\n",
    "            for i in sort_order\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469cac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(2)\n",
    "feats = pf.fit_transform(inputs)\n",
    "names = pf.get_feature_names_out([f\"x_{i}\" for i in range(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30121a3d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Non-sparse linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721fd64e-eae8-4b1c-b2f9-053aab36902a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(feats[train_idxs], targets[train_idxs])\n",
    "learned_expr(linear_reg.coef_, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c928dc9b-eb83-45bc-a416-4357f1cb6c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"R2 Score:\", linear_reg.score(feats[test_idxs], targets[test_idxs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c9020",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Adding polynomial features brings our $R^2$ much closer to that of the neural network, but we have a ton of terms. Let's try making the regression sparser.\n",
    "\n",
    "### Sequentially thresholded least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60408a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stlsq = ps.optimizers.stlsq.STLSQ(alpha=0.001, threshold=0.01)\n",
    "stlsq.fit(feats[train_idxs], targets[train_idxs])\n",
    "learned_expr(stlsq.coef_[0], names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3918224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stlsq.score(feats[test_idxs], targets[test_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45196067",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Looks like we can get a much sparser expression without sacrificing too much performance, though if we increase the threshold slightly, we will see a trade-off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d8e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stlsq2 = ps.optimizers.stlsq.STLSQ(alpha=0.001, threshold=0.02)\n",
    "stlsq2.fit(feats[train_idxs], targets[train_idxs])\n",
    "learned_expr(stlsq2.coef_[0], names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3aaf12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stlsq2.score(feats[test_idxs], targets[test_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d666fd8e",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "This is much simpler, but less performant. It still strongly outperformed the particluar genetic programming method we used, though. Let's visualize some of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165960d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6), dpi=150)\n",
    "plt.title(\"Neural network vs. quadratic regressions\", fontsize=16)\n",
    "\n",
    "preds_nn = nn_3l(torch.from_numpy(inputs)).detach().numpy()\n",
    "preds_lr_quad = linear_reg.predict(feats)\n",
    "preds_st_quad = stlsq.predict(feats)\n",
    "preds_st_quad2 = stlsq2.predict(feats)\n",
    "random_idx = np.random.choice(len(targets - 1000))\n",
    "random_slice = slice(random_idx, random_idx + 1000)\n",
    "\n",
    "plt.plot(preds_nn[random_slice], label=\"NN predicted values\")\n",
    "plt.plot(preds_lr_quad[random_slice], label=\"LR + quadratic\")\n",
    "plt.plot(preds_st_quad[random_slice], label=r\"STLSQ + quadratic, threshold=$0.01$\")\n",
    "plt.plot(preds_st_quad2[random_slice], label=r\"STLSQ + quadratic, threshold=$0.02$\")\n",
    "plt.plot(\n",
    "    targets[random_slice], label=\"True values\", ls=\"--\", color=\"black\", alpha=0.667\n",
    ")\n",
    "plt.xlabel(f\"Time (starting at idx {random_idx})\", fontsize=16)\n",
    "plt.ylabel(\"Subgrid forcing\", fontsize=16)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbfaa2c",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Add threshold features\n",
    "\n",
    "The 2D histogram of $x_0$ looks like it might be potentially modelable by piecewise linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa3a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4), dpi=150)\n",
    "plt.hist2d(inputs[:, 0], targets, bins=50)\n",
    "plt.xlim(-6, 14)\n",
    "plt.xlabel(\"$x_0$\", fontsize=16)\n",
    "plt.ylabel(\"Forcing\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5bb5ba",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Sparse regression\n",
    "\n",
    "Let's see if sparse regression will discover that if we provide it with thresholding features and cross-terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd19c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x0_thresholds = np.array(\n",
    "    [np.ones_like(inputs[:, 0])] + [inputs[:, 0] > i for i in range(6)]\n",
    ")\n",
    "x0_threshold_names = [\"\"] + [f\"*(x0>{t})\" for t in range(6)]\n",
    "\n",
    "feats2 = np.array(\n",
    "    [\n",
    "        feats[:, i] * x0_thresholds[j]\n",
    "        for i in range(feats.shape[1])\n",
    "        for j in range(len(x0_thresholds))\n",
    "    ]\n",
    ").T\n",
    "names2 = [\n",
    "    f\"{names[i]}{x0_threshold_names[j]}\"\n",
    "    for i in range(feats.shape[1])\n",
    "    for j in range(len(x0_thresholds))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ab7fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stlsq3 = ps.optimizers.stlsq.STLSQ(alpha=0.001, threshold=0.25)\n",
    "stlsq3.fit(feats2[train_idxs], targets[train_idxs])\n",
    "learned_expr(stlsq3.coef_[0], names2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ef1ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stlsq3.score(feats2[test_idxs], targets[test_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d853e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4), dpi=150)\n",
    "plt.hist2d(inputs[:, 0], targets, bins=50)\n",
    "\n",
    "fn = (\n",
    "    lambda x0: -2.1200 * 1 * (x0 > 2)\n",
    "    + -1.2695 * 1 * (x0 > 0)\n",
    "    + -0.9488 * x0\n",
    "    + -0.6694 * 1 * (x0 > 4)\n",
    "    + 0.5175 * x0 * (x0 > 1)\n",
    ")\n",
    "\n",
    "xt = np.linspace(-6, 14, 100)\n",
    "yt = fn(xt)\n",
    "plt.plot(xt, yt, color=\"red\")\n",
    "plt.xlim(-6, 14)\n",
    "plt.xlabel(\"$x_0$\", fontsize=16)\n",
    "plt.ylabel(\"Forcing\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f67e0",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "It looks like the model learns a piecewise linear function in a single variable, even though we didn't explicitly require it!\n",
    "\n",
    "### Genetic programming\n",
    "\n",
    "We can also try using genetic programming here, but with a different function set that removes some of the nonlinear operators we included before and replaces them with min and max, which allow for piecewise behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310dfba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "symbolic_reg_pw = gplearn.genetic.SymbolicRegressor(\n",
    "    population_size=5000,\n",
    "    generations=100,\n",
    "    p_crossover=0.7,\n",
    "    p_subtree_mutation=0.1,\n",
    "    p_hoist_mutation=0.05,\n",
    "    p_point_mutation=0.1,\n",
    "    max_samples=0.9,\n",
    "    verbose=1,\n",
    "    parsimony_coefficient=0.001,\n",
    "    const_range=(-2, 2),\n",
    "    function_set=(\"add\", \"mul\", \"max\", \"min\"),\n",
    ")\n",
    "symbolic_reg_pw.fit(inputs[train_idxs][:20000], targets[train_idxs][:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a9027c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(symbolic_reg_pw._program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a133d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"R2 Score:\",\n",
    "    r2_score(symbolic_reg_pw.predict(inputs[test_idxs]), targets[test_idxs]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828836f",
   "metadata": {},
   "source": [
    "While this leads to slightly higher $R^2$ than before, the model is still pretty complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45225961-2844-4d42-8756-b99eed311443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "graph = pydot.graph_from_dot_data(symbolic_reg_pw._program.export_graphviz())[0]\n",
    "graph.write_png(\"figs/symbolic_regression_pw.png\")\n",
    "\n",
    "# Display the graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(\"figs/symbolic_regression_pw.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b55c24",
   "metadata": {},
   "source": [
    "Tuning the hyperparameters could help, but requires more compute."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
