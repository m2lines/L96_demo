{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Using Neural Networks for L96 Parameterization: Online Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline:\n",
    "In this notebook, we'll extend upon the concepts learned in the [previous](https://m2lines.github.io/L96_demo/notebooks/L96_offline_nn.html) notebook by implementing the neural networks as a parameterization in the single time scale Lorenz 96, and testing its impact in a simulation. Testing the impacts of parameterizations in a simulation is sometimes also referred to as online testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from L96_model import L96, RK4, L96_eq1_xdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring reproducibility\n",
    "np.random.seed(14)\n",
    "torch.manual_seed(14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the pre-trained parameterization networks\n",
    "\n",
    "We first load in the networks that were trained in the previous [notebook](https://m2lines.github.io/L96_demo/notebooks/L96_offline_NN.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_weights = torch.load(\"./networks/linear.pth\")\n",
    "local_FCNN_weights = torch.load(\"./networks/local_FCNN.pth\")\n",
    "nonlocal_FCNN_weights = torch.load(\"./networks/non_local_FCNN.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the architecture that each model has before the weights can be assigned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model architectures\n",
    "# ---------------------------\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(1, 1)  # A single input and a single output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This method is automatically executed when\n",
    "        # we call a object of this class\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "    \n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(1, 16)  # 8 inputs\n",
    "        self.linear2 = nn.Linear(16, 16)\n",
    "        self.linear3 = nn.Linear(16, 1)  # 8 outputs\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x    \n",
    "\n",
    "class NonLocal_FCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs\n",
    "        self.linear2 = nn.Linear(16, 16)\n",
    "        self.linear3 = nn.Linear(16, 8)  # 8 outputs\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network instances and assign pre-trained weights.\n",
    "linear_network = LinearRegression()\n",
    "linear_network.load_state_dict(linear_weights)\n",
    "\n",
    "local_fcnn_network = FCNN()\n",
    "local_fcnn_network.load_state_dict(local_FCNN_weights)\n",
    "\n",
    "nonlocal_fcnn_network = NonLocal_FCNN()\n",
    "nonlocal_fcnn_network.load_state_dict(nonlocal_FCNN_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Adding Parameterizations to GCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "To recap, {cite}`Lorenz1995` describes a two-time scale dynamical system using two equations which are:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F - \\left( \\frac{hc}{b} \\right) \\sum_{j=0}^{J-1} Y_{j,k}\n",
    "\\end{gather*}\n",
    "\n",
    "\\begin{gather*}\n",
    "\\frac{d}{dt} Y_{j,k}\n",
    "&= - cbY_{j+1,k} \\left( Y_{j+2,k} - Y_{j-1,k} \\right) - c Y_{j,k} + \\frac{hc}{b} X_k\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "```{note}\n",
    "All the GCM networks used in this notebook have been introduced earlier in notebooks [Key aspects of GCMs parameterizations](https://m2lines.github.io/L96_demo/notebooks/gcm-parameterization-problem.html) and [Tuning GCM Parameterizations](https://m2lines.github.io/L96_demo/notebooks/estimating-gcm-parameters.html). To see the definition of those networks, expand the cells in the respective GCM sections below.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test = 10\n",
    "forcing = 18\n",
    "dt = 0.01\n",
    "\n",
    "k=8\n",
    "j=32 \n",
    "\n",
    "W = L96(k, j, F=forcing)\n",
    "\n",
    "# Full L96 model (two time scale model)\n",
    "X_full, _, _ = W.run(dt, T_test)\n",
    "X_full = X_full.astype(np.float32)\n",
    "\n",
    "init_conditions = X_full[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### GCM *Without* Neural Network Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "class GCM_without_parameterization:\n",
    "    \"\"\"GCM without parameterization\n",
    "\n",
    "    Args:\n",
    "        F: Forcing term\n",
    "        time_stepping: Time stepping method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F, time_stepping=RK4):\n",
    "        self.F = F\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, _):\n",
    "        \"\"\"Compute right hand side of the the GCM equations\"\"\"\n",
    "        return L96_eq1_xdot(X, self.F)\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"Run GCM\n",
    "\n",
    "        Args:\n",
    "            X0: Initial conditions of X\n",
    "            dt: Time increment\n",
    "            nt: Number of forward steps to take\n",
    "            param: Parameters of closure\n",
    "\n",
    "        Returns:\n",
    "            Model output for all variables of X at each timestep\n",
    "            along with the corresponding time units\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcm_no_param = GCM_without_parameterization(forcing)\n",
    "X_no_param, t = gcm_no_param(init_conditions, dt, int(T_test / dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### GCM *With* Neural Network Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "class GCM_network:\n",
    "    \"\"\"GCM with neural network parameterization\n",
    "\n",
    "    Args:\n",
    "        F: Forcing term\n",
    "        network: Neural network\n",
    "        time_stepping: Time stepping method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F, network, time_stepping=RK4):\n",
    "        self.F = F\n",
    "        self.network = network\n",
    "        self.time_stepping = time_stepping\n",
    "\n",
    "    def rhs(self, X, _):\n",
    "        \"\"\"Compute right hand side of the the GCM equations\"\"\"\n",
    "        if self.network.linear1.in_features == 1:\n",
    "            X_torch = torch.from_numpy(X)\n",
    "            X_torch = torch.unsqueeze(X_torch, 1)\n",
    "        else:\n",
    "            X_torch = torch.from_numpy(np.expand_dims(X, 0))\n",
    "\n",
    "        # Adding NN parameterization\n",
    "        return L96_eq1_xdot(X, self.F) + np.squeeze(self.network(X_torch).data.numpy())\n",
    "\n",
    "    def __call__(self, X0, dt, nt, param=[0]):\n",
    "        \"\"\"Run GCM\n",
    "\n",
    "        Args:\n",
    "            X0: Initial conditions of X\n",
    "            dt: Time increment\n",
    "            nt: Number of forward steps to take\n",
    "            param: Parameters of closure\n",
    "\n",
    "        Returns:\n",
    "            Model output for all variables of X at each timestep\n",
    "            along with the corresponding time units\n",
    "        \"\"\"\n",
    "        time, hist, X = (\n",
    "            dt * np.arange(nt + 1),\n",
    "            np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "            X0.copy(),\n",
    "        )\n",
    "        hist[0] = X\n",
    "\n",
    "        for n in range(nt):\n",
    "            X = self.time_stepping(self.rhs, dt, X, param)\n",
    "            hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "        return hist, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with linear network\n",
    "gcm_linear_net = GCM_network(forcing, linear_network)\n",
    "Xnn_linear, t = gcm_linear_net(init_conditions, dt, int(T_test / dt), linear_network)\n",
    "\n",
    "# Evaluate with local FCNN\n",
    "gcm_local_net = GCM_network(forcing, local_fcnn_network)\n",
    "Xnn_local, t = gcm_local_net(init_conditions, dt, int(T_test / dt), local_fcnn_network)\n",
    "\n",
    "# Evaluate with nonlocal FCNN\n",
    "gcm_nonlocal_net = GCM_network(forcing, nonlocal_fcnn_network)\n",
    "Xnn_nonlocal, t = gcm_nonlocal_net(init_conditions, dt, int(T_test / dt), nonlocal_fcnn_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Comparing Results\n",
    "\n",
    "Comparing the predictions of GCM with different parameterizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_i = 200\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(t[:time_i], X_full[:time_i, 4], label=\"Full L96\")\n",
    "plt.plot(t[:time_i], X_no_param[:time_i, 4], '--', label=\"No parameterization\")\n",
    "\n",
    "plt.plot(t[:time_i], Xnn_linear[:time_i, 4], label=\"linear parameterization\")\n",
    "\n",
    "plt.plot(t[:time_i], Xnn_local[:time_i, 4],  label=\"local NN\")\n",
    "plt.plot(t[:time_i], Xnn_nonlocal[:time_i, 4], label=\"nonlocal NN\")\n",
    "plt.legend(loc=\"upper left\", fontsize=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Checking over many different initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_linear, err_local, err_nonlocal = [], [], []\n",
    "T_test = 1\n",
    "\n",
    "for i in range(90):\n",
    "    init_conditions_i = X_full[i * 10, :]\n",
    "\n",
    "    # Evaluate with linear network\n",
    "    gcm_linear_net = GCM_network(forcing, linear_network)\n",
    "    Xnn_linear, t = gcm_linear_net(init_conditions_i, dt, int(T_test / dt), linear_network)\n",
    "\n",
    "    # Evaluate with local FCNN\n",
    "    gcm_local_net = GCM_network(forcing, local_fcnn_network)\n",
    "    Xnn_local, t = gcm_local_net(init_conditions_i, dt, int(T_test / dt), local_fcnn_network)\n",
    "\n",
    "    # Evaluate with nonlocal FCNN\n",
    "    gcm_nonlocal_net = GCM_network(forcing, nonlocal_fcnn_network)\n",
    "    Xnn_nonlocal, t = gcm_nonlocal_net(init_conditions_i, dt, int(T_test / dt), nonlocal_fcnn_network)\n",
    "\n",
    "    # GCM parameterized by the global 3-layer network\n",
    "    #gcm_net_3layers = GCM_network(forcing, nn_3l)\n",
    "    #Xnn_3layer_i, t = gcm_net_3layers(init_conditions_i, dt, int(T_test / dt), nn_3l)\n",
    "\n",
    "    # GCM parameterized by the linear network\n",
    "    #gcm_net_1layers = GCM_network(forcing, linear_network)\n",
    "    #Xnn_1layer_i, t = gcm_net_1layers(init_conditions_i, dt, int(T_test / dt), linear_network)\n",
    "\n",
    "    err_linear.append(\n",
    "        np.sum(np.abs(X_full[i * 10 : i * 10 + T_test * 100 + 1] - Xnn_linear))\n",
    "    )\n",
    "    \n",
    "    err_local.append(\n",
    "        np.sum(np.abs(X_full[i * 10 : i * 10 + T_test * 100 + 1] - Xnn_local))\n",
    "    )\n",
    "    \n",
    "    err_nonlocal.append(\n",
    "        np.sum(np.abs(X_full[i * 10 : i * 10 + T_test * 100 + 1] - Xnn_nonlocal))\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "print(f\"Sum of errors for linear: {sum(err_linear):.2f}\")\n",
    "print(f\"Sum of errors for local neural network: {sum(err_local):.2f}\")\n",
    "print(f\"Sum of errors for non-local neural network: {sum(err_nonlocal):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this evaluation we see that the neural networks perform much better than the linear model. However, incorporating non-locality did not improve model performance over the local model. It may be possible to have improved this by building a better non-local model by a larger parameter sweep. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "In this notebook we showed how neural networks can be added as a parameterization to the single timescale L96 model (the gcm analogue of the two timescale L96 model). The neural network parameterizations performed better than the linear parameterization, as shown by evaluating the model performance over many initial conditions. \n",
    "\n",
    "In these notebook we have followed the strategy of :\n",
    "- running a realistic (more time scales resolved) simulation.\n",
    "- evaluating the impact of the fast scales (which could not be resolved if the simulation resolved fewer time scales) on the slow scales in the realistic simulation.\n",
    "- learning a functional relationship between the impact of the fast scales on the slow scale and the slow scales.\n",
    "- incoporating this relationship (ML based parameterization) in the model of the slow time scales, and evaluating the success (if any). \n",
    "\n",
    "This is a strategy of offline training and online testing. \n",
    "\n",
    "A different strategy to have would have been to not necessarily learn the impact of or the patterns in the sub-grid scales. But rather optimize a model such that when it is added to the slow time scale model, it leads to the time evolution of the slow time scale model being closer to the two time scale model (which is our ultimate goal). This strategy is sometimes called online training, and is not discussed here. However, a short primer to this was presented in [the notebook on gcm tuning](https://m2lines.github.io/L96_demo/notebooks/estimating-gcm-parameters.html), where we tuned parameters to match the long time evolution of the more realistic model. \n",
    "\n",
    "In the next few notebooks we show a few tricks to potentially improve performance of neural networks, how to interpret neural networks, and how physics constraints can be added in their architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
