{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Improving Performance of Neural Networks\n",
    "\n",
    "In this notebook, we'll see several techniques that are generally used to improve the accuracy of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from L96_model import L96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring reproducibility\n",
    "np.random.seed(14)\n",
    "torch.manual_seed(14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "```{note}\n",
    "The dataset, the neural network, and the training and evaluation functions that we use in this notebook are same as the one defined in [Using Neural Networks for L96 Parameterization](https://m2lines.github.io/L96_demo/notebooks/Neural_network_for_Lorenz96.html#).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Generating the Dataset\n",
    "\n",
    "First we need to create the dataset on which the neural network will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 20000\n",
    "forcing, dt, T = 18, 0.01, 0.01 * time_steps\n",
    "\n",
    "# Create a \"real world\" with K=8 and J=32\n",
    "W = L96(8, 32, F=forcing)\n",
    "\n",
    "# The effect of Y on X is `xy_true`\n",
    "X_true, _, _, xy_true = W.run(dt, T, store=True, return_coupling=True)\n",
    "\n",
    "# Change the data type to `float32` in order to avoid doing type conversions later on\n",
    "X_true, xy_true = X_true.astype(np.float32), xy_true.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Splitting the Dataset into Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time steps for validation\n",
    "val_size = 4000\n",
    "\n",
    "# Training Data\n",
    "X_true_train = X_true[\n",
    "    :-val_size, :\n",
    "]  # Flatten because we first use single input as a sample\n",
    "subgrid_tend_train = xy_true[:-val_size, :]\n",
    "\n",
    "# Test Data\n",
    "X_true_test = X_true[-val_size:, :]\n",
    "subgrid_tend_test = xy_true[-val_size:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Build the Dataset and the Dataloaders\n",
    "\n",
    "The dataset will have *8 inputs* and *8 outputs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of sample in each batch\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Create the training dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlocal_data = TensorDataset(\n",
    "    torch.from_numpy(X_true_train),\n",
    "    torch.from_numpy(subgrid_tend_train),\n",
    ")\n",
    "\n",
    "loader_train = DataLoader(dataset=nlocal_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlocal_data_test = TensorDataset(\n",
    "    torch.from_numpy(X_true_test), torch.from_numpy(subgrid_tend_test)\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(dataset=nlocal_data_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Defining Functions to Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, criterion, loader, optimizer):\n",
    "    \"\"\"Train the network for one epoch\"\"\"\n",
    "    network.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        # Get predictions\n",
    "        if len(batch_x.shape) == 1:\n",
    "            # This if block is needed to add a dummy dimension if our inputs are 1D\n",
    "            # (where each number is a different sample)\n",
    "            prediction = torch.squeeze(network(torch.unsqueeze(batch_x, 1)))\n",
    "        else:\n",
    "            prediction = network(batch_x)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(prediction, batch_y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backpropagation to compute the gradients and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(network, criterion, loader):\n",
    "    \"\"\"Test the network\"\"\"\n",
    "    network.eval()  # Evaluation mode (important when having dropout layers)\n",
    "\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            # Get predictions\n",
    "            if len(batch_x.shape) == 1:\n",
    "                # This if block is needed to add a dummy dimension if our inputs are 1D\n",
    "                # (where each number is a different sample)\n",
    "                prediction = torch.squeeze(network(torch.unsqueeze(batch_x, 1)))\n",
    "            else:\n",
    "                prediction = network(batch_x)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(prediction, batch_y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        # Get an average loss for the entire dataset\n",
    "        test_loss /= len(loader)\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_model(network, criterion, optimizer, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"Train and validate the network\"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_model(network, criterion, train_loader, optimizer)\n",
    "        val_loss = test_model(network, criterion, val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {int(end_time - start_time)} seconds.\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Creating the Neural Network\n",
    "\n",
    "We build a 3 layer neural network consisting of two hidden layers and an output layer. Each hidden layer is followed by the `ReLU` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)  # 8 inputs\n",
    "        self.linear2 = nn.Linear(16, 16)\n",
    "        self.linear3 = nn.Linear(16, 8)  # 8 outputs\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Regularization and Overfitting\n",
    "\n",
    "One of the most common issues that happen while training a neural network is when the model memorizes the training dataset. It causes the model to perform very accurately on the training set but shows very poor performance on the validation set. This phenomenon is termed overfitting. One of the ways to prevent overfitting is to add regularization to our model as described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "```{figure} figs/overfitting.png\n",
    ":name: overfitting-curve\n",
    ":width: 700\n",
    "\n",
    "Curves showing different types of model fits. The image is taken from the Python Machine Learning book by Sebastian Raschka.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The curve on the far right of the plot above predicts perfectly on the given set, yet it's not the best choice. This is because if you were to gather some new data points, they most likely would not be on that curve. Instead, those new points would be closer to the curve in the middle graph since it generalizes better to the dataset.\n",
    "\n",
    "All ML algorithms have some form of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Regularization Intuition\n",
    "\n",
    "Regularization can be thought of as **putting constraints on the model** to obtain better generalizability i.e. *avoiding remembering* the training data.\n",
    "\n",
    "One of the ways to achieve this can be by adding a term to the loss function such that:\n",
    "> Loss = Training Loss + Regularization\n",
    "\n",
    "This puts a penalty for making the model more complex.\n",
    "\n",
    "Very braodly speaking (just to gain intuition) - if we want to reduce the training loss (reduce bias) we should try using a more complex model (if we have enough data) and if we want to reduce overfitting (reduce variace) we should simplify or constraint the model (increase regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Regularization of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Some of the ways to add regularization in neural networks are\n",
    "\n",
    "- Dropout (added in the definition of the network). \n",
    "- Early stopping\n",
    "- Weight decay (added in the optimizer part - see `optim.Adam` in PyTorch)\n",
    "- Data augmentation (usually for images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Weight decay (L2 norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Weight decay is usually defined as a term that’s added directly to the update rule.\n",
    "Namely, to update a certain weight $w$ in the $i+1$ iteration, we would use a modified rule:\n",
    "\n",
    "$w_{i+1} = w_{i} - \\gamma ( \\frac{\\partial L}{\\partial w} + A w_{i})$\n",
    "\n",
    "In practice, this is almost identical to L2 regularization, though there is some difference (e.g., see [here](https://bbabenko.github.io/weight-decay/))\n",
    "\n",
    "Weight decay is one of the parameters of the optimizer - see `torch.optim.SGD`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Using Weight Decay\n",
    "\n",
    "Now we try to train our `NetANN` model again but this time by adding a weight decay to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_3l_decay = NetANN()\n",
    "\n",
    "n_epochs = 10\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(nn_3l_decay.parameters(), lr=0.003, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss, val_loss = fit_model(\n",
    "    nn_3l_decay, criterion, optimizer, loader_train, loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Plotting the training and validation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"Training loss\")\n",
    "plt.plot(val_loss, \"r\", label=\"Validation loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout means randomly deactivating or temporarily removing some units from a layer of the network while training, along with all its incoming and outgoing connections. See more details [here](http://jmlr.org/papers/v15/srivastava14a.html).\n",
    "It is usually the most useful regularization that we can do in fully connected layers.\n",
    "\n",
    "In convolutional layers dropout makes less sense - see more discussion [here](https://www.kdnuggets.com/2018/09/dropout-convolutional-networks.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "```{figure} figs/Dropout_layer.png\n",
    ":name: dropout-layer\n",
    ":width: 700\n",
    "\n",
    "Dropout Neural Net Model. Left: A standard neural net with 2 hidden layers. Right: An example of a thinned net produced by applying dropout to the network on the left. Crossed units have been dropped. Image taken from [here](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the network defined below, we add dropout to with a probability of 20% to each layer. This means that during each training step, random 20% of the units within each layer will be deactivated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetANNDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(8, 16)\n",
    "        self.linear2 = nn.Linear(16, 16)\n",
    "        self.linear3 = nn.Linear(16, 8)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout regularization\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network with very high dropout\n",
    "nn_3l_drop = NetANNDropout(dropout_rate=0.8)\n",
    "\n",
    "n_epochs = 10\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(nn_3l_drop.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss, val_loss = fit_model(\n",
    "    nn_3l_drop, criterion, optimizer, loader_train, loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Plotting the training and validation loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"Training loss\")\n",
    "plt.plot(val_loss, \"r\", label=\"Validation loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Choosing the Learning Rate\n",
    "\n",
    "While training a neural network **selecting a good learning rate (LR) is essential for both fast convergence and a lower error**. A high learning rate can cause the training loss to never converge while a too small learning rate will cause the model to converge extremely slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Finding the Optimal Learning Rate\n",
    "\n",
    "To choose the optimal learning rate for our network, we can use an LR finding algorithm. The objective of a LR Finder is to find the highest LR which still minimises the loss and does not make the loss diverge/explode. This is done by first starting with an extremely small LR and then increasing the LR after each batch until the corresponding loss starts to explode. To read more about learning rate finders, read [this blog](https://towardsdatascience.com/speeding-up-neural-net-training-with-lr-finder-c3b401a116d0).\n",
    "\n",
    "For our use case, we use the LR finder from the `torch-lr-finder` package to find the best learning rate for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Define the model and the optimizer. The optimizer is **initialized with a very small learning rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_3l_lr = NetANN()\n",
    "optimizer = optim.Adam(nn_3l_lr.parameters(), lr=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now we setup the LR finder and make it run for 200 iterations during which the learning rate varies from 1e-7 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_finder = LRFinder(nn_3l_lr, optimizer, criterion)\n",
    "lr_finder.range_test(loader_train, end_lr=100, num_iter=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now we plot the LR vs the loss curve to find the best learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the lr vs the loss curve\n",
    "lr_finder.plot()\n",
    "\n",
    "# Reset the model and optimizer to their initial state\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "From the curve, we see that at the learning of approximately 0.01 we get the steepest gradient. So we choose 0.01 as the learning rate for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "optimizer = optim.Adam(nn_3l_lr.parameters(), lr=0.01)\n",
    "\n",
    "train_loss, val_loss = fit_model(\n",
    "    nn_3l_lr, criterion, optimizer, loader_train, loader_test, n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Plotting the training and validation loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(train_loss, \"b\", label=\"Training loss\")\n",
    "plt.plot(val_loss, \"r\", label=\"Validation loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the loss curves we can see that **the loss has converged much faster** than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNormalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the activation values such that the hidden representation don’t vary drastically and also helps to get improvement in the training speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclic learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cyclic learning rate policy, introduced in [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186), cycles the learning rate between two boundaries with a constant frequency in a triangular fashion. To read more about the cyclic learning rates and the one cycle policy, read [here](https://sgugger.github.io/the-1cycle-policy.html).\n",
    "\n",
    "In PyTorch, cyclic learning rate can be used from `optim.lr_scheduler.CyclicLR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "```{figure} figs/cyclic_lr.png\n",
    ":name: cyclic-lr\n",
    ":width: 500\n",
    "\n",
    "Cyclical learning rates oscillate back and forth between two bounds when training, slowly increasing the learning rate after every batch update. Image taken from [here](https://pyimagesearch.com/2019/07/29/cyclical-learning-rates-with-keras-and-deep-learning/).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
