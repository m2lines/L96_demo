{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN, CNN and polynomial maps, an illustration using L96 and Burger's equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "from torch import nn, optim\n",
    "\n",
    "from L96_model import L96, EulerFwd, RK2, RK4\n",
    "from utils import GCM_no_param  # The GCM object based on prior tutorials\n",
    "from utils import GCM_discrete  # New GCM with tendency by global network\n",
    "from utils import GCM_local_discrete  # New GCM with tendency by local network\n",
    "from utils import Net_ANN  # Import a simple neural network\n",
    "\n",
    "from utils import (\n",
    "    train_model,\n",
    ")  # Train model for each epoch (Adapted from Yani's tutorial)\n",
    "from utils import test_model  # Calculate loss (Adapted from Yani's tutorial)\n",
    "from utils import train_network  # Wrapper function for training a neural network\n",
    "\n",
    "from utils import (\n",
    "    gen_local_training_data,\n",
    ")  # Generate individual patches as training data\n",
    "from utils import (\n",
    "    get_poly_features,\n",
    ")  # Generate 2nd-order polynomial features from patches\n",
    "\n",
    "# Import Burger's equation class\n",
    "from utils import Burgers\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "The petagogical model we use to illustrate different methods at M2LInES is the [Lorenz-96 model](file:///Users/ziweili/Downloads/10829-predictability-problem-partly-solved.pdf) by Edward Lorenz published in 1996. However, Ed Lorenz did not provide much physical reasoning behind the equation, and the dynamics of the slow, large-scale variables and fast, small-scale variables are seemingly ad-hoc. To see how, consider the original L96 equation:\n",
    "\n",
    "\\begin{align}\n",
    "\\dot{X_k} &= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k - (hc/b)\\sum_jY_{j, k}\\\\\n",
    "\\dot{Y_{k,j}} &= - cb Y_{j+1, k} \\left( Y_{j+2, k} - Y_{j-1, k} \\right) - cY_{j, k} + (hc/b)X_k. \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The advection terms of $X$ and $Y$ are similar to the usual 1D advection form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\dot{X_k} \\sim - X_{k-1} \\left( X_{k-2} - X_{k+1}\\right) \\sim X\\nabla X,\n",
    "\\end{equation}\n",
    "\n",
    "but the advection term in $X$ is negative compared with the usual momentum conservation in fluid dynamics: $Du = 0\\Rightarrow \\partial_t u = - u\\nabla u$. \n",
    "\n",
    "To simplify the problem a little bit, and understand the skill and stability of machine-learning algorithms when they are applied to physical systems, we consider the 1-variable version of the L96 system. This approach is similar to Brandon Reichl's approach (see notebook at this [link](https://github.com/m2lines/L96_demo/blob/main/08-Implementation/Neural%20Network%20Advection.ipynb)). Here we use this view to consider the performance of the following networks: \n",
    "1. Local ANN\n",
    "2. Local polynomial stencils\n",
    "3. Global CNN\n",
    "4. Global ANN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## One-variable L96 equation\n",
    "\n",
    "The one-variable L96 equation writes\n",
    "\n",
    "\\begin{equation}\n",
    "\\dot{X_k} = - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "To see how machine learning helps us with simulating Earth's dynamics, we use three approaches to simulate the 1-variable L96 system. Our approach is based on the following principles: \n",
    " 1. The networks' perception fields must be local, as required by physical laws and the practical constraints of implementation. \n",
    " 2. The network structure must obey symmetries. They should be translational invariant, and is compatible with periodic boundary conditions. \n",
    " 3. The complexity of the networks must be small for interpretability. \n",
    "\n",
    "## Local networks\n",
    "\n",
    "In one-dimensional setting, we define local maps as the following approximation problem: given a receptive field of the network of width $(2m+1)$: $\\mathbf{X^{(m)}_k} = (X_{k-m}, ..., X_{k}, ..., X_{k+m})$, we approximate function $f(\\mathbf{X^{(m)}_k})$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "f: \\mathbf{X^{(m)}_k} \\rightarrow dX_k/dt\n",
    "\\end{equation}\n",
    "\n",
    "In this tutorial, we set $m = 2$, and the local map is illustrated in the following figure: \n",
    "```{figure} figs/local_map.png\n",
    ":width: 400px\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "we choose the functional form, $f(\\cdot)$, to be one of the following two choices, simple feedforward neural network, and polynomial stencils. \n",
    "\n",
    "### 1. Simple feedforward neural network (ANN)\n",
    "\n",
    "The feedforward neural network is embedded in the GCM, replacing all terms on the right-hand-side. For this tutorial, the network is fully connected, has two hidden layers with 10 neurons each, and has $tanh()$ as its activation function. The structure can be seen in the following figure:  \n",
    "\n",
    "```{figure} figs/local_map_ANN.png\n",
    ":width: 400px\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 2. Polynomial stencil\n",
    "\n",
    "Physical dynamical systems are usually written as ODEs and PDEs, and when they are solved numerically, they are discretized in space and time into regular ODE sets (whether it's in the spectral space or in the real space). Take the real space discretization for example, the terms on the right-hand side usually take the form of polynomials of the information from adjacent points. The one-variable L96 system is a nice example: \n",
    "\n",
    "```{figure} figs/local_map_polynet.png\n",
    ":width: 400px\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Global networks\n",
    "\n",
    "### 1. Global convolutional neural network (global CNN)\n",
    "\n",
    "We apply a simple global convolutional neural network to predict future tendencies. The network has a filter width of 5 (consistent with $m = 2$ as above), 2 covolutional layers and 2 fully connected-layers with 100 neurons each. The first convolution layer has 1 channel as input, 2 channels as output, and the second convolution layer has 2 channels as input, 4 channels as output.  \n",
    "\n",
    "\n",
    "```{figure} figs/diagram_CNN.png\n",
    ":width: 400px\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Global feedforward neural network (global ANN)\n",
    "\n",
    "As a point of comparison with the global CNN, we also train a global ANN with takes in global field and predicts global tendencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GCM objects\n",
    "Here, we define GCM objects for\n",
    "1. the 1-variable L96 equation\n",
    "2. the L96 system with tendency represented by global networks (global ANN, global CNN)\n",
    "3. the L96 system with tendency represented by local networks (local ANN, polynet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neural network objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a convolutional neural network object\n",
    "class Net_CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inout_size=[100, 100],\n",
    "        kernel_size=5,\n",
    "        hidden_neurons=[20, 20],\n",
    "        strides=[1, 1],\n",
    "        channels=[2, 4],\n",
    "    ):\n",
    "        # Class of a very simple 1-dimensional convolutional neural network. This network contains\n",
    "        # 2 convolutional layers with different channels channel each, and 2 fully-connected layers\n",
    "        # after the convolutional layers.\n",
    "\n",
    "        # Input parameters:\n",
    "        #\n",
    "        # inout_size:     A 2-element list, the first element contains the input size;\n",
    "        #                 the second element contains the output size.\n",
    "        #\n",
    "        # kernel_size:    The size of the convolutional kernel. Same across all conv layers\n",
    "        #\n",
    "        # hidden_neurons: List of hidden neurons of the fully-connected layers after the conv layers\n",
    "\n",
    "        # Inherit from default class\n",
    "        super(Net_CNN, self).__init__()\n",
    "\n",
    "        # Set padding to half the size of the kernel, and padding mode to periodic (circular)\n",
    "        padding = kernel_size // 2\n",
    "        padding_mode = \"circular\"\n",
    "\n",
    "        # Set dilation to 1, i.e., no dilation\n",
    "        dilation = 1\n",
    "\n",
    "        # Define two convolutional layers\n",
    "        self.conv1 = torch.nn.Conv1d(\n",
    "            1,\n",
    "            channels[0],\n",
    "            kernel_size,\n",
    "            stride=strides[0],\n",
    "            padding=padding,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "        self.conv2 = torch.nn.Conv1d(\n",
    "            channels[0],\n",
    "            channels[1],\n",
    "            kernel_size,\n",
    "            stride=strides[1],\n",
    "            padding=padding,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "\n",
    "        # Define a function to calculate the dimensionality of the input to the fully-connected layer\n",
    "        # (see definition of L_out in https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
    "        output_size = lambda L_in, padding, dilation, kernel_size, stride: int(\n",
    "            (L_in + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1\n",
    "        )\n",
    "        # Use the function to calculate the flattened output size of the convolutional layers\n",
    "        output_size1 = output_size(\n",
    "            inout_size[0], padding, dilation, kernel_size, strides[0]\n",
    "        )\n",
    "        output_size2 = output_size(\n",
    "            output_size1, padding, dilation, kernel_size, strides[1]\n",
    "        )\n",
    "\n",
    "        # Assign information to be used in the fully connected layer and self.forward()\n",
    "        self.output_size2 = output_size2\n",
    "        self.channels = channels\n",
    "\n",
    "        # Define fully-connected layers after the convolutional layers\n",
    "        self.linear1 = nn.Linear(output_size2 * channels[1], hidden_neurons[0])\n",
    "        self.linear2 = nn.Linear(hidden_neurons[0], hidden_neurons[1])\n",
    "        self.linear3 = nn.Linear(hidden_neurons[1], inout_size[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))  # Convolution layer 1\n",
    "        x = torch.relu(self.conv2(x))  # Convolution layer 2\n",
    "        x = x.view(-1, 1, self.channels[1] * self.output_size2)  # Flatten\n",
    "        x = torch.tanh(self.linear1(x))  # Fully-connected layer 1\n",
    "        x = torch.tanh(self.linear2(x))  # Fully-connected layer 2\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate 1-variable L96 equation as ground truth\n",
    "The 1-variable simulation has 100 grid points of $X$, and is run 20000 timesteps with $dt = 0.001$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(14)  # For reproducibility\n",
    "torch.manual_seed(14)  # For reproducibility\n",
    "plot_path = \"./figs/\"\n",
    "if not os.path.isdir(plot_path):\n",
    "    os.mkdir(plot_path)\n",
    "\n",
    "# Default case, 1-variable L96\n",
    "K = 100  # The number of slow variables, and\n",
    "J = 0  # The number of fast variables per slow variable, set to 0\n",
    "F = 18  # Constant external forcing\n",
    "h = 0  # No interaction with subgrid scale\n",
    "dt = 0.001  # Timestep\n",
    "\n",
    "# Define an 1-variable L96 object\n",
    "W = L96(K, J, F=F, dt=dt, h=h)\n",
    "\n",
    "# Define total timesteps, time, and sampling interval\n",
    "Nt = 20000  # Total timesteps\n",
    "T = dt * Nt  # Total time of simulation\n",
    "si = dt  # Sampling interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the L96 model as the ground truth\n",
    "X_true_temp = W.run(si, T, store=True)[0]\n",
    "X_true = X_true_temp[:-1, :]\n",
    "dX_true = X_true_temp[1:, :] - X_true_temp[:-1, :]\n",
    "del X_true_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Train neural networks with local and global setup, and evaluate their performance\n",
    "First, we define some training parameters. These training parameters are the same across all training procedures in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 2000  # Number of training data\n",
    "test_size = 400  # Number of test data\n",
    "batch_size = 100  # Number of training data per batch\n",
    "n_epochs = 1000  # Total number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 1. Train a global CNN\n",
    "The global CNN has 2 convolution layers with ReLU activation, and 2 fully connected layers with tanh activation. The number of input/output channels of the two convolutional layers are all 1. The fully-connected layers has 100 hidden neurons each. \n",
    "\n",
    "First, we define training data for the global network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a random index to select training and test data\n",
    "ind = np.random.choice(X_true.shape[0], train_size + test_size)\n",
    "\n",
    "# Add one extra dimension to represent channels\n",
    "X_feature_train_global = X_true[ind[:train_size], :][:, np.newaxis, :]\n",
    "X_target_train_global = dX_true[ind[:train_size], :][:, np.newaxis, :]\n",
    "X_feature_test_global = X_true[ind[train_size:], :][:, np.newaxis, :]\n",
    "X_target_test_global = dX_true[ind[train_size:], :][:, np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN parameters\n",
    "inout_size = [K, K]  # Input and output dimensions, global field\n",
    "kernel_size = 5  # Kernel width\n",
    "channels = [2, 4]  # Number of channels of the first and second layer\n",
    "hidden_neurons = [100, 100]  # Number of hidden neurons of the fully-connected layers\n",
    "\n",
    "# Define a global CNN\n",
    "net_CNN_global = Net_CNN(\n",
    "    inout_size=inout_size,\n",
    "    kernel_size=kernel_size,\n",
    "    hidden_neurons=hidden_neurons,\n",
    "    channels=channels,\n",
    ").double()\n",
    "\n",
    "# Train the global CNN using global data\n",
    "net_CNN_global, train_loss_CNN, test_loss_CNN = train_network(\n",
    "    net_CNN_global,\n",
    "    X_feature_train_global,\n",
    "    X_target_train_global,\n",
    "    X_feature_test_global,\n",
    "    X_target_test_global,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 2. Train a global ANN\n",
    "The global ANN has 2 hidden layers with 100 hidden neurons each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a global ANN\n",
    "net_ANN_global = Net_ANN([K, 100, 100, K], filter_loc=[0]).double()\n",
    "\n",
    "# Train the global ANN using global data\n",
    "net_ANN_global, train_loss_ANN_global, test_loss_ANN_global = train_network(\n",
    "    net_ANN_global,\n",
    "    X_feature_train_global,\n",
    "    X_target_train_global,\n",
    "    X_feature_test_global,\n",
    "    X_target_test_global,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 3. Train a local polynomial map\n",
    "To train local maps, we first generate training data as local information around a grid point. Here, we define a local stencil that extends 2 grid points to the left and right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the local map location\n",
    "filter_start = -2\n",
    "filter_end = 2  # 5 points: k+2, k+1, k, k-1, k-2\n",
    "filter_loc = np.arange(filter_start, filter_end + 1)  # Filter location list\n",
    "\n",
    "# Generate local maps consistent with the geometry of the stencil\n",
    "X_local_train, dX_train, X_local_test, dX_test = gen_local_training_data(\n",
    "    X_true, dX_true, filter_loc, train_size, test_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate polynomial features from data\n",
    "X_feature_train_poly = get_poly_features(X_local_train)  # Training data\n",
    "X_feature_test_poly = get_poly_features(X_local_test)  # Test data\n",
    "\n",
    "# Define a polynet as a regular ANN, with no hidden layers and a single bias\n",
    "net_poly = Net_ANN([X_feature_train_poly.shape[-1], 1], filter_loc=filter_loc).double()\n",
    "\n",
    "# Train the polynet\n",
    "net_poly, train_loss_poly, test_loss_poly = train_network(\n",
    "    net_poly,\n",
    "    X_feature_train_poly,\n",
    "    dX_train,\n",
    "    X_feature_test_poly,\n",
    "    dX_test,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 4. Train a local ANN map\n",
    "Using the local information from above, we train a local ANN as another point of comparison. This local ANN has two hidden layers with 10 neurons each, and has $tanh()$ as activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular ANN consistent with the local map\n",
    "net_ANN_local = Net_ANN([len(filter_loc), 10, 10, 1], filter_loc=filter_loc).double()\n",
    "\n",
    "# Train the local ANN\n",
    "net_ANN_local, train_loss_ANN, test_loss_ANN = train_network(\n",
    "    net_ANN_local,\n",
    "    X_local_train,\n",
    "    dX_train,\n",
    "    X_local_test,\n",
    "    dX_test,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Display training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_var = dX_train.var()\n",
    "fig = plt.figure(figsize=(6, 5), dpi=150)\n",
    "plt.plot(train_loss_ANN_global / dX_var, color=\"tab:blue\", label=\"global ANN\")\n",
    "plt.plot(test_loss_ANN_global / dX_var, \":\", color=\"tab:blue\")\n",
    "plt.plot(train_loss_CNN / dX_var, color=\"tab:red\", label=\"global CNN\")\n",
    "plt.plot(test_loss_CNN / dX_var, \":\", color=\"tab:red\")\n",
    "plt.plot(train_loss_ANN / dX_var, color=\"tab:grey\", label=\"local ANN\")\n",
    "plt.plot(test_loss_ANN / dX_var, \":\", color=\"tab:grey\")\n",
    "plt.plot(train_loss_poly / dX_var, color=\"tab:orange\", label=\"local polynet\")\n",
    "plt.plot(test_loss_poly / dX_var, \":\", color=\"tab:orange\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=7)\n",
    "plt.title(\"Normalized MSE loss of different models\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Normalized MSE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# II. Test predictions of the networks\n",
    "global ANN, global CNN, local ANN, and polynomial stencils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction parameters\n",
    "temp_dt = si  # Timestep same as the sampling interval (same as training data)\n",
    "t_plot = int(T / temp_dt)  # Total testing timesteps\n",
    "t_series_plot = np.arange(0, t_plot + 1) * temp_dt\n",
    "\n",
    "# Define GCM objects with tendencies represented by networks\n",
    "gcm_ANN = GCM_discrete(net_ANN_global)\n",
    "gcm_CNN = GCM_discrete(net_CNN_global)\n",
    "gcm_local_ANN = GCM_local_discrete(net_ANN_local, None)\n",
    "gcm_local_poly = GCM_local_discrete(net_poly, get_poly_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Instantaneous predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instataneous predictions by polynomial network and tanh network\n",
    "X_predict_poly = np.zeros(X_true.shape)\n",
    "X_predict_local_ANN = np.zeros(X_true.shape)\n",
    "X_predict_global_ANN = np.zeros(X_true.shape)\n",
    "X_predict_global_CNN = np.zeros(X_true.shape)\n",
    "for t in range(X_true.shape[0] - 1):\n",
    "    X_predict_poly[t + 1, :] = gcm_local_poly(X_true[t, :], 1)[1, :]\n",
    "    X_predict_local_ANN[t + 1, :] = gcm_local_ANN(X_true[t, :], 1)[1, :]\n",
    "    X_predict_global_ANN[t + 1, :] = gcm_ANN(X_true[t, :], 1)[1, :]\n",
    "    X_predict_global_CNN[t + 1, :] = gcm_CNN(X_true[t, :], 1)[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = 9  # Example variable, the 9th grid point\n",
    "fig = plt.figure(figsize=[5, 5])\n",
    "t_ind = np.arange(1000, 11000)\n",
    "plt.figure(dpi=150)\n",
    "plt.title(\"Instantaneous predictions (shifted)\")\n",
    "plt.plot(\n",
    "    dX_true[t_ind, X_plot],\n",
    "    0.03\n",
    "    + X_predict_global_ANN[t_ind + 1, X_plot]\n",
    "    - X_predict_global_ANN[t_ind, X_plot],\n",
    "    label=\"global ANN\",\n",
    ")\n",
    "plt.plot(\n",
    "    dX_true[t_ind, X_plot],\n",
    "    0.02\n",
    "    + X_predict_global_CNN[t_ind + 1, X_plot]\n",
    "    - X_predict_global_CNN[t_ind, X_plot],\n",
    "    label=\"global CNN\",\n",
    ")\n",
    "plt.plot(\n",
    "    dX_true[t_ind, X_plot],\n",
    "    0.01 + X_predict_poly[t_ind + 1, X_plot] - X_predict_poly[t_ind, X_plot],\n",
    "    label=\"local polynet\",\n",
    ")\n",
    "plt.plot(\n",
    "    dX_true[t_ind, X_plot],\n",
    "    X_predict_local_ANN[t_ind + 1, X_plot] - X_predict_local_ANN[t_ind, X_plot],\n",
    "    label=\"local ANN\",\n",
    ")\n",
    "plt.legend(frameon=False, fontsize=7)\n",
    "plt.ylim([-0.3, 0.2])\n",
    "plt.xlim([-0.3, 0.2])\n",
    "plt.xlabel(r\"Ground truth $dX_{0}$\".format(X_plot))\n",
    "plt.ylabel(r\"Predicted $dX_{0}$\".format(X_plot))\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Time-dependent initial-value simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform simulations with the GCM objects\n",
    "init_cond = X_true[0, :]\n",
    "X_simulation_global_ANN = gcm_ANN(init_cond, t_plot)\n",
    "X_simulation_global_CNN = gcm_CNN(init_cond, t_plot)\n",
    "X_simulation_local_ANN = gcm_local_ANN(init_cond, t_plot)\n",
    "X_simulation_poly = gcm_local_poly(init_cond, t_plot)\n",
    "\n",
    "# Perform the groud truth simulation\n",
    "gcm_no_param = GCM_no_param(F, time_stepping=RK4)\n",
    "X_no_param, _ = gcm_no_param(init_cond, temp_dt, t_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show trajectories of a single variable\n",
    "temp_t = min([X_no_param.shape[0], 1200])\n",
    "t_ind = np.arange(0, 1000)\n",
    "fig = plt.figure(figsize=[9, 4], dpi=150)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Initial-value simulations, $X_9$\")\n",
    "plt.plot(t_series_plot[t_ind], X_no_param[t_ind, X_plot], \"k\", label=\"Truth\")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind],\n",
    "    X_simulation_global_ANN[t_ind, X_plot],\n",
    "    \"-.\",\n",
    "    label=\"Global ANN\",\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind],\n",
    "    X_simulation_global_CNN[t_ind, X_plot],\n",
    "    \"-.\",\n",
    "    label=\"Global CNN\",\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind], X_simulation_poly[t_ind, X_plot], \"--\", label=\"Local polynet\"\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind], X_simulation_local_ANN[t_ind, X_plot], \"--\", label=\"Local ANN\"\n",
    ")\n",
    "plt.legend(frameon=False, loc=\"best\", fontsize=7)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(r\"$X_{0:d}$\".format(X_plot))\n",
    "fig.tight_layout()\n",
    "\n",
    "# Energy conservation\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Initial-value simulations, energy\")\n",
    "plt.plot(t_series_plot[t_ind], np.sum(X_no_param[t_ind, :] ** 2, 1), \"k\", label=\"Truth\")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind],\n",
    "    np.sum(X_simulation_global_ANN[t_ind, :] ** 2, 1),\n",
    "    \"-.\",\n",
    "    label=\"Global ANN\",\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind],\n",
    "    np.sum(X_simulation_global_CNN[t_ind, :] ** 2, 1),\n",
    "    \"-.\",\n",
    "    label=\"Global CNN\",\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind],\n",
    "    np.sum(X_simulation_poly[t_ind, :] ** 2, 1),\n",
    "    \"--\",\n",
    "    label=\"Local polynet\",\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind],\n",
    "    np.sum(X_simulation_local_ANN[t_ind, :] ** 2, 1),\n",
    "    \"--\",\n",
    "    label=\"Local ANN\",\n",
    ")\n",
    "plt.legend(frameon=False, loc=\"best\", fontsize=7)\n",
    "plt.ylim(np.array([0, 2]) * np.sum(init_cond**2))\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(r\"Energy ($|\\mathbf{X}|^2$)\")\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Show parameters learned by the polynet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to show parameters in the polynet\n",
    "def plot_parameters(net_poly, si, filter_loc, var_str, weights_true):\n",
    "    # Store the weights of the polynomial networks and normalize by sampling interval\n",
    "    weights_poly = (list(net_poly.named_parameters())[0][1].data.numpy() / si).squeeze()\n",
    "\n",
    "    n = len(filter_loc)\n",
    "    n_feature = int(n + n + n * (n - 1) / 2)\n",
    "    from itertools import combinations\n",
    "\n",
    "    index_list = list(combinations(np.arange(n) + filter_loc[0], 2))\n",
    "    x_plot = np.arange(n_feature) * 0.8\n",
    "\n",
    "    def label_part(f):\n",
    "        if f > 0:\n",
    "            return r\"+{0:d}\".format(f)\n",
    "        elif f < 0:\n",
    "            return r\"-{0:d}\".format(-f)\n",
    "        elif f == 0:\n",
    "            return r\"\"\n",
    "\n",
    "    xlabels = (\n",
    "        [r\"$\" + var_str + \"_{k\" + label_part(-f) + \"}$\" for f in filter_loc]\n",
    "        + [r\"$\" + var_str + \"^2_{k\" + label_part(-f) + \"}$\" for f in filter_loc]\n",
    "        + [\n",
    "            r\"$\"\n",
    "            + var_str\n",
    "            + \"_{k\"\n",
    "            + label_part(-index[0])\n",
    "            + \"}\"\n",
    "            + var_str\n",
    "            + \"_{k\"\n",
    "            + label_part(-index[1])\n",
    "            + \"}$\"\n",
    "            for index in index_list\n",
    "        ]\n",
    "    )\n",
    "    # Scatter plot\n",
    "    fig = plt.figure(figsize=[9, 3])\n",
    "    cmap = matplotlib.colormaps[\"bone_r\"]\n",
    "    plt.bar(\n",
    "        x_plot - 0.10,\n",
    "        weights_poly,\n",
    "        label=\"Polynet\",\n",
    "        color=\"tab:blue\",\n",
    "        edgecolor=\"k\",\n",
    "        width=0.15,\n",
    "    )\n",
    "    plt.bar(\n",
    "        x_plot + 0.10,\n",
    "        weights_true,\n",
    "        label=\"Truth\",\n",
    "        color=\"tab:brown\",\n",
    "        edgecolor=\"k\",\n",
    "        width=0.15,\n",
    "    )\n",
    "    plt.plot([x_plot[0] - 5, x_plot[-1] + 5], np.zeros(2), \"--k\", linewidth=1.0)\n",
    "    plt.legend(frameon=False, loc=\"best\")\n",
    "    plt.xticks(x_plot, rotation=-30)\n",
    "    plt.gca().set_xticklabels(xlabels)\n",
    "    plt.xlim([x_plot[0] - 1, x_plot[-1] + 1])\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "# Show parameters learned by the polynet\n",
    "weights_true = np.array([0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1])\n",
    "plot_parameters(net_poly, si, filter_loc, \"X\", weights_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Compare with the original 1-variable L96 equation: \n",
    "\\begin{equation}\n",
    "\\dot{X_k} = - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F\n",
    "\\end{equation}\n",
    "we see that the polynomial stencil can reconstruct the advection term and the exponential decay term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## III. Energy-conserved case\n",
    "It would be interesting to see the performance of ANN, CNN, and polynomial stencils when the underlying system conserves energy:\n",
    "\n",
    "\\begin{equation}\n",
    "\\dot{X_k} = - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Train networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = 0  # no forcing\n",
    "a = 0  # no exponential decay\n",
    "h = 0  # no interaction with subgrid scale\n",
    "\n",
    "Nt = 20000\n",
    "dt = 0.001\n",
    "si = dt  # Sampling interval\n",
    "T = dt * Nt  # Total time of simulation\n",
    "\n",
    "K = 100  # The number of slow variables, and\n",
    "J = 0  # The number of fast variables, set to 0\n",
    "\n",
    "# Define the ground truth model to generate training data\n",
    "W = L96(K, J, F=F, dt=dt, h=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_analysis\n",
    "\n",
    "(\n",
    "    net_ANN_global_2,\n",
    "    net_CNN_global_2,\n",
    "    net_ANN_local_2,\n",
    "    net_poly_2,\n",
    "    X_true_2,\n",
    "    dX_true_2,\n",
    ") = run_analysis(W, si, T, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Generate simulation data with networks trained on the energy-conserved case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test predictions\n",
    "temp_dt = si\n",
    "t_plot = X_true_2.shape[0] - 1\n",
    "t_series_plot = np.arange(0, t_plot + 1) * temp_dt\n",
    "\n",
    "# Define GCM objects using the trained networks\n",
    "gcm_ANN = GCM_discrete(net_ANN_global_2)\n",
    "gcm_CNN = GCM_discrete(net_CNN_global_2)\n",
    "gcm_local_ANN = GCM_local_discrete(net_ANN_local_2, None)\n",
    "gcm_local_poly = GCM_local_discrete(net_poly_2, get_poly_features)\n",
    "\n",
    "# Instataneous predictions by polynomial network and tanh network\n",
    "X_predict_poly = np.zeros(X_true_2.shape)\n",
    "X_predict_local_ANN = np.zeros(X_true_2.shape)\n",
    "X_predict_global_ANN = np.zeros(X_true_2.shape)\n",
    "X_predict_global_CNN = np.zeros(X_true_2.shape)\n",
    "for t in range(X_true_2.shape[0] - 1):\n",
    "    X_predict_poly[t + 1, :] = gcm_local_poly(X_true_2[t, :], 1)[1, :]\n",
    "    X_predict_local_ANN[t + 1, :] = gcm_local_ANN(X_true_2[t, :], 1)[1, :]\n",
    "    X_predict_global_ANN[t + 1, :] = gcm_ANN(X_true_2[t, :], 1)[1, :]\n",
    "    X_predict_global_CNN[t + 1, :] = gcm_CNN(X_true_2[t, :], 1)[1, :]\n",
    "\n",
    "# Perform simulations with the GCM objects\n",
    "init_cond = X_true_2[0, :]\n",
    "X_simulation_global_ANN = gcm_ANN(init_cond, t_plot)\n",
    "X_simulation_global_CNN = gcm_CNN(init_cond, t_plot)\n",
    "X_simulation_local_ANN = gcm_local_ANN(init_cond, t_plot)\n",
    "X_simulation_poly = gcm_local_poly(init_cond, t_plot)\n",
    "\n",
    "# Perform the groud truth simulation with different forward-stepping schemes\n",
    "# 1. RK4\n",
    "gcm_no_param = GCM_no_param(F, time_stepping=RK4)\n",
    "X_no_param, _ = gcm_no_param(init_cond, temp_dt, t_plot)\n",
    "# 2. RK2\n",
    "gcm_no_param_RK2 = GCM_no_param(F, time_stepping=RK2)\n",
    "X_no_param_RK2, _ = gcm_no_param_RK2(init_cond, temp_dt, t_plot)\n",
    "# 3. Euler\n",
    "gcm_no_param_Euler = GCM_no_param(F, time_stepping=EulerFwd)\n",
    "X_no_param_Euler, _ = gcm_no_param_Euler(init_cond, temp_dt, t_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Plot instantaneous prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = 9  # Example variable, the 9th grid point\n",
    "fig = plt.figure(figsize=[5, 5])\n",
    "t_ind = np.arange(1000, 11000)\n",
    "plt.figure(dpi=150)\n",
    "plt.title(\"Instantaneous predictions (shifted)\")\n",
    "plt.plot(\n",
    "    dX_true_2[t_ind, X_plot],\n",
    "    0.18\n",
    "    + X_predict_global_ANN[t_ind + 1, X_plot]\n",
    "    - X_predict_global_ANN[t_ind, X_plot],\n",
    "    label=\"global ANN\",\n",
    ")\n",
    "plt.plot(\n",
    "    dX_true_2[t_ind, X_plot],\n",
    "    0.12\n",
    "    + X_predict_global_CNN[t_ind + 1, X_plot]\n",
    "    - X_predict_global_CNN[t_ind, X_plot],\n",
    "    label=\"global CNN\",\n",
    ")\n",
    "plt.plot(\n",
    "    dX_true_2[t_ind, X_plot],\n",
    "    0.06 + X_predict_poly[t_ind + 1, X_plot] - X_predict_poly[t_ind, X_plot],\n",
    "    label=\"local polynet\",\n",
    ")\n",
    "plt.plot(\n",
    "    dX_true_2[t_ind, X_plot],\n",
    "    X_predict_local_ANN[t_ind + 1, X_plot] - X_predict_local_ANN[t_ind, X_plot],\n",
    "    label=\"local ANN\",\n",
    ")\n",
    "plt.legend(frameon=False, fontsize=7)\n",
    "# plt.ylim([-0.3, 0.2])\n",
    "# plt.xlim([-0.3, 0.2])\n",
    "plt.xlabel(r\"Ground truth $dX_{0}$\".format(X_plot))\n",
    "plt.ylabel(r\"Predicted $dX_{0}$\".format(X_plot))\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution of one variable\n",
    "temp_t = min([X_no_param.shape[0], 1200])\n",
    "X_plot = 9\n",
    "t_ind = np.arange(0, 2000)\n",
    "fig = plt.figure(figsize=[8, 6])\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(\"Initial-value simulations, $X_9$\")\n",
    "plt.plot(t_series_plot[t_ind], X_no_param[t_ind, X_plot], \"k\", label=\"Truth (RK4)\")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind], X_no_param_Euler[t_ind, X_plot], \"grey\", label=\"Truth (Euler)\"\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind],\n",
    "    X_simulation_global_ANN[t_ind, X_plot],\n",
    "    \"-.\",\n",
    "    label=\"global ANN\",\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind],\n",
    "    X_simulation_global_CNN[t_ind, X_plot],\n",
    "    \"-.\",\n",
    "    label=\"global CNN\",\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind], X_simulation_poly[t_ind, X_plot], \"--\", label=\"local polynet\"\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind], X_simulation_local_ANN[t_ind, X_plot], \"--\", label=\"local ANN\"\n",
    ")\n",
    "plt.legend(frameon=False, loc=\"best\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(r\"$X_{0:d}$\".format(X_plot))\n",
    "plt.xlim([-0.1, 3])\n",
    "plt.ylim([-80, 80])\n",
    "fig.tight_layout()\n",
    "\n",
    "# Energy evolution\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"Initial-value simulations, energy\")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind], np.sum(X_no_param[t_ind, :] ** 2, 1), \"k\", label=\"Truth (RK4)\"\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind],\n",
    "    np.sum(X_no_param_Euler[t_ind, :] ** 2, 1),\n",
    "    \"grey\",\n",
    "    label=\"Truth (Euler)\",\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind],\n",
    "    np.sum(X_simulation_global_ANN[t_ind, :] ** 2, 1),\n",
    "    \"-.\",\n",
    "    label=\"global ANN\",\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind],\n",
    "    np.sum(X_simulation_global_CNN[t_ind, :] ** 2, 1),\n",
    "    \"-.\",\n",
    "    label=\"global CNN\",\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind],\n",
    "    np.sum(X_simulation_poly[t_ind, :] ** 2, 1),\n",
    "    \"--\",\n",
    "    label=\"local polynet\",\n",
    ")\n",
    "plt.plot(\n",
    "    t_series_plot[t_ind],\n",
    "    np.sum(X_simulation_local_ANN[t_ind, :] ** 2, 1),\n",
    "    \"--\",\n",
    "    label=\"local ANN\",\n",
    ")\n",
    "plt.legend(frameon=False, loc=\"best\")\n",
    "plt.ylim(np.array([0, 2]) * np.sum(init_cond**2))\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(r\"Energy ($|\\mathbf{X}|^2$)\")\n",
    "plt.xlim([-0.1, 3])\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# III. A more realistic model: the Burger's equation\n",
    "\n",
    "## Problem with the 1-variable L96 equation\n",
    "\n",
    "One problem with the L96 model is that it sometimes lacks the physical basis. If we look at the temporal evolution of the 1-variable L96 equation, we see that its dynamics look quite unstable, and most of its variabilities can be numerical artifacts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Start from a bump initial condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy conserving case, 1-variable L96\n",
    "K = 100  # The number of slow variables, and\n",
    "J = 0  # The number of fast variables per slow variable, set to 0\n",
    "F = 0  # no external forcing\n",
    "a = 0  # no exponential decay\n",
    "h = 0  # no interaction with subgrid scale\n",
    "\n",
    "# Set a bump initial condition\n",
    "x = np.arange(0, K)\n",
    "X_init = np.exp(-(((x - x.mean()) / K * 10) ** 2)) * 10\n",
    "\n",
    "W_conserve = L96(K, J, F=F, dt=dt, h=h)\n",
    "(\n",
    "    X_true_3,\n",
    "    _,\n",
    "    _,\n",
    ") = W_conserve.run(si, T, store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate conservative 1-variable L96 equation\n",
    "plt.ioff()  # close notebook mode\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "interval = 10\n",
    "frames = 200\n",
    "\n",
    "fig = plt.figure(dpi=150)\n",
    "ax = plt.axes(xlim=(0, K), ylim=(-25, 25))\n",
    "plt.title(\"Energy-conserved initial value problem\")\n",
    "(line,) = ax.plot([], [], lw=3)\n",
    "plt.xlabel(r\"$k$\")\n",
    "plt.ylabel(r\"$X$\")\n",
    "\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    return (line,)\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    x = np.arange(0, K)\n",
    "    y = X_true_3[i * 15, :]\n",
    "    line.set_data(x, y)\n",
    "    return (line,)\n",
    "\n",
    "\n",
    "anim = FuncAnimation(fig, animate, init_func=init, frames=frames, interval=1, blit=True)\n",
    "gif_name = os.path.join(plot_path, \"L96_one_variable.gif\")\n",
    "anim.save(gif_name, writer=\"pillow\")\n",
    "# reopen inline mode\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "![SegmentLocal](figs/L96_one_variable.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1-variable energy-conserved case with the L96 system is just an advection equation with the sign of the sign advection term flipped:\n",
    "\n",
    "\\begin{equation}\n",
    "\\dot{X_k} = - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "We can find much similarity with the Burger's equation. \n",
    "\n",
    "## The Burger's equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the performance of the neural networks on a more realistic model, we use the Burger's equation as an example. The Burger's equation is an advection-diffusion equation. It has a diffusive term which reduces the tendency to develop shock waves and numerical instabilities. \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial u}{\\partial t} = -u\\cdot\\nabla u + \\nu\\nabla^2 u. \n",
    "\\end{equation}\n",
    "\n",
    "When discretized with simple finite difference, the tendency of $u$ at a particular location be written as functions of adjacent variables\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d u_k}{d t} = -\\frac{1}{2\\Delta x}u_k (u_{k+1} - u_{k-1}) + \\frac{\\nu}{\\Delta x^2} (u_{k+1} - 2u_k + u_{k-1}), \n",
    "\\end{equation}\n",
    "\n",
    "which makes it an ideal target for learning local maps with machine-learning algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data from Burger's equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters of the Burger's equation\n",
    "nu = 0.3\n",
    "dx = 1\n",
    "K = 100\n",
    "dt = 0.01\n",
    "T = 100\n",
    "si = dt\n",
    "x = np.arange(0, K)\n",
    "\n",
    "# Use two sets of initial-value simulations for training\n",
    "W1 = Burgers(K, nu, dx, dt=dt)\n",
    "init_cond1 = np.exp(-(((x - x.mean()) / K * 10) ** 2))\n",
    "W1.set_init(init_cond1)\n",
    "\n",
    "init_cond2 = -np.exp(-np.abs((x - x.mean()) / K * 15))\n",
    "W2 = Burgers(K, nu, dx, dt=dt)\n",
    "W2.set_init(init_cond2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(init_cond1, lw=2, label=\"initial condition 1\")\n",
    "plt.plot(init_cond2, lw=2, label=\"initial condition 2\")\n",
    "plt.legend(frameon=False, fontsize=7)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"u\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Train the same set of networks with the Burger's equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a wrapper function to perform all of the training above\n",
    "from utils import run_analysis\n",
    "\n",
    "(\n",
    "    net_ANN_global_burger,\n",
    "    net_CNN_global_burger,\n",
    "    net_ANN_local_burger,\n",
    "    net_poly_burger,\n",
    "    X_true_burger,\n",
    "    dX_true_burger,\n",
    ") = run_analysis([W1, W2], si, T, n_epochs=n_epochs, Poly_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test predictions with a different initial condition\n",
    "gcm_ANN = GCM_discrete(net_ANN_global_burger)\n",
    "gcm_CNN = GCM_discrete(net_CNN_global_burger)\n",
    "gcm_local_ANN = GCM_local_discrete(net_ANN_local_burger, None)\n",
    "gcm_local_poly = GCM_local_discrete(net_poly_burger, get_poly_features)\n",
    "\n",
    "# Define a different initial condition\n",
    "x = np.arange(0, K)\n",
    "t_plot = int(T / si)\n",
    "init_cond = np.exp(-np.abs((x - 0.1 * x.mean()) / K * 20))\n",
    "\n",
    "# Calculate ground truth using Burger's equation\n",
    "W1.set_init(init_cond)\n",
    "X_simulation_truth = W1.run(si, T, store=True)[0]\n",
    "\n",
    "# Calculate initial-value simulations with the trained networks\n",
    "X_simulation_global_ANN = gcm_ANN(init_cond, t_plot)\n",
    "X_simulation_global_CNN = gcm_CNN(init_cond, t_plot)\n",
    "X_simulation_local_ANN = gcm_local_ANN(init_cond, t_plot)\n",
    "X_simulation_poly = gcm_local_poly(init_cond, t_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "from utils import animate_Burgers\n",
    "\n",
    "animate_Burgers(\n",
    "    X_simulation_truth,\n",
    "    X_simulation_global_ANN,\n",
    "    X_simulation_global_CNN,\n",
    "    X_simulation_local_ANN,\n",
    "    X_simulation_poly,\n",
    "    plot_path,\n",
    ")\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "![SegmentLocal](figs/Burgers_simulation.gif \"segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_true = np.array(\n",
    "    [\n",
    "        0,\n",
    "        nu / dx**2,\n",
    "        -2 * nu / dx**2,\n",
    "        nu / dx**2,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        -1 / 2 / dx,\n",
    "        0,\n",
    "        0,\n",
    "        1 / 2 / dx,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "filter_start = -2\n",
    "filter_end = 2\n",
    "# this includes k+2, k+1, k, k-1, k-2, note that roll by +2 means looking at location k-2\n",
    "filter_loc = np.arange(filter_start, filter_end + 1)\n",
    "plot_parameters(net_poly_burger, si, filter_loc, \"u\", weights_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The polynet appropriately recovers the terms in the Burger's equation: \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d u_k}{d t} = 0.5 u_k (u_{k+1} - u_{k-1}) + 0.3 (u_{k+1} - 2u_k + u_{k-1}), \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- Convolutional neural networks has better error performance during training, partially due to its added complexity\n",
    "- Local maps is potentially more useful than global maps in the following aspects:\n",
    "    - It achieves better performance in initial-value simulations with smaller number of parameters, energy-conservation properties, and less training data. \n",
    "    - It is not constrained by training data geometry -- can be easily generalized to larger simulation domains. \n",
    "    - It does not need access to global data\n",
    "    - Local maps are more explanable due to its simplified structure and parameters\n",
    "- Local polynomial stencils might be useful as an alternative functional class than off-the-shelf ML algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
